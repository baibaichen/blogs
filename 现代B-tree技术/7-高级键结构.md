## 7-高级键结构

如前几节所示，为了完全支持数据库索引结构（如B-tree），需要大量的设计和编码。没有其他索引结构受到数据库研究人员和软件开发人员如此的关注。但是，通过仔细和创造性地构造B-tree的键，可以少量修改（如果有的话）核心B-tree代码，从而启用额外的索引功能。本节调查其中几个。

本节不考虑**计算列**的索引，即从同一表中的其他列派生的值，并在数据库表中给出名称。 这些列根据需要进行计算，无需存储在表的主数据结构中。 但是，如果此列上存在索引，则会在此二级索引中存储相应的键值。

类似地，本节不考虑**部分索引**，即基于**选择谓词**的索引，索引的数据记录少于基础表的数据记录。典型的谓词确保只有非空的值被索引。**计算列**和**部分索引**这两个主题，与B-tree中的高级键结构正交。

前面的讨论（第2.5节）给出了一个独特的B-tree键的示例，即哈希索引基于哈希值上的B-tree来实现。B-tree代码中的一些小调整，模拟了传统上与哈希索引相关的主要性能优点，即最坏情况下单个I/O、哈希目录内的直接地址计算、和高效的键值比较。将一个非常大的根页（非常类似于一个大的散列目录）固定在缓冲池中可以模拟第一个优势；另外两个好处可以通过适当的键值来模拟，包括最小规范化键。

- 具有高级键结构的B-tree保留了B-tree的所有优点，例如，键区间锁定的理论和实现、日志和恢复的优化，高性能的索引创建以及用于高效数据库操作的其他工具。
- 哈希值上的B-tree和传统哈希索引相比有许多优点，性能相当。

### 7.1 Multi-dimensional UB-trees 

By their nature, B-trees support only a single sort order. If multiple key columns are used, they may form a hierarchy of major sort key, minor sort key, etc. In this case, queries that restrict the leading key columns perform better than those that do not. If, however, the key columns represent dimensions in a space, e.g., a geometric space, queries may restrict the leading column only by a range predicate or not at all. Leslie et al. [82] describe sophisticated algorithms for accessing B-trees in those cases.

An alternative approach projects multi-dimensional data onto a sin-gle dimension based on space-filling curves. The principal tradeoffin the design of space-filling curves is conceptual and computational simplic-ity on one hand and preservation of locality and thus search efficiency on the other hand. The simplest construction of a space-filling curve first maps each dimension into an unsigned integer and then inter-leaves individual bits from these integers. When drawn as a line in a 2-dimensional space, this space-filling curve resembles nested *Z*shapes, which is why it is also called the *z*-order. This is the design underlying multi-dimensional indexing and query processing in Probe [106] and Transbase [109]. Alternatives to this Morton curve include the Hilbert curve and others.

> ==TODO：==Fig 7.1

Figure 7.1 (copied from [106]) illustrates *z*-order curves. The origi-nal *x*- and *y*-coordinates have 3 bits and therefore 8 distinct values; the interleaved *z*-values contain 6 bits and therefore 64 points. The “*Z*” shape is repeated at 3 scales. The technique applies to any number of dimensions, not only 2-dimensional spaces as shown in Figure 7.1.

UB-trees are B-tree indexes on such *z*-values. Each point and range query against the original dimensions is mapped to the appropriate intervals along the *z*-curve. The crucial component of this mapping is determining the *z*-values at which the *z*-curve enters and exits the multi-dimensional range defined by the query predicate. Known algo-rithms are linear in the number of original dimensions, their resolution (i.e., the number of bits in the *z*-values), and the number of entry and exit points [109].

In addition to points in multi-dimensional space, space-filling curves, *z*-order mappings, and UB-trees can index multi-dimensional rectangles (boxes) by treating start and end points as separate dimen-sions. In other words, UB-trees can index not only information about points but also about intervals and rectangles. Both space and time (including intervals) can be indexed in this way. For moving objects, location and speed (in each spatial dimension) can be treated as separate dimensions. Finally, even precision in location or speed can be indexed, if desired. Unfortunately, indexing based on space-filling curves loses effectiveness with the number of dimensions, just as the performance of traditional B-tree applications suffers when a B-tree contains many columns but only a few of them are specified in a query predicate.

- *Z*-values (or other space-filling curves) provide some multi-dimensional indexing with all the advantages of B-trees. 
-  The query performance of specialized multi-dimensional indexes is probably better, but load and update performance of B-trees are not easy to match. 

### 7.2 Partitioned B-trees

As discussed earlier in the section on bulk insertions and illustrated in Figure 6.6, the essence of partitioned B-trees [43] is to maintain partitions within a single B-tree, by means of an artificial leading key field. Partitions and the artificial leading key field are hidden from the database user. They exist in order to speed up large operations on B-trees, not to carry any information. Partitions are optimized using the merge step well known from external merge sort. By default, the same single value appears in all records in a B-tree, and most of the specific techniques rely on exploiting multiple alternative values, but only temporarily. If a table or index is represented in multiple B-trees, the artificial leading key field should be defined separately for each such B-tree.

The leading artificial key column effectively defines partitions within a single B-tree. Each existing distinct value implicitly defines a par-tition, and partitions appear and vanish automatically as records are inserted and deleted. The design differs from traditional horizontal par-titioning using a separate B-tree for each partition in an important way: Most advantages of the design depend on partitions (or distinct values in the leading artificial key column) being created and removed very dynamically. In a traditional implementation of partitioning (using multiple B-trees), creation or removal of a partition is a change of the table’s schema and catalog entries, which requires locks on the table’s schema or catalog entries and thus excludes concurrent or long-running user accesses to the table, as well as forcing recompilation of cached query and update plans. If partitions within a single B-tree are created and removed as easily as inserting and deleting rows, smooth contin-uous operation is relatively easy to achieve. It is surprising how many problems this simple technique can help address in data management software and its real-world usage.

**First**, it permits putting all runs in an external merge sort into a single B-tree (with the run number as the artificial leading key field), which in turn permits improvements to asynchronous read-ahead and to adaptive memory usage. In SAN and NAS environments, hid-ing latency by exploiting asynchronous read-ahead is important. With striped disks, forecasting multiple I/O operations is important. Finally, in very large online databases, the ability to dynamically grow and shrink resources dedicated to a single operation is very important, and the proposed changes permit doing so even to the extremes of pausing an operation altogether and of letting a single operation use a machine’s entire memory and entire set of processors during an otherwise idle batch window. While sorting is used to build B-tree indexes efficiently and B-trees are used to avoid the expense of sorting and to reduce the expense of searching during query processing, the mutually beneficial relationship between sorting and B-trees can go substantially further.

**Second**, partitioned B-trees can substantially reduce, by at least a factor of two, the wait time before a newly created index is available for query answering. While the initial form of an index does not perform as well as the final, fully optimized index or a traditional index, at least it is usable by queries and permits replacing table scans with index searches, resulting in better query response time as well as a smaller “locking footprint” and thus a reduced likelihood of deadlocks. Moreover, the index can be improved incrementally from its initial form to its final and fully optimized form, which is very similar to the final form after traditional index creation. Thus, the final index is extremely similar in performance to indexes created oﬄine or with traditional online methods; the main difference is cutting in half (or better) the delay between a decision to create a new index and its first beneficial impact on query processing.

**Third**, adding a large amount of data to a large, fully indexed data warehouse so far has created a dilemma between dropping and rebuilding all indexes or updating all indexes one record at a time, implying random insertions, poor performance, a large log volume, and a large incremental backup. Partitioned B-trees resolve this dilemma in most cases without special new data structures. A load operation simply appends a number of new partitions to each affected index; the size of these partitions is governed by the memory allocation for the in-memory run generation during the load operation. Updates (both insertion and deletions) can be appended to an existing B-tree in one or multiple new partitions, to be integrated into the main partition at the earliest convenient time, at which time deletions can be applied to the appropriate old records. Appending partitions is, of course, yet another variation on the theme of differential files [117]. Batched maintenance in a partitioned B-tree reduces the overall update time; in addition, it can improve the overall space requirements if pages of the main parti-tion are filled completely with compressed records; and it may reduce query execution times if the main partition remains unfragmented and its pages optimized for efficient search, e.g., interpolation search.

While a partitioned B-tree actually contains multiple partitions, any query must search all of them. It is unlikely (and probably not even per-mitted by the query syntax) that a user query limits itself to a subset of partitions or even a single one. On the other hand, a historic or “as of” query might map to a single partition even when newer partitions are already available. In general, however, all existing partitions must be searched. As partitioning is implemented with an artificial leading key field in an otherwise standard B-tree implementation, this is equivalent to a query failing to restrict the leading column in a traditional multi-column B-tree index. Efficient techniques for this situation are known and not discussed further here [82].

Partitions may remain as initially saved or they may be merged. Merging may be eager (e.g., merging as soon as the number of parti-tions reaches a threshold), opportunistic (e.g., merging whenever there is idle time), or lazy (e.g., merging key ranges required to answer actual queries). The latter is called adaptive merging [53]. Rather than merg-ing partitions in preparation of query processing, merging can be inte-grated into query execution, i.e., be a side effect of query execution. Thus, even if key ranges are left as parameters in query predicates, this technique merges only key ranges actually queried. All other key ranges remain in the initial partitions. No merge effort is spent on them yet they are ready for query execution and index optimization should the workload and its access pattern change over time.

- In partitioned B-trees, partitions are identified by an artificial leading key field. Partitions appear and disappear simply by insertion and deletion of B-tree entries with appropriate key values, without catalog updates. 
- Partitioned B-trees are useful for efficient sorting (e.g., deep read-ahead), index creation (e.g., early query processing), bulk insertion (append-only data capture with in-memory sorting), and bulk deletion (victim preparation). 
- Query performance equals that of traditional B-trees once all partitions have been merged, which is the default state. 

### 7.3 Merged Indexes

As others have observed, “optimization techniques that reduce the number of physical I/Os are generally more effective than those that improve the efficiency in performing the I/Os” [70]. It is a common belief that clustering related records requires pointers between records. An example relational database management system with record clus-tering is Starburst [20], which uses hidden pointers between related records and affects their automatic maintenance during insertions, dele-tions, and updates. The technique serves only tables and their primary storage structures, not secondary indexes, and it requires many-to-one relationships defined with foreign key integrity constraints.

The desirability of clustering secondary indexes is easily seen in a many-to-many relationship such as “enrollment” as many-to-many relationship between “courses” and “students.” In order to support table-to-table, index-to-index, and record-to-record navigation both from students to courses and from courses to students, the enrollment table requires at least two indexes, only one of which can be the primary index. For efficient data access in both directions, however, it would be desirable to cluster one enrollment index with student records and one enrollment index with course records.

Merged indexes [49] are B-trees that contain multiple traditional indexes and interleave their records based on a common sort order. In relational databases, merged indexes implement “master-detail clustering” of related records, e.g., orders and order details. Thus, merged indexes shift de-normalization from the logical level of tables and rows to the physical level of indexes and records, which is a more appropriate place for it. For object-oriented applications, clustering can reduce the I/O cost for joining rows in related tables to a fraction com-pared to traditional indexes, with additional beneficial effects on buffer pool requirements.

> ==TODO：==Fig 7.2

Figure 7.2 shows the sort order of records within such a B-tree. The sort order alone keeps related records co-located; no additional point-ers between records are needed. In its most limited form, master-detail clustering combines two secondary indexes, e.g., associating two lists of row identifiers with each key value. Alternatively, master-detail clus-tering may merge two primary indexes but not admit any secondary indexes. The design for merged indexes accommodates any combination of primary and secondary indexes in a single B-tree, thus enabling clus-tering of entire complex objects. Moreover, the set of tables, views, and indexes can evolve without restriction. The set of clustering columns can also evolve freely. A relational query processor can search and update index records just as in traditional indexes. With these abilities, the proposed design may finally bring general master-detail clustering to traditional databases together with its advantages in performance and cost.

In order to simplify design and implementation of merged indexes, a crucial first step is to separate implementation of the B-tree structure from its contents. One technique is to employ normalized keys, dis-cussed and illustrated earlier in Figure 3.4, such that the B-tree struc-ture manages only binary records and binary keys. In merged indexes, the mapping from multi-column keys to binary search keys in a B-tree is a bit more complex than in traditional indexes, in particular if adding and removing any index at any time is desired and if individual indexes may have different key columns. Thus, it is essential to design a flex-ible mapping from keys in the index to byte strings in the B-tree. A tag that indicates a key column’s domain and precedes the actual key fields, as shown in Figure 7.3, can easily achieve this. In other words, when constructing a normalized key for a merged index, domain tags and field values alternate up to and including the identifier for the index.

> ==TODO：==Fig 7.3

In practice, different than illustrated in Figure 7.3, a domain tag will be a small number, not a string. It is possible to combine the domain tag with the Null indicator (omitted in Figure 7.3) such that the desired sort order is achieved yet actual values are stored on byte boundaries. Similarly, the index identifier will be a number rather than a string.

Domain tags are not required for all fields in a B-tree record. They are needed only for key columns, and more specifically only for those leading key columns needed for clustering within the merged index. Following these leading key columns is a special tag and the identifier of the individual index to which the record belongs. For example, in Figure 7.3, there are only 2 domain tags for key values plus the index identifier. If there never will be any need to cluster on the line numbers in order details, only leading key fields up to order number require the domain tag. Thus, the per-record storage overhead for merged indexes is small and may indeed be hidden in the alignment of fields to word boundaries for fast in-memory processing. An overhead of 2–4 single-byte domain tags per record may prove typical in practice.

- Merging multiple indexes into a single B-tree provides master-detail clustering with all the advantages of B-trees. A single B-tree may contain any number of primary and sec-ondary indexes of any number of tables. 
- The B-tree key alternates domain tags and values up to and including the index identifier. 
- Merged indexes permit tables in traditional normal forms with the performance of free denormalization. 
- Merged indexes are particularly valuable in systems with deep storage hierarchies. 

### 7.4 Column Stores

Columnar storage has been proposed as a performance enhancement for large scans and therefore for relational data warehouses where ad-hoc queries and data mining might not find appropriate indexes. Lack of indexes might be due to complex arithmetic expressions in query predicates or to unacceptable update and load performance. The basic idea for columnar storage is to store a relational table not in the tradi-tional format based on rows but in columns, such that scanning a single column can fully benefit from all the data bytes in a page fetched from disk or in a cache line fetched from memory.

If each column is sorted by the values it contains, values must be tagged with some kind of logical row identifier. Assembling entire rows requires join operations, which may be too slow and expensive. In order to avoid this expense, the columns in a table must be stored all in the same order. This order might be called the order of the rows in the table, since no one index determines it, and B-trees can realize column storage using tags with practically zero additional space.

These tags are in many ways similar to row identifiers, but there is an important difference between these tags and traditional row identi-fiers: tag values are not physical but logical. In other words, they do not capture or represent a physical address such as a page identifier, and there is no way to calculate a page identifier from a tag value. If a calculation exists that maps tag values to row address and back, this calculation must assume maximal length of variable-length columns. Thus, storage space would be wasted in some or all of the vertical par-titions, which would contradict the goal of columnar storage, namely very fast scans.

Since most database management systems rely on B-trees for most or all of their indexes, reuse and adaptation of traditional storage struc-tures mean primarily adaptation of B-trees, including their space man-agement and their reliance on search keys. In order to ensure that rows and their columns appear in the same sequence in all B-trees, the search key in all indexes must be the same. Moreover, in order to achieve the objectives, the storage requirement for search keys must be practically zero, which seems rather counter-intuitive.

The essence of the required technique is quite simple. Rows are assigned tag values sequentially numbered in the order in which they are added to the table. Note that tag values identify rows in a table, not records in an individual partition or in an individual index. Each tag value appears precisely once in each index, i.e., it is paired with one value for each column in the table. All vertical partitions are stored in B-tree format with the tag value as the leading key. The important aspect is how storage of this leading key is reduced to practically zero.

The page header in each B-tree page stores the lowest tag value among all entries on that page. The actual tag value for each individual B-tree entry is calculated by adding this value and the slot number of the entry within the page. There is no need to store the tag value in the individual B-tree entries; only a single tag value is required per page. If a page contains tens, hundreds, or even thousands of B-tree entries, the overhead for storing the minimal tag value is practically zero for each individual record. If the size of the row identifier is 4 or 8 bytes and the size of a B-tree node is 8 KB, the per-page row identifier imposes an overhead of 0.1% or less.

If all the records in a page have consecutive tag values, this method not only solves the storage problem but also reduces “search” for a particular key value in the index to a little bit of arithmetic followed by a direct access to the desired B-tree entry. Thus, the access performance in leaf pages of these B-trees can be even better than that achieved with interpolation search or in hash indexes.

> ==TODO：==Fig 7.4

Figure 7.4 illustrates a table with 2 columns and 3 rows and colum-nar storage for it. The values in parentheses indicate row identifiers or tags. The right part of the diagram shows two disk pages, one for each column. The column headers of each page (dashed lines) show a row count and the lowest tag in the page.

The considerations so far have covered only the B-tree’s leaf pages. Of course, the upper index pages also need to be considered. Fortunately, they introduce only moderate additional storage needs. Storage needs in branch nodes is determined by the key size, the pointer size, and any over-head for variable-length entries. In this case, the key size is equal to that of row identifiers, typically 4 or 8 bytes. The pointer size is equal to a page identifier, also typically 4 or 8 bytes. The overhead for managing variable-length entries, although not strictly needed for the B-tree indexes under consideration, is typically 4 bytes for a byte offset and a length indicator. Thus, the storage needs for each separator entry is 8 to 20 bytes. If the node size is, for example, 8 KB, and average utilization is 70%, the average B-tree fan-out is 280 to 700. Thus, all upper B-tree pages together require disk space less than or equal to 0.3% of the disk space for all the leaf pages, which is a negligible in practice.

Compared to other schemes for storing vertical partitions, the described method permits very efficient storage of variable-length val-ues in the same order across multiple partitions. Thus, assembly of entire rows in a table is very efficient using a multi-way merge join. In addition, assembly of an individual row is also quite efficient, because each partition is indexed on the row identifier. All traditional opti-mizations of B-tree indexing apply, e.g., very large B-tree nodes and interpolation search. Note that interpolation search among a uniform data distribution is practically instant.

> ==TODO：==Fig 7.5

Figure 7.5 illustrates the value of B-trees for columnar storage, in particular if column values can vary in size either naturally or due to compression. The alphabet strings are actual values; the dashed boxes represent page headers with record count and lowest tag value. The upper levels of the B-tree indicate the lowest tag value in their respective subtrees. Leaf pages with varying record counts per page can readily be managed and assembly of individual rows by look-up of tags can be very efficient. Depending on the distributions of key values and their sizes, further compression may be possible and is often employed in relational database management system with columnar storage.

- With appropriate compression adapting run-length encoding to series of row identifiers, columnar storage may be based on B-trees. 

### 7.5 Large Values

In addition to B-trees containing many records, each smaller than a single leaf page, B-trees can also represent large binary objects or byte strings with many bytes. In that case, the leaf nodes contain data bytes and the branch nodes contain sizes or offsets. The data bytes in the leaf nodes can be divided into records as in traditional B-tree indexes or they can be without any additional structure, i.e., byte strings. In the latter case, most or all size information is kept in the branch nodes. Sizes or offsets serve as separator keys within branch nodes. In order to minimize effort and scope of update operations, in particular insertion and deletion of individual bytes or of substrings, sizes and offsets are counted locally, i.e., within a node and its children, rather than globally within the entire large binary object.

> ==TODO：==Fig 7.6

Figure 7.6, adapted from [18, 19], illustrates these ideas. In this example, the total size of the object is 900 bytes. The tree nodes at the leaf level indicate byte ranges. The values are shown in the leaf nodes only for illustration here; instead, the leaf nodes should contain the actual data bytes and possibly a local count of valid bytes. The branch nodes of the tree indicate sizes and offsets within the large object. Key values in the left half of the figure and in the root node are fairly obvious. The most interesting entries in this tree are the key values in the right parent node. They indicate the count of valid bytes in their child nodes; they do not indicate the position of those bytes within the entire object. In order to determine absolute positions, one needs to add the key values from the root to the leaf. For example, the absolute position of the left-most byte in the right-most leaf node is 421 + 365 = 786.

Similarly, search may use binary search (or even interpolation search) within a node but must adjust for key values in upper nodes. For example, a root-to-leaf traversal in search of byte 698 may use a binary search in the right parent node but only after subtracting the key value in the root (421) from the search key (698), i.e., searching for key value 698 *−*421 = 277 within the right parent node and finding the interval between 192 and 365. With that leaf, local byte position 277 *−*192 = 85 corresponds to global byte position 698.

Insertion or deletion of some bytes in some leaf node affect only the branch nodes along one root-to-leaf path. For example, deletion of 10 bytes at position 30 reduces the values 120, 282, and 421 in Fig-ure 7.6. Although such a deletion changes the absolute positions of the data bytes in the right subtree, the right parent node and its children remain unchanged. Similarly, insertion or deletion of an entire leaf node and its data bytes affect only along a single root-to-leaf path. Main-tenance of the key values along the path can be part of the initial root-to-leaf traversals in search of the affected leaves or it can follow maintenance of the data bytes in the leaf nodes. All nodes can be kept 50–100% full using algorithms very similar to traditional B-trees. Aggressive load balancing among sibling nodes can delay node splits. A B-tree representing a large object enables such a merge-before-split policy more than a standard B-tree because a parent contains suffi-cient information to decide whether or not sibling leaves are promising candidates for load balancing.

- With relative byte offsets as key values, a B-tree can be adapted to store large objects spanning many pages, even permitting efficient insertions and deletions of byte ranges.

### 7.6 Record Versions

Many applications require notions of “transaction time” and “real-world time,” i.e., information about when a fact has been inserted into the database and when the fact is valid in the real world. Both notions of time enable what is sometimes called “time travel,” includ-ing “what result would this query have had yesterday?” and “what is known now about yesterday’s status?” Both types of queries and their results can have legal importance.^1^

> 1. Michael Carey used to explain the need for editing large objects in a database with the following play on US presidential politics of the 1970s: “Suppose you have an audio object representing a recorded phone conversation and you feel the need to erase 18 minutes in the middle of it . . . ” Playing on US presidential politics of the 1980s, one might say here: “What did the database know, and when did he know it?” 

Since most transactions in most applications require the most up-to-date state, one implementation technique updates database records in place and, if required for an old transaction, rolls back the data page using a second copy in the buffer pool. The rollback logic is very similar to that for transaction rollback, except that it is applied to a copy of the data page. Transaction rollback relies on the chain of log records for each transaction; efficient rollback of a data page requires a chain of log records pertaining to each data page, i.e., each log record contains a pointer to the prior log record of the same transaction and another pointer to the prior log record pertaining to the same data page.

An alternative design relies on multiple actual records per logical record, i.e., versions of records. Versioning might be applied to and managed in a table’s main data structure only, e.g., the primary index, or it can be managed in each data structure, i.e., each secondary index, each materialized view, etc. If a design imposes substantial overheads in terms of space or effort, the former choice may be more appropriate. For greatest simplicity and uniformity of data structures and algorithms, it seems desirable to reduce overheads such that versioning can be applied in each data structure, e.g., each B-tree index in a database.

> ==TODO：==Fig 7.7

Figure 7.7 illustrates how some designs for record versioning tag each version record with the version’s start time, its end time, and a pointer to the next record in the chain of versions. In the example, changing a single small field to reflect a worker’s increased hourly wage requires an entire new record with all fields and tags. In a secondary index with few fields in each index entry, three additional fields impose a high overhead. By an appropriate modification of B-tree keys, however, two of these three fields can be avoided. Moreover, new versions can require much less space than complete new copies of the versioned record.

Specifically, if the start time provides the least significant part of a B-tree key, all versions of the same logical record (with the same user-defined key value) are neighbors in the sequence of records. Pointers or a version chain are not required as the sequence of versions is simply the sequence of B-tree entries. End times can be omitted if one version’s start time is interpreted as the prior version’s end time. Upon deletion of a logical record, a ghost record is required with the appropriate start time. This ghost record must be protected as long as it carries information about the logical record’s history and final deletion.

> ==TODO：==Fig 7.8

Figure 7.8 illustrates the design. The record keys are underlined. Start times are the only additional required field in version records, avoiding 2 of 3 additional fields required by the simplest design for version records with timestamps and naturally ensuring the desired placement of version records.

Start times can be compressed by storing, in each B-tree leaf, a base time equal to the oldest record version within the page. In that case, start times are represented within each record by the difference from the base time, which hopefully is a small value. In other words, an additional key field appended to the B-tree key can enable record versioning with a small number of bytes, possibly even a single byte.

Moreover, record contents can be compressed by explicitly storing only the difference between a version and its predecessor. For fastest retrieval and assembly of the most recent version, version records should store the difference between a version and its successor. In this case, retrieval of an older version requires multiple records somewhat similar to “undo” of log records. Alternatively, actual log records could be used, leading to a design similar to the one based on rollback of pages but applied to individual records.

> ==TODO：==Fig 7.9

Figure 7.9 illustrates these techniques. A field in the page header indicates the time of the oldest version record currently on the page. The individual records store the difference from this base time rather than a complete timestamp. Moreover, unchanged field values are not repeated. The order of version records is such that the current record is most readily available and older versions can be constructed by a local scan in forward direction. In the diagram, the absolute value of the prior value is shown, although for many data types, a relative value could be used, e.g., “*−*$3.” Further optimizations and compression, e.g., prefix truncation, may be employed as appropriate.

If “time travel” within a database can be limited, for example to one year into the past, all version records older than this interval can be interpreted as ghost records. Therefore, they are subject to removal and space reclamation just like traditional ghost records, with all rules and optimizations for locking and logging during ghost removal. When all other versions for a logical record have been thus removed, the ghost record indicating deletion of a logical record can also be removed and its space can be reclaimed.

If versioning in secondary indexes is independent from versioning in the table’s primary index, pointers in a secondary index can refer only to the appropriate logical record (unique user-defined key value) in the primary index. The transaction context must provide a value for the remaining key field in the primary index, i.e., the time value for which the record from the primary index is desired. For example, a secondary index might contain two versions due to an update two days ago, whereas the primary index might contain three versions due to an additional update of a non-indexed field only one day ago. A transaction might query the index as of four days ago and determine that the old index entries satisfies the query predicate; following the pointer from the secondary index into the primary index leads to all three version records, among which the transaction chooses the one valid four days ago. If most transactions require the most recent record version, and if forward scans are more efficient than backward scans, it might be useful to store this record first among all versions, i.e., to sort version records by decreasing start time as shown in Figure 7.9.

- Appending a version number to each key value and compressing neighboring records as much as possible turns B-trees into a version store efficient in space (storage) and time (query and update).

### 7.7 Summary

In summary, B-trees can solve a wide variety of indexing, data movement, and data placement problems. Not every issue requires changes in the index structure; very often, a carefully chosen key structure enables new functionality in B-tree indexes. A B-tree with a newly designed key structure retains the traditional operational benefits of B-trees, e.g., index creation by sorting, key range locking, physiological logging, and more. Thus, when new functionality is required, enabling this functionality by a new key structure for B-tree indexes may be easier than definition and implementation of a new index structure. 

Advanced key structures can be derived from user-defined keys in various ways. The preceding discussion includes adding artificial pre-fixes (partitioned B-trees) or suffixes (record versions), interleaving user keys with each other (UB-trees) or with artificial key components (merged indexes). While this list of alternative key enhancements might seem exhaustive, new key structures will probably be invented in the future in order to expand the power of indexing without mirroring the effort already spent on B-trees in form of research, development, and testing.

Obviously, many of the techniques discussed above can be combined. For example, a merged index can hold multiple complex objects by combining object or attribute identifiers with offsets within individual objects. Also, an artificial leading key field can be added to UB-trees or to merged indexes, thus combining efficient loading and incremental index optimization with multi-dimensional indexing or master-detail clustering. Similarly, merged indexes may contain (and thus cluster) not only traditional records (from various indexes) but also bitmaps or large fields. The opportunities for combinations seem endless.