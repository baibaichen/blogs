## 7-高级键结构

如前几节所示，为了完全支持数据库索引结构（如B-tree），需要大量的设计和编码。没有其他索引结构受到数据库研究人员和软件开发人员如此的关注。但是，通过仔细和创造性地构造B-tree的键，可以少量修改（如果有的话）核心B-tree代码，从而启用额外的索引功能。本节调查其中几个。

本节不考虑**计算列**的索引，即从同一表中的其他列派生的值，并在数据库表中给出名称。 这些列根据需要进行计算，无需存储在表的主数据结构中。 但是，如果此列上存在索引，则会在此二级索引中存储相应的键值。

类似地，本节不考虑**部分索引**，即基于**选择谓词**的索引，索引的数据记录少于基础表的数据记录。典型的谓词确保只有非空的值被索引。**计算列**和**部分索引**这两个主题，与B-tree中的高级键结构正交。

前面的讨论（第2.5节）给出了一个独特的B-tree键的示例，即哈希索引基于哈希值上的B-tree来实现。B-tree代码中的一些小调整，模拟了传统上与哈希索引相关的主要性能优点，即最坏情况下单个I/O、哈希目录内的直接地址计算、和高效的键值比较。将一个非常大的根页（非常类似于一个大的散列目录）固定在缓冲池中可以模拟第一个优势；另外两个好处可以通过适当的键值来模拟，包括最小规范化键。

- 具有高级键结构的B-tree保留了B-tree的所有优点，例如，键区间锁定的理论和实现、日志和恢复的优化，高性能的索引创建以及用于高效数据库操作的其他工具。
- 哈希值上的B-tree和传统哈希索引相比有许多优点，性能相当。

### 7.1 Multi-dimensional UB-trees 

By their nature, B-trees support only a single sort order. If multiple key columns are used, they may form a hierarchy of major sort key, minor sort key, etc. In this case, queries that restrict the leading key columns perform better than those that do not. If, however, the key columns represent dimensions in a space, e.g., a geometric space, queries may restrict the leading column only by a range predicate or not at all. Leslie et al. [82] describe sophisticated algorithms for accessing B-trees in those cases.

An alternative approach projects multi-dimensional data onto a sin-gle dimension based on space-filling curves. The principal tradeoffin the design of space-filling curves is conceptual and computational simplic-ity on one hand and preservation of locality and thus search efficiency on the other hand. The simplest construction of a space-filling curve first maps each dimension into an unsigned integer and then inter-leaves individual bits from these integers. When drawn as a line in a 2-dimensional space, this space-filling curve resembles nested *Z*shapes, which is why it is also called the *z*-order. This is the design underlying multi-dimensional indexing and query processing in Probe [106] and Transbase [109]. Alternatives to this Morton curve include the Hilbert curve and others.

> ==TODO：==Fig 7.1

Figure 7.1 (copied from [106]) illustrates *z*-order curves. The origi-nal *x*- and *y*-coordinates have 3 bits and therefore 8 distinct values; the interleaved *z*-values contain 6 bits and therefore 64 points. The “*Z*” shape is repeated at 3 scales. The technique applies to any number of dimensions, not only 2-dimensional spaces as shown in Figure 7.1.

UB-trees are B-tree indexes on such *z*-values. Each point and range query against the original dimensions is mapped to the appropriate intervals along the *z*-curve. The crucial component of this mapping is determining the *z*-values at which the *z*-curve enters and exits the multi-dimensional range defined by the query predicate. Known algo-rithms are linear in the number of original dimensions, their resolution (i.e., the number of bits in the *z*-values), and the number of entry and exit points [109].

In addition to points in multi-dimensional space, space-filling curves, *z*-order mappings, and UB-trees can index multi-dimensional rectangles (boxes) by treating start and end points as separate dimen-sions. In other words, UB-trees can index not only information about points but also about intervals and rectangles. Both space and time (including intervals) can be indexed in this way. For moving objects, location and speed (in each spatial dimension) can be treated as separate dimensions. Finally, even precision in location or speed can be indexed, if desired. Unfortunately, indexing based on space-filling curves loses effectiveness with the number of dimensions, just as the performance of traditional B-tree applications suffers when a B-tree contains many columns but only a few of them are specified in a query predicate.

- *Z*-values (or other space-filling curves) provide some multi-dimensional indexing with all the advantages of B-trees. 
-  The query performance of specialized multi-dimensional indexes is probably better, but load and update performance of B-trees are not easy to match. 

### 7.2 Partitioned B-trees

As discussed earlier in the section on bulk insertions and illustrated in Figure 6.6, the essence of partitioned B-trees [43] is to maintain partitions within a single B-tree, by means of an artificial leading key field. Partitions and the artificial leading key field are hidden from the database user. They exist in order to speed up large operations on B-trees, not to carry any information. Partitions are optimized using the merge step well known from external merge sort. By default, the same single value appears in all records in a B-tree, and most of the specific techniques rely on exploiting multiple alternative values, but only temporarily. If a table or index is represented in multiple B-trees, the artificial leading key field should be defined separately for each such B-tree.

The leading artificial key column effectively defines partitions within a single B-tree. Each existing distinct value implicitly defines a par-tition, and partitions appear and vanish automatically as records are inserted and deleted. The design differs from traditional horizontal par-titioning using a separate B-tree for each partition in an important way: Most advantages of the design depend on partitions (or distinct values in the leading artificial key column) being created and removed very dynamically. In a traditional implementation of partitioning (using multiple B-trees), creation or removal of a partition is a change of the table’s schema and catalog entries, which requires locks on the table’s schema or catalog entries and thus excludes concurrent or long-running user accesses to the table, as well as forcing recompilation of cached query and update plans. If partitions within a single B-tree are created and removed as easily as inserting and deleting rows, smooth contin-uous operation is relatively easy to achieve. It is surprising how many problems this simple technique can help address in data management software and its real-world usage.

**First**, it permits putting all runs in an external merge sort into a single B-tree (with the run number as the artificial leading key field), which in turn permits improvements to asynchronous read-ahead and to adaptive memory usage. In SAN and NAS environments, hid-ing latency by exploiting asynchronous read-ahead is important. With striped disks, forecasting multiple I/O operations is important. Finally, in very large online databases, the ability to dynamically grow and shrink resources dedicated to a single operation is very important, and the proposed changes permit doing so even to the extremes of pausing an operation altogether and of letting a single operation use a machine’s entire memory and entire set of processors during an otherwise idle batch window. While sorting is used to build B-tree indexes efficiently and B-trees are used to avoid the expense of sorting and to reduce the expense of searching during query processing, the mutually beneficial relationship between sorting and B-trees can go substantially further.

**Second**, partitioned B-trees can substantially reduce, by at least a factor of two, the wait time before a newly created index is available for query answering. While the initial form of an index does not perform as well as the final, fully optimized index or a traditional index, at least it is usable by queries and permits replacing table scans with index searches, resulting in better query response time as well as a smaller “locking footprint” and thus a reduced likelihood of deadlocks. Moreover, the index can be improved incrementally from its initial form to its final and fully optimized form, which is very similar to the final form after traditional index creation. Thus, the final index is extremely similar in performance to indexes created oﬄine or with traditional online methods; the main difference is cutting in half (or better) the delay between a decision to create a new index and its first beneficial impact on query processing.

**Third**, adding a large amount of data to a large, fully indexed data warehouse so far has created a dilemma between dropping and rebuilding all indexes or updating all indexes one record at a time, implying random insertions, poor performance, a large log volume, and a large incremental backup. Partitioned B-trees resolve this dilemma in most cases without special new data structures. A load operation simply appends a number of new partitions to each affected index; the size of these partitions is governed by the memory allocation for the in-memory run generation during the load operation. Updates (both insertion and deletions) can be appended to an existing B-tree in one or multiple new partitions, to be integrated into the main partition at the earliest convenient time, at which time deletions can be applied to the appropriate old records. Appending partitions is, of course, yet another variation on the theme of differential files [117]. Batched maintenance in a partitioned B-tree reduces the overall update time; in addition, it can improve the overall space requirements if pages of the main parti-tion are filled completely with compressed records; and it may reduce query execution times if the main partition remains unfragmented and its pages optimized for efficient search, e.g., interpolation search.

While a partitioned B-tree actually contains multiple partitions, any query must search all of them. It is unlikely (and probably not even per-mitted by the query syntax) that a user query limits itself to a subset of partitions or even a single one. On the other hand, a historic or “as of” query might map to a single partition even when newer partitions are already available. In general, however, all existing partitions must be searched. As partitioning is implemented with an artificial leading key field in an otherwise standard B-tree implementation, this is equivalent to a query failing to restrict the leading column in a traditional multi-column B-tree index. Efficient techniques for this situation are known and not discussed further here [82].

Partitions may remain as initially saved or they may be merged. Merging may be eager (e.g., merging as soon as the number of parti-tions reaches a threshold), opportunistic (e.g., merging whenever there is idle time), or lazy (e.g., merging key ranges required to answer actual queries). The latter is called adaptive merging [53]. Rather than merg-ing partitions in preparation of query processing, merging can be inte-grated into query execution, i.e., be a side effect of query execution. Thus, even if key ranges are left as parameters in query predicates, this technique merges only key ranges actually queried. All other key ranges remain in the initial partitions. No merge effort is spent on them yet they are ready for query execution and index optimization should the workload and its access pattern change over time.

- In partitioned B-trees, partitions are identified by an artificial leading key field. Partitions appear and disappear simply by insertion and deletion of B-tree entries with appropriate key values, without catalog updates. 
- Partitioned B-trees are useful for efficient sorting (e.g., deep read-ahead), index creation (e.g., early query processing), bulk insertion (append-only data capture with in-memory sorting), and bulk deletion (victim preparation). 
- Query performance equals that of traditional B-trees once all partitions have been merged, which is the default state. 

### 7.3 Merged Indexes

As others have observed, “optimization techniques that reduce the number of physical I/Os are generally more effective than those that improve the efficiency in performing the I/Os” [70]. It is a common belief that clustering related records requires pointers between records. An example relational database management system with record clus-tering is Starburst [20], which uses hidden pointers between related records and affects their automatic maintenance during insertions, dele-tions, and updates. The technique serves only tables and their primary storage structures, not secondary indexes, and it requires many-to-one relationships defined with foreign key integrity constraints.

The desirability of clustering secondary indexes is easily seen in a many-to-many relationship such as “enrollment” as many-to-many relationship between “courses” and “students.” In order to support table-to-table, index-to-index, and record-to-record navigation both from students to courses and from courses to students, the enrollment table requires at least two indexes, only one of which can be the primary index. For efficient data access in both directions, however, it would be desirable to cluster one enrollment index with student records and one enrollment index with course records.

Merged indexes [49] are B-trees that contain multiple traditional indexes and interleave their records based on a common sort order. In relational databases, merged indexes implement “master-detail clustering” of related records, e.g., orders and order details. Thus, merged indexes shift de-normalization from the logical level of tables and rows to the physical level of indexes and records, which is a more appropriate place for it. For object-oriented applications, clustering can reduce the I/O cost for joining rows in related tables to a fraction com-pared to traditional indexes, with additional beneficial effects on buffer pool requirements.

> ==TODO：==Fig 7.2

Figure 7.2 shows the sort order of records within such a B-tree. The sort order alone keeps related records co-located; no additional point-ers between records are needed. In its most limited form, master-detail clustering combines two secondary indexes, e.g., associating two lists of row identifiers with each key value. Alternatively, master-detail clus-tering may merge two primary indexes but not admit any secondary indexes. The design for merged indexes accommodates any combination of primary and secondary indexes in a single B-tree, thus enabling clus-tering of entire complex objects. Moreover, the set of tables, views, and indexes can evolve without restriction. The set of clustering columns can also evolve freely. A relational query processor can search and update index records just as in traditional indexes. With these abilities, the proposed design may finally bring general master-detail clustering to traditional databases together with its advantages in performance and cost.

In order to simplify design and implementation of merged indexes, a crucial first step is to separate implementation of the B-tree structure from its contents. One technique is to employ normalized keys, dis-cussed and illustrated earlier in Figure 3.4, such that the B-tree struc-ture manages only binary records and binary keys. In merged indexes, the mapping from multi-column keys to binary search keys in a B-tree is a bit more complex than in traditional indexes, in particular if adding and removing any index at any time is desired and if individual indexes may have different key columns. Thus, it is essential to design a flex-ible mapping from keys in the index to byte strings in the B-tree. A tag that indicates a key column’s domain and precedes the actual key fields, as shown in Figure 7.3, can easily achieve this. In other words, when constructing a normalized key for a merged index, domain tags and field values alternate up to and including the identifier for the index.

> ==TODO：==Fig 7.3

In practice, different than illustrated in Figure 7.3, a domain tag will be a small number, not a string. It is possible to combine the domain tag with the Null indicator (omitted in Figure 7.3) such that the desired sort order is achieved yet actual values are stored on byte boundaries. Similarly, the index identifier will be a number rather than a string.

Domain tags are not required for all fields in a B-tree record. They are needed only for key columns, and more specifically only for those leading key columns needed for clustering within the merged index. Following these leading key columns is a special tag and the identifier of the individual index to which the record belongs. For example, in Figure 7.3, there are only 2 domain tags for key values plus the index identifier. If there never will be any need to cluster on the line numbers in order details, only leading key fields up to order number require the domain tag. Thus, the per-record storage overhead for merged indexes is small and may indeed be hidden in the alignment of fields to word boundaries for fast in-memory processing. An overhead of 2–4 single-byte domain tags per record may prove typical in practice.

- Merging multiple indexes into a single B-tree provides master-detail clustering with all the advantages of B-trees. A single B-tree may contain any number of primary and sec-ondary indexes of any number of tables. 
- The B-tree key alternates domain tags and values up to and including the index identifier. 
- Merged indexes permit tables in traditional normal forms with the performance of free denormalization. 
- Merged indexes are particularly valuable in systems with deep storage hierarchies. 

### 7.4 列式存储

对于关系数据仓库，即席查询和数据挖掘可能因为找不到合适的索引，而采用扫描；而列式存储被认为是对大型扫描的性能增强。缺少索引可能是由于查询谓词中的复杂算术表达式，或不能接受索引更新和加载性能所致。列存储的基本思想是将关系数据库中的表按列存储，而不是基于行的传统格式，这样，不管页面是从磁盘还是从内存的缓存池获取，单列扫描可以充分页面内所有的数据字节。

如果每列按其包含的值排序，则必须使用**某种逻辑行标识符**标记值。 组装一条完整的记录需要连接操作，这可能太慢，且成本高。为了避免这种开销，表中的列必须以相同的顺序存储，因次没有一个索引确定它；这个顺序可以称为表中记录的顺序，并且B-tree可以使用几乎没有额外空间的标签实现列存储。

这些标签在很多方面类似于行标识符，但它们和传统的行标识符之间存在重要差异：标签值不是物理值，而是逻辑值。换言之，它们不==**捕获**==或表示物理地址，例如页标识符，并且无法从标签值计算页标识符。如果存在标签值和行地址之间相互映射的计算，则此计算必须假定可变长度列的最大长度。因此，在某些或所有垂直分区中会浪费存储空间，这与列式存储的目标（即非常快速的扫描）相矛盾。

由于大多数数据库管理系统的索引几乎都依赖于B-tree，因此重用和调整传统存储结构，主要意味着对B-tree的调整，包括调整其空间管理和对搜索键的依赖。为了确保行及其列在所有B-tree中以相同的顺序出现，所有索引中的搜索键必须相同。此外，为了实现这些目标，搜索键的存储需求必须几乎为零，这似乎有违直觉。

所需技术的本质非常简单。为行分配标签值，按它们添加到表中的顺序编号。请注意，标签值标识表中的行，而不是单个分区或单个索引中的记录。每个标签值在每个索引中精确地出现一次，即，它与表中每列的一个值配对。所有垂直分区都以B-tree格式存储，标签值为前导键。 重点是如何将该前导键的存储减少到几乎为零。

每个B-tree页面中的页头存储该页面上所有记录中的**最小标签值**。通过将该值与页面中记录的**槽号**相加，计算每条单独B-tree记录的实际标签值。B-tree记录中不需要单独存储标签值；每页只需要一个标签值。如果一个页面包含数十、数百甚至数千个B-tree记录，那么对于单独每条记录，存储**最小标签值**的开销实际上为零。如果**行标识符**（标签值）的大小为4或8字节，而B-tree节点的大小为8 KB，则每页行标识符的开销不超过0.1%。

如果页面中所有记录都具有连续的标记值，则此方法不仅可以解决存储问题，还可以将索引中特定键值的“搜索”减少为一点算术运算，然后直接访问所需的B-tree记录。 因此，这些B-tree的叶页中的访问性能甚至可以比使用插值搜索或哈希索引更好。

> ==TODO：==Fig 7.4

图7.4展示了一个表，它有2列3行，使用列存储。括号中的值表示行标识符或标签值。图的右边部分显示了两个磁盘页，每列一个。每页的列**header**（虚线）显示一个行计数和页中的最小的标签值。

到目前为止，这些考虑只涉及到B-tree的叶节点。当然，也需要考虑上层的索引页（分支节点）。幸运的是，它们只带来了适度的额外存储需求。分支节点中的存储需求由键值大小、指针大小和可变长度记录带来的开销决定。在这种情况下，键值大小等于**行标识符**的大小，通常为4或8字节。指针大小等于页标识符，通常也是4或8字节。管理可变长度记录的开销，虽然对于所考虑的B-tree索引不是严格需要的，但是对于字节偏移量和长度指示器通常是4字节。因此，每个分隔符记录的存储需求是8到20字节。例如，如果节点大小为8KB，平均利用率为70%，则B-tree的平均扇出为280到700。所以，B-tree的分支节点一起需要的磁盘空间小于或等于所有叶点磁盘空间的0.3%，这在实践中是可以忽略的。

与存储垂直分区的其他方案相比，这里所描述的方法允许在多个分区上以相同顺序高效地存储可变长度值。 因此，使用多路归并连接可以非常高效地组装表中的记录。此外，组装单条记录也非常有效，因为每个分区都在**行标识符**上建立索引。所有传统的B-tree索引优化都适用，例如**非常大的B-tree节点**、**插值搜索**等。注意，均匀数据分布之间的插值搜索几乎是瞬间的。

> ==TODO：==Fig 7.5

图7.5说明了用于列存储的B-tree的值，特别是如果列值由于压缩而改变大小，或者自身就是可变大小的。字符串是实际值；虚线框里是页头信息，表示该叶节点的记录计数和最小标签值。B-tree的上层分支节点表示其各自子树中最小的标签值。可以很容易管理有不同记录数的叶节点，通过查找标签来组装各条记录可以非常高效。根据键值的分布及其大小，可以进一步压缩，列式存储的关系数据库管理系统通常都会用到。

- 通过适当的压缩，使行程编码适应一连串的行标识符，列式存储可以基于B-tree。

### 7.5 Large Values

In addition to B-trees containing many records, each smaller than a single leaf page, B-trees can also represent large binary objects or byte strings with many bytes. In that case, the leaf nodes contain data bytes and the branch nodes contain sizes or offsets. The data bytes in the leaf nodes can be divided into records as in traditional B-tree indexes or they can be without any additional structure, i.e., byte strings. In the latter case, most or all size information is kept in the branch nodes. Sizes or offsets serve as separator keys within branch nodes. In order to minimize effort and scope of update operations, in particular insertion and deletion of individual bytes or of substrings, sizes and offsets are counted locally, i.e., within a node and its children, rather than globally within the entire large binary object.

> ==TODO：==Fig 7.6

Figure 7.6, adapted from [18, 19], illustrates these ideas. In this example, the total size of the object is 900 bytes. The tree nodes at the leaf level indicate byte ranges. The values are shown in the leaf nodes only for illustration here; instead, the leaf nodes should contain the actual data bytes and possibly a local count of valid bytes. The branch nodes of the tree indicate sizes and offsets within the large object. Key values in the left half of the figure and in the root node are fairly obvious. The most interesting entries in this tree are the key values in the right parent node. They indicate the count of valid bytes in their child nodes; they do not indicate the position of those bytes within the entire object. In order to determine absolute positions, one needs to add the key values from the root to the leaf. For example, the absolute position of the left-most byte in the right-most leaf node is 421 + 365 = 786.

Similarly, search may use binary search (or even interpolation search) within a node but must adjust for key values in upper nodes. For example, a root-to-leaf traversal in search of byte 698 may use a binary search in the right parent node but only after subtracting the key value in the root (421) from the search key (698), i.e., searching for key value 698 *−*421 = 277 within the right parent node and finding the interval between 192 and 365. With that leaf, local byte position 277 *−*192 = 85 corresponds to global byte position 698.

Insertion or deletion of some bytes in some leaf node affect only the branch nodes along one root-to-leaf path. For example, deletion of 10 bytes at position 30 reduces the values 120, 282, and 421 in Fig-ure 7.6. Although such a deletion changes the absolute positions of the data bytes in the right subtree, the right parent node and its children remain unchanged. Similarly, insertion or deletion of an entire leaf node and its data bytes affect only along a single root-to-leaf path. Main-tenance of the key values along the path can be part of the initial root-to-leaf traversals in search of the affected leaves or it can follow maintenance of the data bytes in the leaf nodes. All nodes can be kept 50–100% full using algorithms very similar to traditional B-trees. Aggressive load balancing among sibling nodes can delay node splits. A B-tree representing a large object enables such a merge-before-split policy more than a standard B-tree because a parent contains suffi-cient information to decide whether or not sibling leaves are promising candidates for load balancing.

- With relative byte offsets as key values, a B-tree can be adapted to store large objects spanning many pages, even permitting efficient insertions and deletions of byte ranges.

### 7.6 Record Versions

Many applications require notions of “transaction time” and “real-world time,” i.e., information about when a fact has been inserted into the database and when the fact is valid in the real world. Both notions of time enable what is sometimes called “time travel,” includ-ing “what result would this query have had yesterday?” and “what is known now about yesterday’s status?” Both types of queries and their results can have legal importance.^1^

> 1. Michael Carey used to explain the need for editing large objects in a database with the following play on US presidential politics of the 1970s: “Suppose you have an audio object representing a recorded phone conversation and you feel the need to erase 18 minutes in the middle of it . . . ” Playing on US presidential politics of the 1980s, one might say here: “What did the database know, and when did he know it?” 

Since most transactions in most applications require the most up-to-date state, one implementation technique updates database records in place and, if required for an old transaction, rolls back the data page using a second copy in the buffer pool. The rollback logic is very similar to that for transaction rollback, except that it is applied to a copy of the data page. Transaction rollback relies on the chain of log records for each transaction; efficient rollback of a data page requires a chain of log records pertaining to each data page, i.e., each log record contains a pointer to the prior log record of the same transaction and another pointer to the prior log record pertaining to the same data page.

An alternative design relies on multiple actual records per logical record, i.e., versions of records. Versioning might be applied to and managed in a table’s main data structure only, e.g., the primary index, or it can be managed in each data structure, i.e., each secondary index, each materialized view, etc. If a design imposes substantial overheads in terms of space or effort, the former choice may be more appropriate. For greatest simplicity and uniformity of data structures and algorithms, it seems desirable to reduce overheads such that versioning can be applied in each data structure, e.g., each B-tree index in a database.

> ==TODO：==Fig 7.7

Figure 7.7 illustrates how some designs for record versioning tag each version record with the version’s start time, its end time, and a pointer to the next record in the chain of versions. In the example, changing a single small field to reflect a worker’s increased hourly wage requires an entire new record with all fields and tags. In a secondary index with few fields in each index entry, three additional fields impose a high overhead. By an appropriate modification of B-tree keys, however, two of these three fields can be avoided. Moreover, new versions can require much less space than complete new copies of the versioned record.

Specifically, if the start time provides the least significant part of a B-tree key, all versions of the same logical record (with the same user-defined key value) are neighbors in the sequence of records. Pointers or a version chain are not required as the sequence of versions is simply the sequence of B-tree entries. End times can be omitted if one version’s start time is interpreted as the prior version’s end time. Upon deletion of a logical record, a ghost record is required with the appropriate start time. This ghost record must be protected as long as it carries information about the logical record’s history and final deletion.

> ==TODO：==Fig 7.8

Figure 7.8 illustrates the design. The record keys are underlined. Start times are the only additional required field in version records, avoiding 2 of 3 additional fields required by the simplest design for version records with timestamps and naturally ensuring the desired placement of version records.

Start times can be compressed by storing, in each B-tree leaf, a base time equal to the oldest record version within the page. In that case, start times are represented within each record by the difference from the base time, which hopefully is a small value. In other words, an additional key field appended to the B-tree key can enable record versioning with a small number of bytes, possibly even a single byte.

Moreover, record contents can be compressed by explicitly storing only the difference between a version and its predecessor. For fastest retrieval and assembly of the most recent version, version records should store the difference between a version and its successor. In this case, retrieval of an older version requires multiple records somewhat similar to “undo” of log records. Alternatively, actual log records could be used, leading to a design similar to the one based on rollback of pages but applied to individual records.

> ==TODO：==Fig 7.9

Figure 7.9 illustrates these techniques. A field in the page header indicates the time of the oldest version record currently on the page. The individual records store the difference from this base time rather than a complete timestamp. Moreover, unchanged field values are not repeated. The order of version records is such that the current record is most readily available and older versions can be constructed by a local scan in forward direction. In the diagram, the absolute value of the prior value is shown, although for many data types, a relative value could be used, e.g., “*−*$3.” Further optimizations and compression, e.g., prefix truncation, may be employed as appropriate.

If “time travel” within a database can be limited, for example to one year into the past, all version records older than this interval can be interpreted as ghost records. Therefore, they are subject to removal and space reclamation just like traditional ghost records, with all rules and optimizations for locking and logging during ghost removal. When all other versions for a logical record have been thus removed, the ghost record indicating deletion of a logical record can also be removed and its space can be reclaimed.

If versioning in secondary indexes is independent from versioning in the table’s primary index, pointers in a secondary index can refer only to the appropriate logical record (unique user-defined key value) in the primary index. The transaction context must provide a value for the remaining key field in the primary index, i.e., the time value for which the record from the primary index is desired. For example, a secondary index might contain two versions due to an update two days ago, whereas the primary index might contain three versions due to an additional update of a non-indexed field only one day ago. A transaction might query the index as of four days ago and determine that the old index entries satisfies the query predicate; following the pointer from the secondary index into the primary index leads to all three version records, among which the transaction chooses the one valid four days ago. If most transactions require the most recent record version, and if forward scans are more efficient than backward scans, it might be useful to store this record first among all versions, i.e., to sort version records by decreasing start time as shown in Figure 7.9.

- Appending a version number to each key value and compressing neighboring records as much as possible turns B-trees into a version store efficient in space (storage) and time (query and update).

### 7.7 Summary

In summary, B-trees can solve a wide variety of indexing, data movement, and data placement problems. Not every issue requires changes in the index structure; very often, a carefully chosen key structure enables new functionality in B-tree indexes. A B-tree with a newly designed key structure retains the traditional operational benefits of B-trees, e.g., index creation by sorting, key range locking, physiological logging, and more. Thus, when new functionality is required, enabling this functionality by a new key structure for B-tree indexes may be easier than definition and implementation of a new index structure. 

Advanced key structures can be derived from user-defined keys in various ways. The preceding discussion includes adding artificial pre-fixes (partitioned B-trees) or suffixes (record versions), interleaving user keys with each other (UB-trees) or with artificial key components (merged indexes). While this list of alternative key enhancements might seem exhaustive, new key structures will probably be invented in the future in order to expand the power of indexing without mirroring the effort already spent on B-trees in form of research, development, and testing.

Obviously, many of the techniques discussed above can be combined. For example, a merged index can hold multiple complex objects by combining object or attribute identifiers with offsets within individual objects. Also, an artificial leading key field can be added to UB-trees or to merged indexes, thus combining efficient loading and incremental index optimization with multi-dimensional indexing or master-detail clustering. Similarly, merged indexes may contain (and thus cluster) not only traditional records (from various indexes) but also bitmaps or large fields. The opportunities for combinations seem endless.