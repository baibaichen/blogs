## 4-事务技术

上节是关于如何优化B-tree数据结构及其算法的综述，本节关注B-tree的『并发控制和恢复相关』的技术。现实系统中，磁盘数据结构（主要是B-tree）的『绝大部分开发和测试工作』主要是并发控制和恢复。本节和随后几节描述的几个技术，使得『传统的数据库管理系统』与『现今各种Web服务使用的Key-Value存储系统』显著不同[Chang et al. 2008, DeCandia et al. 2007]。

本节描述的B-tree不仅支持只读搜索，同时也支持并发修改：即插入、删除、以及修改已存在的记录（包括键值和普通字段）。我们关注的是即时修改，而非是**差分文件**这类使用延迟更改的技术。下一节描述更新计划（update plans），用于在『成熟的数据库管理系统』中维护相关的多个索引、物化视图、完整性约束等。

由于在众多用户和应用程序之间共享数据，就能说明建立数据库的复杂性和高代价，因此从一开始，在并发事务和执行线程之间访问共享数据库，就和高可用性以及『快速可靠地从故障软硬件中恢复』一样，处在数据库研发的最前沿。最近，多核处理器使得高度并发的内存数据结构获得更多的关注，事务内存可能是一种解决方案，但这要求正确地理解事务的边界，因此需要选择一致的中间状态。

除了并发用户之外，还存在一种使用异步工具的趋势，它们在持久性存储上执行可选或是强制性任务。一个常见的例子是：从数据库删除表或是索引后，并不是马上将它们的页面加到空闲空间中去，而只是简单地把它们标记为过期，这就要有强制性异步任务来合并空闲空间。『可选异步任务』的常见例子是碎片整理，即为了高效的『区间查询和索引序扫描』，而在B-tree叶子之间平衡负载和优化磁盘布局。还有其它许多异步任务，并不特定属于B-tree，比如收集和更新『用于编译时优化查询』的统计信息。

> ==TODO：==Fig.4.1

图4.1列出了事务的“ACID”四个属性及其简要的解释，这些属性的细节在数据库教科书中有更加深入地论述。这里值得通过一个具体的例子，进一步澄清原子性中“逻辑”一词的含义：假设用户事务尝试在数据库的表中插入一条新记录，为了获得足够的空间，不得不分裂B-tree的节点，但是节点分裂后事务就失败了；事务回滚时，必须撤销插入操作，不过为了确保数据库的内容正确，并不需要严格地要求合并分裂的节点。如果分裂节点的效果仍然保留在数据库中，即使事务回滚后存在“物理变化”，但逻辑上数据库并没有改变。这里“逻辑”是根据“查询结果”来定义；“物理”是根据『磁盘中的位』这样的物理表示来定义。

逻辑数据库和物理数据库之间的不同，或是数据库内容和数据库表示之间的不同，渗透到下面的讨论。一个特别有用的概念是“系统事务”，即对数据库内容没有影响，但在数据表示层面有变化的事务（这类事务仍会在数据表示层面修改，生成日志和提交改变）。对于B-tree的节点分裂、空间分配和回收等，系统事务特别有用。在上面的例子中，当用户事务回滚时，分裂后的节点（由系统事务提交）仍然原地保留。系统事务一般都很简单，并运行在单个线程中，通常是在执行用户事务的线程中，这样用户事务将会等待系统事务完成。如果用户事务运行在多个线程中，那么每个线程可以调用自己的系统事务。

如果将大表或索引分区，并将分区指派到分布式系统中多个节点中，那么通常都是在节点内执行各自的并发控制，在需要的时候，通过两阶段提交来协调恢复。如果在单个站点内使用多个恢复日志，那么要用到同样的技术。本文是关于B-tree索引的综述，分布式事务、两阶段提交等超出了本书的范围。

> ==TODO：==Fig.4.2

并发控制的常用机制就是锁。基本的锁模式，即——共享锁（S）、独占锁（X）和更新锁（U）——之间的兼容性矩阵，如图4.2所示。左列表示当前持有的锁，顶行表示当前请求的锁，矩阵中的空格表示不能授予请求的锁。两个共享锁是兼容的，显然这就是共享的要点；反之独占锁和其它类型的锁都不兼容。共享锁也被称之为读锁，独占锁则被被称之为写锁。

对于允许的锁请求，这个兼容矩阵显示了（锁请求成功后）的聚合模式，目的是为了加速处理新的锁请求，即使有许多事务持有某个特定资源的锁，新的锁请求也只需检查是否与聚合后的锁模式兼容，而不必检查是否与『该资源上所有的』锁模式兼容。换言之，图4.2中左列表示（资源）当前的锁模式，就是当前的聚合模式。大多数情况下，图4.2中锁的聚合模式都不重要。后续有一个例子，包含了更多锁模式，新的聚合模式即不等于老的聚合模式，也不等于请求的锁模式。图4.2没有反应的一个特殊情况是将锁从更新模式降级为共享模式，由于只有一个事务能持有更新锁，从更新模式降级为共享模式后，锁的聚合模式是共享锁。

应用程序有时需要先测试一个条件，再决定是否更新数据记录，这就需要用到更新锁。如果一开始就采用独占锁，这阻止了处理同样逻辑的其它事务；如果采用共享锁，倒是允许两个事务同时锁定共享的数据，但是当它们都试图升级成独占锁时，将会导致死锁！更新锁一次只允许一个事务处于此种状态：即不清楚自己未来将执行何种操作。完成条件测试后，更新锁要么确实升级成独占锁，要么降级成共享锁。注意，更新锁并不比共享锁有更多的权利，它们之间的不同主要是在调度和死锁预防，而不是在并发控制和访问数据的权利。这样，降级到共享锁是允许的。

更新锁也被称之为升级锁。考虑到更新锁并不是授予锁更新数据的权利，而是锁升级的优先级，因此“升级锁”是一个更加准确的名字，然而，更新锁似乎变得越来越常用。Korth在[Korth 1983]中深入考察了派生的锁模式（比如升级锁）和基本的锁模式（比如共享锁和独占锁）之间的关系。

注意图23中那个带问号的字段，当事务持有（某个资源的）更新锁时，有些系统允许新的事务请求（该资源的）共享锁，有些则不允许。前面的处理方式，仅当事务请求互斥锁时，才不再允许额外的共享锁请求；最可能的是（但并不是百分之百）『持有更新锁的事务』请求互斥锁。因此，即使后面的方案在锁矩阵中引入了不对称性，但它能更有效地预防死锁[Gray and Reuter1993]。后面锁矩阵的例子都假设这种不对称的设计。

出故障时『保证原子性和持久性』的主要手段是先写日志，这就要求在『就地修改数据库』之前，先在恢复日志中记录『数据库的变化』。因此，每种类型的“修改”操作需要：一个“do”方法，在初始处理时调用；一个“redo”方法，在出现故障，或是数据库崩溃时，确保数据库处于“修改”后的状态；一个“undo”方法，将数据库恢复至“修改”之前的状态。由“do”方法创建『包含足够信息的日志记录』，以便调用“redo”和“undo”；并指示缓冲池保留这些脏页，直到对应的日志安全地到达“稳定的存储”。因为稳定的存储是可靠的，所以恢复也是可靠的。镜像日志设备是一个通用的技术。日志页一旦生成，就决不能被修改或是覆盖[1]。

早期的恢复技术要求“redo”和“undo”是幂等的[56]，即不管『应用同样的操作多少次』，结果都一样。一个潜在的假设就是，恢复日志在恢复时是只读的，也就是从故障中恢复时，不生成日志。后来的技术，尤其是ARIES[95]，通过为“undo”操作生成恢复日志，并在数据页上保存“Page LSN”（日志序列号）以指明『该页所包含的最近的更新』，来确保只会应用“redo”和“undo”操作一次。此外，逻辑“补偿”，而非物理“补偿”更新操作，比如，删除补偿插入，然而，如果在『B-tree索引的叶节点分裂』之后再删除，那么删除的叶节点和插入的叶节点可能并不相同。中止事务只需生成更新的“补偿记录”，然后正常提交即可，唯一不同的是，此时无需强迫刷新提交记录到稳定存储。

- ACID四个属性（原子性、一致性、隔离性和持久性）定义了事务。先写日志和“do-redo-undo”三人组是恢复和可靠性的基础。Latching和Locking是并发控制的基础
- 在B-tree中，记录级的锁定是指『key-value locking』和『key-range locking』。锁的粒度（即，记录级或是Key级）小于恢复的粒度（即，页级）需要在日志中记录“undo”操作，在恢复时实施逻辑补偿，而不是通过调用『无需在日志中记录的』幂等操作，来实施严格的物理恢复
- 独立物理数据，区分数据库的**逻辑内容**和**物理表示**。这样，在数据库系统的关系层，数据库的物理设计是独立的，且必须自动优化查询；在数据库的存储层，使得『并发控制和恢复的实现』可以有很多优化。
- 一个重要的优化是区分用户事务和系统事务：用户事务『查询或者修改数据库的逻辑内容』；系统事务『则只影响逻辑内容的物理表示』。在系统事务中分裂B-tree的节点，是其优点的一个经典例子。

### 4.1Latching and Locking

### 4.2 幻影记录

如果在事务中删除B-tree记录，那么在提交事务提前，必须保留回滚事务的能力。因此，必须确保在回滚事务时，空间分配不会失败，并且另一个事务不能插入有相同唯一B-tree键的新记录。满足这些要求的一种简单技术是将记录及记录的键保留在B-tree中，仅将其标记为无效；在提交删除记录的事务之前，一直锁定它们（记录及记录的键）。另一个好处是，用户事务可以延迟处理甚至避免一些空间管理工作，例如，移动页面间接数组中的元素。此外，用户事务只需锁定被删除的记录，而不是记录两个相邻键之间的整个键区间。

结果记录称为伪删除记录或**幻影记录**。记录头中的一个位足以指示记录的幻影状态。因此，**<u>删除变成了对幻影位的修改</u>**。如果并发控制依赖于**键区间锁定**（下面讨论），则只需要锁定键本身，并且键之间的所有间隙可以保持未锁定状态。

> ==TODO：==Fig.4.4

图4.4显示了一个带有幻影记录的B-tree页面，即在删除键为27的记录后的中间状态。显然，这是在页面内的移除幻影和回收空间之前。有效记录包含一些与其键相关联的信息，用省略号表示，而幻影记录中的信息字段可能会保留，但没有意义。回收空间的第一步可以尽可能地缩短这些区域，尽管首选方法可能是完全删除幻影记录。

查询必须忽略（跳过）幻影记录；因此，扫描带幻影记录的系统始终含有隐藏的谓词，尽管该谓词的计算内置在B-tree代码中，而无需谓词解释器。回收空间留给后续事务，可能是一个插入（需要比页面中已有空闲空间更多的空闲空间）、显式调用页面压缩或B-tree碎片整理实用工具。

幻影记录被锁定时无法删除。换言之，幻影记录至少在原地被保留到（删除它的）事务提交，将有效记录变成幻影记录。随后，另一个事务可能会锁定一个幻影记录，例如，以确保继续缺少该键值。锁定不存在的键值对于可串行化至关重要；没有它，在同一事务中重复查询 `select count(*)` 可能会返回不同的结果。

同一页中可能同时存在多个幻影记录，一个系统事务就可以删除所有这些记录。将删除幻影的日志记录与提交事务的日志记录合并，则无需在日志中记录已删除记录的内容。==<u>合并这些日志记录使事务不可能在幻影删除和提交之间失败</u>==。因此，永远不需要回滚幻影删除，从而恢复日志中的记录内容。换句话说，幻影记录不仅可以确保在需要时成功地回滚事务，而且还经常减少与删除相关的日志总量。

> ==TODO：==Fig.4.5

图4.5说明了没有和使用幻影记录时，删除事务的日志。在左侧，用户事务删除记录并记录其全部内容。如果需要，可以使用恢复日志中的信息重新插入记录。在右侧，用户事务只修改幻影位。稍后，系统事务会创建一条日志记录，包含事务启动、幻影删除和事务提交。无法从恢复日志中重新插入已删除的幻影记录，但这没有必要，因为此时删除操作已经提交了。

如果使用与B-tree中幻影记录相同的键插入新行，则可以重用旧记录。因此，插入可能会变成对幻影位的修改，在大多数情况下，还会修改记录中其它一些除了键之外的字段。==<u>与删除时一样，键区间锁定只需要锁定键值，而不需要锁定插入新键的键区间</u>==。

虽然幻影记录通常与B-tree中的记录删除相关，但它们也可以帮助插入新键。将插入拆分为两步可以减少事务所需的锁。首先，在**latch**的保护下，用所需的键创建一个重幻记录，此步不需要锁。第二，用户事务根据需要锁定和修改新记录。如果用户事务失败并回滚，则保留幻影记录。第二步需要锁定键值，而不是插入新键的键值区间。

这个想法的另一个改进是，用将来可能插入的键创建多个幻影记录。如果将来插入的键完全可以预测，例如订单号和发票号，则这一点尤其有用。即使无法预测将要插入的键的精确值，这种幻影记录也可以有助于分离后续的插入，从而在后来的插入事务中实现更多的并发性。例如，键可以由多个字段组成，但只能对前导字段预测值，例如，每个订单中的订单号和行号。

> ==TODO：==Fig.4.6

图4.6说明了插入多个幻影记录。插入键值为11、12、13和14的有效记录后，下一个操作可能是插入键值为15、16、17等的记录。由于已经填写了这些键，在空间上预先分配了适当的空间，这样就可以提高这些插入的性能。用户事务节省了分配工作，只锁定键值，既不锁定键值之间的间隙，也不锁定现有最大键值与无穷大之间的间隙，这通常是此类**插入事务序列**的瓶颈。

最后，在有效的B-tree记录中，插入仅包含键、而不包含任何剩余字段的非常短的幻影记录是有益的。将这样的“幻影插槽”==**撒**==到有序记录序列（或间接数组中的槽）中可以实现页面内的有效地插入。在没有这种幻影插槽的页面中，插入需要移动所有条目的一半，比如间接数组中的插槽。在具有幻影插槽的页面中，插入的复杂性不是O (N)而是O (log N)[12]。例如，在每页有数千个小记录的二级索引中，插入需要在间接数组中移动十个而不是几千个元素，删除操作根本不会移动任何元素，只会留下一个幻影插槽，而页面重组则会留下大约10%或20%的幻影插槽。

> ==TODO：==Fig.4.7

图4.7是图3.3的细化，显示了两个差异。第一，间接数组中的元素包含键或键前缀。图中显示了字母，但真正实现时将使用最小规范化键，将它们解释为整数值。第二，其中一个是幻影插槽，因为它包含一个没有引用记录的键（“d”）。这个插槽可以参与二叉搜索和键区间锁定。它可能在页面重组期间被放在那，或者可能是快速删除记录的结果，而没有移动键“g”和“k”的两个槽。一旦存在，它就可以加速插入。例如，插入带有“e”键的新记录可以简单地修改当前包含“d”的槽。当然，这要求当前未锁定键“d”，或者锁管理器允许适当的调整。

- 幻影记录（也称为伪删除记录）通常用于减少删除期间的锁定需要，并简化删除的“撤消”。
- 幻影记录不影响查询结果，但参与键区间锁定。
- 可以在插入时或异步清理时，回收幻影记录或其空间，但前提是它未被锁定。
- 幻影记录也可以加速和简化插入。

### 4.3 键区间锁定

### 4.4 Key Range Locking at Leaf Boundaries

### 4.5 Key Range Locking of Separator Keys

> **In most commercial database systems, the granularities of locking are an entire index, an index leaf (page), or an individual key (with the sub-hierarchy of key value and open interval between keys, as discussed above).** Locking both physical pages and logical key ranges can be confusing, in particular when page splits, defragmentation, etc. must be considered. ==An alternative model relies on key range locking for separator keys in the B-tree level immediately above the leaves [48].== This is different from locking fence keys at the level of B-tree leaves, even if the same key values are used. The scope of each such lock is similar to a page lock, but locks on separator keys are predicate locks in the same way as key range locks in B-tree leaves. <u>==Lock management during splits of leaf pages can rely on the intermediate states of B^link^-trees or by copying the locks from one separator key to a newly posted separator.</u>==

在大多数商业数据库系统中，锁的粒度是一个完整的索引、一个索引叶（叶节点）或一个单独的键（如上所述，具有键值的**子层次结构**和键之间的**开区间**）。锁定物理页和逻辑键区间可能会令人困惑，特别是在必须考虑页分裂、碎片整理等时。另一种模型依赖<u>在叶节点直接上层的B-tree节点中的分隔键</u>的**键区间锁定**[48]。这与在B-tree叶节点锁定**fence键**不同，就算是使用相同的键值。每个此类锁的范围类似于页锁，但分隔键上的锁是**谓词锁**，其方式与B-tree叶节点中的键区间锁相同。<u>==叶节点分裂期间的锁管理可以依赖于 B^link^-tree的中间状态，或者通过将锁从一个分隔键复制到新建的分隔键</u>==。

> Very large database tables and their indexes, however, may require millions of leaf pages, forcing many transactions to acquire many thousands of locks or lock much more data than they access. Lock hierarchies with intermediate levels between an index lock and a page lock have been proposed, although not yet used in commercial systems.

但是，非常大的数据库表及其索引可能需要数百万个叶页，这迫使许多事务获取数千个锁，或锁定比它们访问的数据要多的多的数据。虽然还没有在商业系统中使用，但是已经提出了==**索引锁**==和==**页锁**==之间具有中间级别的锁层次结构。

> One such **proposal** [48] employs the B-tree structure, ==**adding key range locking on separator keys in upper B-tree levels to key range locking on leaf keys**==. In this proposal, the lock identifier includes not only a key value but also the level in the B-tree index (e.g., level 0 are leaves). This technique **promises to** adapt naturally to skewed key distributions just like the set of separator keys also adapts to the actual key distribution.

其中一个**方案**[48]利用B-tree结构，将叶节点上一层的B-tree分隔键上的**键区间锁定**添加到叶节点键上的**键区间锁定**。在该方案中，锁标识符不仅包括键值，还包括B-tree索引中的ç（例如，层0是叶节点）。这种技术可以自然地适应键的倾斜分布，就像分隔键集也可以适应键的实际分布一样。

> Another proposal [48] focuses on the B-tree keys, deriving granularities of locking from compound (i.e., multi-column) keys such as “last name, first name.” **The advantage of this method is that it promises to match predicates in queries and database applications, such that it may minimize the number of locks required.** Tandem’s “generic locking” is a rigid form of this, using a fixed number of leading bytes in the key to define ranges for key range locking.

另一个方案[48侧重于B-tree的键，从复合键（即多列键）如“姓氏，名字”中派生出锁的粒度。这种方法的优点是它承诺在查询和数据库应用程序中匹配谓词，以便它可以最小化所需锁的数量。Tandem的“通用锁定”是一种严格的形式，使用键中固定数量的前导字节来定义键区间锁定的范围。

> > Saracco and Bontempo [113] describe Tandem’s generic locking as follows: “*In addition to the ability to lock a row or a table’s partition, NonStop SQL/MP supports the notion of generic locks for key-sequenced tables. Generic locks typically affect multiple rows within a certain key range. The number of affected rows might be less than, equal to, or more than a single page. When creating a table, a database designer can specify a “lock length” parameter to be applied to the primary key. This parameter determines the table’s finest level of lock granularity. Imagine an insurance policy table with a 10-character ID column as its primary key. If a value of “3” was set for the lock length parameter, the system would lock all rows whose first three bytes of the ID column matched the user-defined search argument in the query.*” Note that Gray and Reuter [1993] explain key-range locking as locking a key prefix, not necessarily entire keys.

> Saracco和Bontempo [113]描述了Tandem的通用锁定如下：“**除了能够锁定行或表分区之外，NonStop SQL/MP还支持<u>键顺序表的通用锁</u>的概念。 通用锁通常会影响某个键范围内的多个行。受影响的行数可能小于、等于或大于单页。建表时，数据库设计者可以指定要应用于主键的“锁定长度”参数。 此参数确定表的最佳锁定粒度。假设，一个保险策略表以10个字符的ID列为主键。 如果为锁定长度参数设置为“3”，系统锁定的所有行是：其ID列的前三个字节匹配查询中用户定义的搜索参数。**”注意，Gray和Reuter[1993]将键区间锁定解释为锁定键的前缀，不一定是整个键。

> Both proposals for locks on large key ranges need many details worked out, many of which will become apparent only during a first industrial-strength implementation. A variant of this method [4] has been employed in XML storage where node identifiers follow a hierarchical scheme such that an ancestor’s identifier is always a prefix of its descendents.

**大范围锁定键值区间**的两个方案需要制定许多细节，在第一次出现工业强度的实现时，才会显现出其中许多细节。 该方法的变体[4]已被用于XML存储中，其中节点标识符遵循分层方案，使得祖先的标识符始终是其后代的前缀。

- *Large indexes require an intermediate granularity of locking between locking a key value and locking an entire index.*大型索引需要在锁定键值和锁定整个索引之间的**中等粒度锁**。
- *Traditional designs include locks on leaf pages in addition to (or instead of) locking key values. The number of pages in an index and thus the number of page locks in a query may far exceed the threshold at which the lock manager escalates to a larger granularity of locking, which is usually a few thousand locks.*除了（或代替）锁定键值之外，传统设计还包括叶节点上的锁。索引中的页节点数量，以及查询中页节点的锁定数量，可能远远超过锁管理器升级到更粗粒度锁的阈值（通常是几千）。
- *Alternatively, key range locking can be applied to separator keys in some or all branch nodes in a B-tree. This design adapts traditional hierarchical locking to B-trees and their organization in levels.*或者，**键区间锁定**可以应用于B-tree中某些或所有分支节点中的分隔键。该设计将传统的分层锁与B-tree及其层次结构相适应。

### 4.6 B^link^-trees

In the original design for B-trees, splitting an overflowing node updates at least three nodes: the overflowing node, the newly allocated node, and their parent node. In the worst case, multiple ancestors must be split. Preventing other threads or transactions from reading or even updating a data structure with incomplete updates requires latches on all affected nodes. A single thread holding latches on many B-tree nodes obviously restricts concurrency, scalability, and thus system performance. Rather than weakening the separation of threads and thus risking inconsistent B-trees, the definition of correct B-trees requires some relaxation. One such design divides a node split into two independent steps, i.e., splitting the nodes and posting a new separator key in the parent node. After the first step, the overflowing node requires a separator key and a pointer to its right neighbor, thus the name B^link^-trees [81].

Until the second step, the right neighbor is not yet referenced in the node’s parent. In other words, a single key range in the parent node and its associated child pointer really refer to two child nodes. A root-to-leaf search, upon following this pointer, must first compare the sought key with the child node’s high fence and proceed to the right neighbor if the sought key is higher. In order to ensure efficient, logarithmic search behavior, this state is only transient and ends at the first opportunity. 

The first step of splitting a node defines the separator key, creates a new right neighbor node, ensures correct fence keys in both nodes, and retains the high fence key of the new node also in the old node. The last action is not required for correct searching in the B-tree but it enables efficient consistency checks of a B-tree even with some nodes in this transient state. In this transient state, the old node could be called a “foster parent” of the new node. 

The second, independent step posts the separator key in the parent. The second step can be made a side effect of any future root-to-leaf traversal, should happen as soon as possible, yet may be delayed beyond a system reboot or even a crash and its recovery without data loss or inconsistency of the on-disk data structures (see Figure 6.11 for more details on the permissible states and invariants).

The advantage of Blink-trees is that allocation of a new node and its initial introduction into the B-tree is a local step, affecting only one preexisting node and requiring a latch only on the overflowing node. The disadvantages are that search may be a bit less efficient during the transient state, a solution is needed to prevent long lists of neighbors nodes during periods of high insertion rates, and verification of a B-tree’s structural consistency is more complex and perhaps less efficient.

> ==TODO：==Fig.4.12

Figure 4.12 illustrates a state that is not possible in a standard B-tree but is a correct intermediate state in a Blink-tree. “Correct” here means that search and update algorithms must cope with this state and that a database utility that verifies correct on-disk data structures must not report an error. In the original state, the parent node has three children. Note that these children might be leaves or branch nodes, and the parent might be the B-tree root or a branch node. The first step is to split a child, resulting in the intermediate state shown in Figure 4.12. The second step later places a fourth child pointer into the parent and abandons the neighbor pointer, unless neighbor pointers are required in a specific implementation of B-trees. Note the similarity to a ternary node in a 2-3-tree as shown in Figure 2.2.

In most cases, posting the separator key in the parent node (the second step above) can be a very fast system transaction invoked by the next root-to-leaf traversal. It is not required that this thread be part of an update transaction, because any changes in the B-tree structure will be part of the system transaction, not the user transaction. When a thread holds latches on both parent and child node, it can check for the presence of a separator key not yet posted. If so, it upgrades its latches to exclusive latches, allocates a new entry in the parent node, and moves the separator key from the child to the parent. If another thread holds a shared latch, the operation is abandoned and left to a subsequent root-to-leaf search. If the parent node cannot accommodate another separator key, a new overflow node is allocated, populated, and linked into the parent. Splitting the parent node should be a separate system transaction. If a root-to-leaf search finds that the root node has a linked overflow node, the tree should grow by another level. If any of the required latches cannot be acquired instantaneously, the system transaction may abort and leave it to a later B-tree traversal to post the separator key in the parent node.

In the unlikely event that a node must be split again before a separator key is posted in the parent node, multiple overflow nodes can form a linked list. Long linked lists due to multiple splits can be pre-vented by restricting the split operation to nodes pointed to by the appropriate parent node. These and further details of B^link^-trees have recently been described in a detailed paper [73].

The split process of B^link^-trees can be reversed in order to enable removal of B-tree nodes [88]. The first step creates a neighbor pointer and removes the child pointer from the parent node, whereupon the second step merges the removal victim with its neighbor node. The transient state of B^link^-trees might even be useful for load balancing among sibling nodes and for defragmentation of B-trees, although this idea has not been tried in research prototypes or industrial implementations.

- B^link^-trees relax the strict B-tree structure in order to enable more concurrency. Splitting a node and posting a new separator key in the parent are two separate steps.
- Each step can be a system transaction that commits to make its changes visible to other threads and other transactions. 
- In the transient state between these two steps, the old node is a “foster parent” to the new node. The transient state should be short-lived but may persist if the second step is delayed, e.g., due to concurrency conflicts. 
- B^link^-trees and their transient state may be useful for other structural changes in B-trees, e.g., removal of a node (merging the key ranges of two nodes) and load balancing among two nodes (replacing the separator key). 

### 4.7 Latches During Lock Acquisition

If locks are defined by actual key values in a B-tree, latches must be managed carefully. Specifically, while a transaction attempts to acquire a key range lock, its thread must hold a latch on the data structure in the buffer pool such that the key value cannot be removed by another thread. On the other hand, if the lock cannot be granted immediately, the thread should not hold a latch while the transaction waits. In fact, the statement could be more general: a thread must never wait while holding a latch. Otherwise, multiple threads may deadlock each other. Recall that deadlock detection and resolution is usually provided only for locks but not for latches.

There are several designs to address this potential problem. In one solution, the lock manager invocation, upon detecting a conflict, only queues a lock request and then returns. This lets the thread release appropriate latches prior to invoking the lock manager a second time and waiting for the lock to become available. It is not sufficient to merely fail the lock request in the first invocation. Until the lock request is inserted into the lock manager’s data structures, the latch on the data structure in the buffer pool is required to ensure the existence of the key value.

Another solution passes a function and appropriate function arguments to the lock manager to be called prior to waiting. This call-back function may release the latch on the data structure in the buffer pool, to be re-acquired after the wait and the lock is acquired. In either case, the sequence of actions during the lock request needs to indicate not only success versus failure but also instant versus delayed success.

While a transaction waits for a key value lock without holding the latch on the data structure in the buffer pool, other transactions might change the B-tree structure with splits, merges, load balancing, or page movements, e.g., during B-tree defragmentation or in a write-optimized B-tree on RAID or flash storage [44]. Thus, after waiting for a key value lock, a transaction must repeat its root-to-leaf search for the key. In order to minimize the cost for this repeated search, log sequence numbers of pages along the root-to-leaf path might be retained prior to the wait and verified after the wait. Alternatively, counters (of structure modifications) might be employed to decide quickly whether or not the B-tree structure might have changed [88]. These counters can be part of the system state, i.e., not part of the database state, and there is no need to recover their prior values after a system crash.

- A data page must remain latched while a key value lock is acquired in order to protect the key value from removal, but the latch on the data page must not be retained while waiting for a lock. 
- Solutions require a call-back or repeated lock manager calls, one to insert the lock into the wait queue and one to wait for lock acquisition. 

### 4.8 Latch Coupling

When a root-to-leaf traversal advances from one B-tree node to one of its children, there is a brief window of vulnerability between reading a pointer value (the page identifier of the child node) and accessing the child node. In the worst case, another thread deletes the child page from the B-tree during that time and perhaps even starts using the page in another B-tree. The probability is low if the child page is present in the buffer pool, but it cannot be ignored. If ignored or not implemented correctly, identifying this vulnerability as the cause for a corrupted database is very difficult. Additional considerations apply if the child page is not present in the buffer pool and I/O is required, which is discussed in the next sub-section.

A technique called latch coupling avoids this problem. The root-to-leaf search retains the latch on the parent page, thus protecting the page from updates, until it has acquired a latch on the child page. Once the child page is located, pinned, and latched in the buffer pool, the latch on the parent page is released. If the child page is readily available in the buffer pool, latches on parent and child pages overlap only for a very short period of time.

Latch coupling was invented fairly early in the history of B-trees [9]. For read-only queries, at most two nodes need to be locked (latched) at a time, both in shared mode. In the original design for insertions, exclusive locks are retained on all nodes along a root-to-leaf path until a node is found with sufficient free space to permit splitting a child and posting a separator key. Unfortunately, variable-size records and keys may force very conservative decisions. Instead, newer designs rely on Blink-trees (temporary neighbor pointers until a separator key can be posted) or on repeated root-to-leaf passes. The initial root-to-leaf pass employs shared latches on root and branch nodes even if the intended operation will modify the leaf node with an insertion or deletion.

Systems that rely on neighbor pointers for efficient cursors, scans, and key range locking, i.e., implementations not exploiting fence keys, employ latch coupling also among neighbor nodes. In those systems, multiple threads may attempt to latch a leaf page from different directions, which could lead to deadlocks. Recall that latches usually do not support deadlock avoidance or detection. Therefore, latch acquisition must include a fail-fast no-wait mode and the B-tree code must cope with failed latch acquisitions.

Most root-to-leaf traversals hold latches on at most two B-tree nodes at a time, i.e., a parent and a child. A split operation needs to hold three B-tree latches, including one for the newly allocated node. In addition, it needs to latch the free space information. In B^link^-trees, split operations require only two latches at a time. Even the final operation that moves separator key and pointer from the child to the parent requires only two latches; there is no need to latch the new node. On the other hand, a complete split sequence in a B^link^-trees requires two periods with exclusive latches, even if the final operation can be delayed until the appropriate latches are readily available.

- During navigation from one B-tree node to another, the pointer must remain valid. The usual implementation keeps the source latched until the destination has been latched. 
- If I/O is required, the latch ought to be released. The B-tree navigation might need to be repeated, possibly starting from the root node. 
- B^link^-trees latch at most two nodes at a time, even while splitting a node and while posting a separator key. 

### 4.9 物理逻辑日志

### 4.10 Non-logged Page Operations

Another logging optimization pertains to structural B-tree operations, i.e., splitting a node, merging neighboring nodes, and balancing the load among neighboring nodes. As for in-page compaction, detailed logging can be avoided because those operations do not change the contents of the B-tree, only its representation. Differently from in-page compaction, however, there are multiple pages involved in these operations and the contents of individual pages indeed changes.

The operations considered are actually reflected in the recovery log; in that sense, the commonly used term “non-logged” is not literally accurate. A better descriptive name might be “allocation-only logging” instead. The savings are nonetheless substantial. For example, in strict physical logging, splitting a node of 8 KB might generate 24 KB of log volume plus log record headers, a few short log records for page allocation, and transaction commit, whereas an optimized implementation might required only these few short log records.

The key insight is that the old page contents, e.g., the full page prior to the split, can be employed to ensure recoverability of both pages after the split. Thus, the old contents must be protected against over-writing until the moved contents is safely written. For example, splitting a full page proceeds in multiple steps after the page is loaded into the buffer pool and found to require a split:

1. a new page is allocated on disk and this allocation is logged, 
2. a new page frame for this new disk page is allocated in the buffer pool, 
3. half the page contents is moved to the new page within the buffer pool; this movement is logged with a short log record that does not include the contents of the moved records but probably includes the record count, 
4. the new page is written to the data store, and 
5. the old page is written to the data store with only half its original content remaining, overwriting the old page contents and thus losing the half moved to the other page. 

The careful write ordering in steps 4 and 5 is crucial. This list of actions does not include posting a new separator key in the parent node of the full node and its new sibling. Further optimizations are possible, in particular for Blink-trees [73]. The log records in the list above could be combined into a single log record in order to save space for record headers. The crucial aspect of the above list is that the last action must not be attempted until the prior one is complete. The delay between the first three actions and these last two actions can be arbitrarily long without putting reliability or recoverability in danger.

Variants of this technique also apply to other structural B-tree operations, in particular merging neighboring nodes, balancing the load among neighboring nodes, and moving entries in neighboring leaves or branch nodes in order to re-establish the desired fraction of free space for the optimal tradeoffbetween fast scans and fast future insertions. In all these cases, allocation-only logging as described above can save most of the log volume required in physical logging. More details on non-logged page operations are discussed in Section 6.6.

- “Non-logged” should be taken to mean “without logging page contents.” Another name is “allocation-only logging” or “minimal logging.” 
- When moving records from one page to another (during split, load balancing, or defragmentation), the old page can serve as backup. It must be protected until the destination page is saved on storage. 

### 4.11 Non-logged Index Creation

The term “non-logged index creation” seems to be commonly used although it is not entirely accurate. Changes in the database catalogs and in the free space management information are logged. The content of B-tree pages, however, is not logged. Thus, non-logged index creation saves perhaps 99% of the log volume compared to logged index creation.

All newly allocated B-tree pages, both leaves and branch nodes, are forced from the buffer pool to permanent storage in the database before committing the operation. Images of the B-tree nodes may, of course, remain in the buffer pool, depending on available space and the replacement policy in the buffer pool. Page allocation on disk is optimized to permit large sequential writes while writing the B-tree initially as well as large sequential reads during future index scans.

> ==TODO：==Fig.4.15

Figure 4.15 compares the log volume in logged and non-logged index creation. The voluminous operations, in particular individual record insertions or full B-tree pages, are not logged. For example, instead of millions of records, only thousands of page allocations are logged. Commit processing is slow if pages have been allowed to linger in the buffer pool during load processing. But just as table scans interact badly with LRU replacement in a buffer pool, pages filled during load processing should be ejected from the buffer pool as soon as possible.

Recovery of non-logged index creation requires precise repetition of the original index creation, in particular is space allocation operations, because subsequent user transactions and their log records may refer to specific keys in specific database pages, e.g., during row deletion. When those transactions are recovered, they must find those keys in those pages. Thus, node splits and allocation of database pages during recovery must precisely repeat the original execution

- Since indexes can be very large, logging the entire contents of a new index can exceed the available log space. Most systems have facilities for non-logged creation of secondary indexes. 
- Upon completion, the new index is forced to storage. 
- A backup of the transaction log must include the new index; otherwise, subsequent updates to the new index cannot be guaranteed even if included in the transaction log and in a log backup. 

### 4.12 Online Index Operations

The other important optimization for index creation is online index creation. Without online index creation, other transactions may be able to query the table based on pre-existing indexes; with online index creation, concurrent transactions may also update the table, including insertions and deletions, with the updates correctly applied to the index before index creation commits.

The traditional techniques described here are sufficient for small updates but it remains unadvisable to perform bulk insertions or deletion while concurrently modifying the physical database design with index creation and removal. There are two principal designs: either the concurrent updates are applied to the structure still being built or these updates are captured elsewhere and applied after the main index creation activity is complete. These designs have been called the “no side file” and “side file” [98]. The recovery log may serve as the “side file.” 

Srinivasan and Carey [119] divide online algorithms for index creation further, specifically the “side file” approach. In their comparison study, all concurrent updates are captured in a list or in an index. They do not consider capturing updates in the recovery log or in the target index (the “no side file” approach). Their various algorithms permit concurrent updates throughout the index creation or only during its scan phase. Some of their algorithms sort the list of concurrent updates or even merge it with the candidate index entries scanned and sorted by the index builder. Their overall recommendation is to use a list of concurrent updates (a side file) and to merge it with the candidate index entries of the index builder.

> ==TODO：==Fig.4.16

Figure 4.16 illustrates the data flow for online index creation with and without “side file.” The upper operation starts with creating an empty side file (unless the recovery log serves as side file). Concurrent transactions buffer their updates there, and the entire contents of the side file is propagated after the scanning, sorting, and B-tree loading are complete. The lower operation starts with creating the new index, albeit entirely empty at this time. Concurrent transactions capture both insertions and deletions into the new index, even before and during B-tree loading.

The “side file” design lets the index creation proceed without regard for concurrent updates. This index creation process ought to build the initial index as fast as an oﬄine index creation. The final “catch up” phase based on the “side file” requires either quiescent concurrent update activity or a race between capturing updates and applying them to the new index. Some systems perform a fixed number of catch-up phases, with the first catch-up phase applying updates captured during index creation, with the second catch-up phase applying updates captured during the first catch-up phase, and with the final catch-up phase applying the remaining updates and preventing new ones. 

The “no side file” design requires that the future index be created empty at the start, concurrent updates modify the future index, and the index creation process work around records in the future index inserted by concurrent update transactions. One concern is that the index creation process may not achieve write bandwidth similar to an oﬄine index creation. Another concern is that concurrent update transactions may need to delete a key in a key range not yet inserted by the index creation process. For example, the index creation may still be sorting records to be inserted into the new index.

Such deletions can be represented by a negative or “anti-matter” record. When the index creation process encounters an anti-matter record, the corresponding record is suppressed and not inserted into the new index. At that time, the anti-matter record has served its function and is removed from the B-tree. When the index creation process has inserted all its records, all anti-matter records must have been removed from the B-tree index. 

An anti-matter record is quite different from a ghost record. A ghost record represents a completed deletion, whereas an anti-matter record represents an incomplete deletion. Put in another way, an anti-matter record indicates that the index creation process must suppress a seemingly valid record. If one were to give weight to records, a valid record would have weight +1, a ghost record would have weight 0, and an anti-matter record would have weight *−*1.

It is possible that a second concurrent transaction inserts a key previously deleted by means of leaving an anti-matter record. In that case, a valid record with a suppression marker is required. The suppression marker indicates that the first transaction performed a deletion; the remainder of the valid record contains the information inserted by the second transaction into the database. A third concurrent transaction may delete this key again. Thus, the suppression marker and the ghost record are entirely orthogonal, except that a ghost record with a suppression marker must not be removed like other ghost records because that would lose the suppression information.

> ==TODO：==Fig.4.17

Figure 4.17 illustrates use of ghost bit and anti-matter bit during online index creation without side-file. Keys 47 and 11 are both updated in the index before the index creation process loads index entries with those key values. This bulk load is shown in the last two entries of Figure 4.17. The history of key value 47 starts with an insertion; thus, it never has the anti-matter bit set. The history of key value 11 starts with a deletion, which necessarily must refer to a future index entry to be loaded by the index creation process; thus, this key value retains its anti-matter bit until it is canceled against a record in the load stream. The final result for key value 11 can be an invalid (ghost) record or no record at all.

In materialized summary (“group by”) views, however, ghost marker and suppression marker can be unified into a single counter that serves a role similar to a reference count [55]. In other words, if its reference count is zero, the summary record is a ghost record; if its reference count is negative, the summary record implies suppression semantics during an online index creation. In non-unique indexes with a list of references for each unique key value, an anti-matter bit is required for individual pairs of key value and reference. The count of references for a unique key value can be used similar to a ghost bit, i.e., a key value can be removed if and only if the count is zero.

 Maintenance of indexes whose validity is in doubt applies not only to online index creation but also to index removal, i.e., dropping an index from a database, with two additional considerations. First, if index removal occurs within a larger transaction, concurrent trans-actions must continue standard index maintenance. This is required as long as the transaction including the index removal could still be aborted. Second, the actual de-allocation of database pages can be asynchronous. After the B-tee removal has been committed, updates by concurrent transaction must stop. At that time, an asynchronous utility may scan over the entire B-tree structure and inserts pages into the data structure with free space information. This process might be broken into multiple steps that may occur concurrently or with pauses in between steps.

Finally, online index creation and removal as described above can easily be perceived by database users as merely the first step. The techniques above require one or two short quiescent periods of time at beginning and end. Exclusive locks are required on the appropriate database catalogs for the table or index. Depending on the application, its transaction sizes and its response time requirements, these quiescent periods may be painfully disruptive. An implementation of “fully online” index operations probably requires multi-value concurrency control for the database catalogs and the cache of pre-compiled query execution plans. No such implementation has been described in the literature. 

- Online index operations permit updates by concurrent trans-actions while future index entries are extracted, sorted, and inserted into the new index. Most implementations lock the affected table and its schema while the new index is inserted in the database catalogs and during final transaction commit. 
- Updates by concurrent transactions may be applied to the new index immediately (“no side file”) or after initial index creation is complete (“side file”). The former requires “anti-matter” records to reflect that the history of a key value in an index started with a deletion; the latter requires “catch-up” operations based on a log of the updates. 

### 4.13 事务隔离级别

### 4.14 小结

In summary, necessity has been spawning many inventions that improve concurrency control, logging, and recovery performance for databases based on B-tree indexes. The separation of locking and latching, of database contents and in-memory data structures, is as important as key range locking aided by ghost records during deletion and possibly also insertion. Reducing log volume during large index utilities, in particular non-logged (or allocation-only logged) index creation, prevents the need for log space almost as large as the database but it introduces the need to force dirty pages from the buffer pool. Finally, weak transaction isolation levels might seem like a good idea for increased concurrency but they can introduce wrong query results and, when used in updates that compute the change from the database, wrong updates to the database.

Perhaps the most urgently needed future direction is simplification. Functionality and code for concurrency control and recovery are too complex to design, implement, test, debug, tune, explain, and maintain. Elimination of special cases without a severe drop in performance or scalability would be welcome to all database development and test teams.