## 4-事务技术

上节是关于如何优化B-tree数据结构及其算法的综述，本节关注B-tree的『并发控制和恢复相关』的技术。现实系统中，磁盘数据结构（主要是B-tree）的『绝大部分开发和测试工作』主要是并发控制和恢复。本节和随后几节描述的几个技术，使得『传统的数据库管理系统』与『现今各种Web服务使用的Key-Value存储系统』显著不同[Chang et al. 2008, DeCandia et al. 2007]。

本节描述的B-tree不仅支持只读搜索，同时也支持并发修改：即插入、删除、以及修改已存在的记录（包括键值和普通字段）。我们关注的是即时修改，而非是**差分文件**这类使用延迟更改的技术。下一节描述更新计划（update plans），用于在『成熟的数据库管理系统』中维护相关的多个索引、物化视图、完整性约束等。

由于在众多用户和应用程序之间共享数据，就能说明建立数据库的复杂性和高代价，因此从一开始，在并发事务和执行线程之间访问共享数据库，就和高可用性以及『快速可靠地从故障软硬件中恢复』一样，处在数据库研发的最前沿。最近，多核处理器使得高度并发的内存数据结构获得更多的关注，事务内存可能是一种解决方案，但这要求正确地理解事务的边界，因此需要选择一致的中间状态。

除了并发用户之外，还存在一种使用异步工具的趋势，它们在持久性存储上执行可选或是强制性任务。一个常见的例子是：从数据库删除表或是索引后，并不是马上将它们的页面加到空闲空间中去，而只是简单地把它们标记为过期，这就要有强制性异步任务来合并空闲空间。『可选异步任务』的常见例子是碎片整理，即为了高效的『区间查询和索引序扫描』，而在B-tree叶子之间平衡负载和优化磁盘布局。还有其它许多异步任务，并不特定属于B-tree，比如收集和更新『用于编译时优化查询』的统计信息。

> ==TODO：==Fig.4.1

图4.1列出了事务的“ACID”四个属性及其简要的解释，这些属性的细节在数据库教科书中有更加深入地论述。这里值得通过一个具体的例子，进一步澄清原子性中“逻辑”一词的含义：假设用户事务尝试在数据库的表中插入一条新记录，为了获得足够的空间，不得不分裂B-tree的节点，但是节点分裂后事务就失败了；事务回滚时，必须撤销插入操作，不过为了确保数据库的内容正确，并不需要严格地要求合并分裂的节点。如果分裂节点的效果仍然保留在数据库中，即使事务回滚后存在“物理变化”，但逻辑上数据库并没有改变。这里“逻辑”是根据“查询结果”来定义；“物理”是根据『磁盘中的位』这样的物理表示来定义。

逻辑数据库和物理数据库之间的不同，或是数据库内容和数据库表示之间的不同，渗透到下面的讨论。一个特别有用的概念是“系统事务”，即对数据库内容没有影响，但在数据表示层面有变化的事务（这类事务仍会在数据表示层面修改，生成日志和提交改变）。对于B-tree的节点分裂、空间分配和回收等，系统事务特别有用。在上面的例子中，当用户事务回滚时，分裂后的节点（由系统事务提交）仍然原地保留。系统事务一般都很简单，并运行在单个线程中，通常是在执行用户事务的线程中，这样用户事务将会等待系统事务完成。如果用户事务运行在多个线程中，那么每个线程可以调用自己的系统事务。

如果将大表或索引分区，并将分区指派到分布式系统中多个节点中，那么通常都是在节点内执行各自的并发控制，在需要的时候，通过两阶段提交来协调恢复。如果在单个站点内使用多个恢复日志，那么要用到同样的技术。本文是关于B-tree索引的综述，分布式事务、两阶段提交等超出了本书的范围。

> ==TODO：==Fig.4.2

并发控制的常用机制就是锁。基本的锁模式，即——共享锁（S）、独占锁（X）和更新锁（U）——之间的兼容性矩阵，如图4.2所示。左列表示当前持有的锁，顶行表示当前请求的锁，矩阵中的空格表示不能授予请求的锁。两个共享锁是兼容的，显然这就是共享的要点；反之独占锁和其它类型的锁都不兼容。共享锁也被称之为读锁，独占锁则被被称之为写锁。

对于允许的锁请求，这个兼容矩阵显示了（锁请求成功后）的聚合模式，目的是为了加速处理新的锁请求，即使有许多事务持有某个特定资源的锁，新的锁请求也只需检查是否与聚合后的锁模式兼容，而不必检查是否与『该资源上所有的』锁模式兼容。换言之，图4.2中左列表示（资源）当前的锁模式，就是当前的聚合模式。大多数情况下，图4.2中锁的聚合模式都不重要。后续有一个例子，包含了更多锁模式，新的聚合模式即不等于老的聚合模式，也不等于请求的锁模式。图4.2没有反应的一个特殊情况是将锁从更新模式降级为共享模式，由于只有一个事务能持有更新锁，从更新模式降级为共享模式后，锁的聚合模式是共享锁。

应用程序有时需要先测试一个条件，再决定是否更新数据记录，这就需要用到更新锁。如果一开始就采用独占锁，这阻止了处理同样逻辑的其它事务；如果采用共享锁，倒是允许两个事务同时锁定共享的数据，但是当它们都试图升级成独占锁时，将会导致死锁！更新锁一次只允许一个事务处于此种状态：即不清楚自己未来将执行何种操作。完成条件测试后，更新锁要么确实升级成独占锁，要么降级成共享锁。注意，更新锁并不比共享锁有更多的权利，它们之间的不同主要是在调度和死锁预防，而不是在并发控制和访问数据的权利。这样，降级到共享锁是允许的。

更新锁也被称之为升级锁。考虑到更新锁并不是授予锁更新数据的权利，而是锁升级的优先级，因此“升级锁”是一个更加准确的名字，然而，更新锁似乎变得越来越常用。Korth在[Korth 1983]中深入考察了派生的锁模式（比如升级锁）和基本的锁模式（比如共享锁和独占锁）之间的关系。

注意图23中那个带问号的字段，当事务持有（某个资源的）更新锁时，有些系统允许新的事务请求（该资源的）共享锁，有些则不允许。前面的处理方式，仅当事务请求互斥锁时，才不再允许额外的共享锁请求；最可能的是（但并不是百分之百）『持有更新锁的事务』请求互斥锁。因此，即使后面的方案在锁矩阵中引入了不对称性，但它能更有效地预防死锁[Gray and Reuter1993]。后面锁矩阵的例子都假设这种不对称的设计。

出故障时『保证原子性和持久性』的主要手段是先写日志，这就要求在『就地修改数据库』之前，先在恢复日志中记录『数据库的变化』。因此，每种类型的“修改”操作需要：一个“do”方法，在初始处理时调用；一个“redo”方法，在出现故障，或是数据库崩溃时，确保数据库处于“修改”后的状态；一个“undo”方法，将数据库恢复至“修改”之前的状态。由“do”方法创建『包含足够信息的日志记录』，以便调用“redo”和“undo”；并指示缓冲池保留这些脏页，直到对应的日志安全地到达“稳定的存储”。因为稳定的存储是可靠的，所以恢复也是可靠的。镜像日志设备是一个通用的技术。日志页一旦生成，就决不能被修改或是覆盖[1]。

早期的恢复技术要求“redo”和“undo”是幂等的[56]，即不管『应用同样的操作多少次』，结果都一样。一个潜在的假设就是，恢复日志在恢复时是只读的，也就是从故障中恢复时，不生成日志。后来的技术，尤其是ARIES[95]，通过为“undo”操作生成恢复日志，并在数据页上保存“Page LSN”（日志序列号）以指明『该页所包含的最近的更新』，来确保只会应用“redo”和“undo”操作一次。此外，逻辑“补偿”，而非物理“补偿”更新操作，比如，删除补偿插入，然而，如果在『B-tree索引的叶节点分裂』之后再删除，那么删除的叶节点和插入的叶节点可能并不相同。中止事务只需生成更新的“补偿记录”，然后正常提交即可，唯一不同的是，此时无需强迫刷新提交记录到稳定存储。

- ACID四个属性（原子性、一致性、隔离性和持久性）定义了事务。先写日志和“do-redo-undo”三人组是恢复和可靠性的基础。Latching和Locking是并发控制的基础
- 在B-tree中，记录级的锁定是指『key-value locking』和『key-range locking』。锁的粒度（即，记录级或是Key级）小于恢复的粒度（即，页级）需要在日志中记录“undo”操作，在恢复时实施逻辑补偿，而不是通过调用『无需在日志中记录的』幂等操作，来实施严格的物理恢复
- 独立物理数据，区分数据库的**逻辑内容**和**物理表示**。这样，在数据库系统的关系层，数据库的物理设计是独立的，且必须自动优化查询；在数据库的存储层，使得『并发控制和恢复的实现』可以有很多优化。
- 一个重要的优化是区分用户事务和系统事务：用户事务『查询或者修改数据库的逻辑内容』；系统事务『则只影响逻辑内容的物理表示』。在系统事务中分裂B-tree的节点，是其优点的一个经典例子。

### 4.1线程锁（Latching） and 事务锁（Locking）

B-tree或是B-tree索引中的加锁有两层含义。一是在**并发的数据库事务**之间实施并发控制，并发事务查询或修改数据库，在该上下文中，主要考虑的是数据库的逻辑内容，而不是其物理表示（比如像B-tree索引这样的数据结构）。二是在**并发的线程**之间实施并发控制，并发线程修改的是内存中的数据结构，这里内存中的数据结构尤其是指『基于磁盘的B-tree节点』在缓存池中的内存映像。

这两方面并不总是区分的很清晰。**但当多个并行线程处理某个数据库请求时，它们之间的区别就变得非常明显了**。具体地说，同一个事务里的两个线程必须“看到”同样的数据库内容，数据库表里有相同的记录数；这还包括事务的某个线程必须要能“看到”代表该事务的另一个线程所实施的修改。然而，当一个线程正在分裂B-tree的节点时，也就是『在特定的数据结构中』修改数据库的物理表示时，其它的线程不能观察到不完整的中间结果。**另一方面，当单个线程为多个事务服务时，它们之间的区别也非常明显**。

|            | 事务锁                   | 线程锁                    |
| ---------- | ------------------------ | ------------------------- |
| 隔离 …     | 用户事务                 | 线程                      |
| 保护 …     | 数据库内容               | 内存数据结构              |
| During …   | 整个事务^2^              | 临界区                    |
| Modes …    | 共享、互斥、更新、       | 读, 写,                   |
|            | 意图、escrow, schema,等. | (可能)更新                |
| Deadlock … | 检测 & 解决              | 避免                      |
| … by …     | 等待图的分析,            | 编码规则,                 |
|            | 超时, 事务中止,          | instant-timeout requests, |
|            | 部分回滚, 锁降级         | “锁分层”^3^               |
| Kept in …  | 锁管理器的哈希表中       | 被保护的数据结构中        |
图4.3 事务锁（Locks）和线程锁（latchs）

> 2. 事务必须保持锁定，直到事务提交，以便等价于串行执行，也称为事务隔离级别“可串行化”。较弱的事务隔离锁的持续时间较短。在许多数据库系统中，默认弱事务隔离，因此以正确和完全隔离并发事务为代价实现更高的并发性。
> 3. 在这种技术中，层次被分配给『线程锁』，线程只能请求高于已持有『线程锁』层次的『线程锁』。

图4.3总结了它们之间的不同。『事务锁』使用读写锁来隔离事务，可以锁定B-tree的页、B-tree的键值，还可以锁定两个键值之间的空隙（开区间）。后面两种锁定方式也被称之为『键值锁定』和『键区间锁定』。『键区间锁定』是谓词锁定的一种形式，这里的谓词是用『实际的B-tree键值』和『B-tree的排序顺序』来定义的。缺省情况下，『事务锁』支持死锁检测，并且在事务提交后才会被释放。『事务锁』亦支持复杂的调度，比如：使用队列缓存等待处理的锁请求；推迟新锁的获取以便完成锁转换（比如，将现有的共享锁转换成互斥锁）。这种级别的复杂性使得『事务锁』的获取和释放相当昂贵，通常需要执行数百个CPU指令，并耗时上千个CPU周期，这包括由『锁管理器中哈希表的缓存失效』而引入的CPU周期。图4.3总结了它们之间的不同。

『线程锁』隔离线程，所有由多个线程共享的内存数据结构，包括B-tree的页、缓冲池的管理表，都由『线程锁』来保护，由于多个线程会共享『锁管理器』的哈希表，因此，当检查或修改『数据库系统事务锁』的信息时，必须请求『线程锁』；对于共享的数据结构，如果事务的某个线程需要一个写锁，即使是同一个事务的其它线程也会与之冲突；只在重要的地方申请『线程锁』，即，读取或修改共享数据结构的代码里。通过恰当的编码规则，比如，小心设计多个『线程锁』的请求顺序，来避免『线程锁』之间的死锁。处理死锁需要有办法回滚到先前的状态，避免死锁则不需要，因此开销更小、性能和扩展性更好，这样，避免死锁对『线程锁』更合适。获取和释放『线程锁』可能只需要数十条指令；同时，有关『线程锁』的信息可以嵌入在其保护的数据结构中，这通常不会引入额外的缓存失效。对于缓冲池中磁盘页的映像，『线程锁』的信息可以嵌入在『包含页标识符的描述结构』中。

由于『事务锁』与数据库内容有关，而与它们的物理表示无关，因此，如果数据都存储在B-tree的叶节点中，那么『事务锁』就不必保护B-tree的非叶节点；相反，『线程锁』则要保护B-tree中的所有节点（无论这些节点在数据库的角色是什么）。『事务锁』和『线程锁』之间的不同，在对二级索引（指向唯一一份数据存储的冗余索引）实施并发控制时，也变得很明显。例如，ARIES/IM[97]的『data-only的锁定模式』，只需锁定（记录的标志符）一次，无需锁定二级索引上对应的键值，就可以保护从属于逻辑行的所有记录（包括B-tree中的记录），从而在更细的粒度上隔离事务，并获得更高的并发性。相反，『由多个线程并发访问』的内存数据结构都要用『线程锁』保护，这显然包括二级索引上的所有节点和页面。

- 『线程锁』协调线程，保护的是内存中的数据结构，包括缓冲池中磁盘页的映像；『事务锁』协调事务，保护数据库内容。
- 事务和『事务锁』需要检测和处理死锁；线程和『线程锁』则需要避免死锁，这需要制定编码规则，请求『线程锁』时，若遇冲突只能选择失败，而非等待。
- 『线程锁』和临界区密切相关，可以由硬件提供支持，例如『硬件事务内存』

### 4.2 幻影记录

如果在事务中删除B-tree记录，那么在提交事务提前，必须保留回滚事务的能力。因此，必须确保在回滚事务时，空间分配不会失败，并且另一个事务不能插入有相同唯一B-tree键的新记录。满足这些要求的一种简单技术是将记录及记录的键保留在B-tree中，仅将其标记为无效；在提交删除记录的事务之前，一直锁定它们（记录及记录的键）。另一个好处是，用户事务可以延迟处理甚至避免一些空间管理工作，例如，移动页面间接数组中的元素。此外，用户事务只需锁定被删除的记录，而不是记录两个相邻键之间的整个键区间。

结果记录称为伪删除记录或**幻影记录**。记录头中的一个位足以指示记录的幻影状态。因此，**<u>删除变成了对幻影位的修改</u>**。如果并发控制依赖于**键区间锁定**（下面讨论），则只需要锁定键本身，并且键之间的所有间隙可以保持未锁定状态。

> ==TODO：==Fig.4.4

图4.4显示了一个带有幻影记录的B-tree页面，即在删除键为27的记录后的中间状态。显然，这是在页面内的移除幻影和回收空间之前。有效记录包含一些与其键相关联的信息，用省略号表示，而幻影记录中的信息字段可能会保留，但没有意义。回收空间的第一步可以尽可能地缩短这些区域，尽管首选方法可能是完全删除幻影记录。

查询必须忽略（跳过）幻影记录；因此，扫描带幻影记录的系统始终含有隐藏的谓词，尽管该谓词的计算内置在B-tree代码中，而无需谓词解释器。回收空间留给后续事务，可能是一个插入（需要比页面中已有空闲空间更多的空闲空间）、显式调用页面压缩或B-tree碎片整理实用工具。

幻影记录被锁定时无法删除。换言之，幻影记录至少在原地被保留到（删除它的）事务提交，将有效记录变成幻影记录。随后，另一个事务可能会锁定一个幻影记录，例如，以确保继续缺少该键值。锁定不存在的键值对于可串行化至关重要；没有它，在同一事务中重复查询 `select count(*)` 可能会返回不同的结果。

同一页中可能同时存在多个幻影记录，一个系统事务就可以删除所有这些记录。将删除幻影的日志记录与提交事务的日志记录合并，则无需在日志中记录已删除记录的内容。==<u>合并这些日志记录使事务不可能在幻影删除和提交之间失败</u>==。因此，永远不需要回滚幻影删除，从而恢复日志中的记录内容。换句话说，幻影记录不仅可以确保在需要时成功地回滚事务，而且还经常减少与删除相关的日志总量。

> ==TODO：==Fig.4.5

图4.5说明了没有和使用幻影记录时，删除事务的日志。在左侧，用户事务删除记录并记录其全部内容。如果需要，可以使用恢复日志中的信息重新插入记录。在右侧，用户事务只修改幻影位。稍后，系统事务会创建一条日志记录，包含事务启动、幻影删除和事务提交。无法从恢复日志中重新插入已删除的幻影记录，但这没有必要，因为此时删除操作已经提交了。

如果使用与B-tree中幻影记录相同的键插入新行，则可以重用旧记录。因此，插入可能会变成对幻影位的修改，在大多数情况下，还会修改记录中其它一些除了键之外的字段。==<u>与删除时一样，键区间锁定只需要锁定键值，而不需要锁定插入新键的键区间</u>==。

虽然幻影记录通常与B-tree中的记录删除相关，但它们也可以帮助插入新键。将插入拆分为两步可以减少事务所需的锁。首先，在**latch**的保护下，用所需的键创建一个重幻记录，此步不需要锁。第二，用户事务根据需要锁定和修改新记录。如果用户事务失败并回滚，则保留幻影记录。第二步需要锁定键值，而不是插入新键的键值区间。

这个想法的另一个改进是，用将来可能插入的键创建多个幻影记录。如果将来插入的键完全可以预测，例如订单号和发票号，则这一点尤其有用。即使无法预测将要插入的键的精确值，这种幻影记录也可以有助于分离后续的插入，从而在后来的插入事务中实现更多的并发性。例如，键可以由多个字段组成，但只能对前导字段预测值，例如，每个订单中的订单号和行号。

> ==TODO：==Fig.4.6

图4.6说明了插入多个幻影记录。插入键值为11、12、13和14的有效记录后，下一个操作可能是插入键值为15、16、17等的记录。由于已经填写了这些键，在空间上预先分配了适当的空间，这样就可以提高这些插入的性能。用户事务节省了分配工作，只锁定键值，既不锁定键值之间的间隙，也不锁定现有最大键值与无穷大之间的间隙，这通常是此类**插入事务序列**的瓶颈。

最后，在有效的B-tree记录中，插入仅包含键、而不包含任何剩余字段的非常短的幻影记录是有益的。将这样的“幻影插槽”==**撒**==到有序记录序列（或间接数组中的槽）中可以实现页面内的有效地插入。在没有这种幻影插槽的页面中，插入需要移动所有条目的一半，比如间接数组中的插槽。在具有幻影插槽的页面中，插入的复杂性不是O (N)而是O (log N)[12]。例如，在每页有数千个小记录的二级索引中，插入需要在间接数组中移动十个而不是几千个元素，删除操作根本不会移动任何元素，只会留下一个幻影插槽，而页面重组则会留下大约10%或20%的幻影插槽。

> ==TODO：==Fig.4.7

图4.7是图3.3的细化，显示了两个差异。第一，间接数组中的元素包含键或键前缀。图中显示了字母，但真正实现时将使用最小规范化键，将它们解释为整数值。第二，其中一个是幻影插槽，因为它包含一个没有引用记录的键（“d”）。这个插槽可以参与二叉搜索和键区间锁定。它可能在页面重组期间被放在那，或者可能是快速删除记录的结果，而没有移动键“g”和“k”的两个槽。一旦存在，它就可以加速插入。例如，插入带有“e”键的新记录可以简单地修改当前包含“d”的槽。当然，这要求当前未锁定键“d”，或者锁管理器允许适当的调整。

- 幻影记录（也称为伪删除记录）通常用于减少删除期间的锁定需要，并简化删除的“撤消”。
- 幻影记录不影响查询结果，但参与键区间锁定。
- 可以在插入时或异步清理时，回收幻影记录或其空间，但前提是它未被锁定。
- 幻影记录也可以加速和简化插入。

### 4.3 键区间锁定

术语键值锁定和键值区间锁定通常可以交换使用。封锁整个键值区间，而不封锁键值的唯一原因就是保护事务不受其它事务插入语句的影响。例如，如果事务执行类似这样的SQL：`select count (*) from . . . where . . . between . . . and . . . ,`，也就是在某个索引列上进行范围查询，并且该查询运行在『可串行的事务隔离级别』，那么如果事务再次执行该查询，得到的结果和前次执行的结果应该一样。换言之，除了保护查询范围内的B-tree记录不被删除之外；事务获得并持有的『事务锁』也必须能阻止『在查询范围内、已有的键值之间』插入新的B-tree记录，也就是，键值区间封锁通过锁定已有键值之间的“空隙”，确保该“空隙”内没有出现的键值，（在事务完成之前（中止或是提交））一直不会出现。比『可串行化』弱的事务隔离级别，不会提供这样的保证，但很多应用开发人员并不能理解它们精确的语义，以及它们对应用程序正确性的有害影响。

键值区间锁定是谓词锁定[31]的一种特殊形式。主流产品既没有采用通用的谓词锁定，也没有采用较为实用的==<u>『precision locking』</u>==[76]。『键值区间封锁』通过B-tree『排序顺序中的区间』来定义它的谓词，区间的边界是B-tree中当前存在的键值，通常的形式是半开区间，这里的半开区间包括相邻的两键值之间的空隙，及其中某端的键值；『右键值锁定』较『左键值锁定』通用。『右键值锁定』要能锁定人为的+∞；『左键值锁定』则可以锁定null，即，假设null是B-tree排序顺序中可能出现的最小值。

最简单的键值区间锁定是将『键值和其相邻的区间』作为整体锁定。对『B-tree记录、记录的键值或是相邻键值之间的区间』任何形式的修改，包括修改记录的非键值字段、删除某个键值、在键值之间的空隙插入新键值等等，都需要『独占事务锁』。删除某个键值，除了要锁定该键值之外，还需要锁定与它（右边）相邻的键值，锁定相邻键值的原因是万一事务回滚，确保能重新插入删除的键值。

> ==TODO：==Fig.4.8

图4.8演示的是单个键值上的键值区间封锁可能保护的几个“对象”，作为示例的B-tree叶节点包含三个键值，在1170到1180之间，键值1174上的『事务锁』可能覆盖的范围，如箭头所示。第一个箭头演示的是传统的『右键值锁定』，即，『事务锁』锁定的是两个键值之间的“空隙”，以及“空隙”后的记录键值。==<u>第二个箭头显示『左键值锁定』</u>==，第三个箭头表明『事务锁』仅仅锁定键值自身，没有覆盖键值左右两边任何一边的空隙，所以在事务持续期间，这个『事务锁』不能阻止某个键值的出现，例如插入键值1176，因此不能保证可串行性。

要简短地讨论下第四个箭头所指示的『事务锁』，它用于补充『键值锁定』，可以在『不锁定已存在的键值』情况下，保证其它事务不能（在键值的“空隙”中）插入新键值。当一个事务像第四个箭头所示那样，锁定键值1174时，第二个事务可以更新『键值为1174』的记录，更具体地说，第二个事务不能更新记录的键值（即1174），但能更新记录的『非键值字段』，因此，在第一个事务释放其『事务锁』之前，第二个事务不能删除（键值为1174的）记录。另一方面，第二个事务可以更新记录的『幻影标志位』，比如，如果发现键值为1174的记录有效，第二个事务可以将其变为幻影记录，从而将其从后续查询的结果集中排除；反过来，第二个事务也可以将幻影记录变为有效记录，并更新其『非键值字段』，这就是在B-tree中实施逻辑插入。为了简化后面的讨论，图4.8少画了一个指示『事务锁』锁定范围的箭头，它可以锁定『被锁定键值』左边的范围。

键值区间封锁广泛用于商业系统中，ARIES/KVL（键值封锁）和ARIES/IM（索引管理）都是键值区间封锁的一种形式，两种封锁技术都不会锁定索引中的单个索引记录，ARIES/KVL锁定索引中唯一的键值，如果是『非唯一二级索引』，一把锁就能锁定某个键值的所有记录，及到其左边键值的开区间。==*『串行隔离级别』的事务向这样的开区间插入时，仍要请求（右边键值的）『事务锁』，即使只有瞬时持续时间*==。如果并发事务持有『相冲突的事务锁』，例如，某个读事务正在访问区间内的某个记录，那么插入要么失败，要么被延迟。在『读取记录的事务』和『插入不同键值的事务』之间，并不存在真正的冲突，只是（设计应用时）选择的隔离级别，使得它们好像有冲突一样。可能正是由于这种人为的冲突，许多数据库都运行在比『可串行化』弱的事务隔离级别上，并且很多软件厂商总是选择较弱的隔离级别作为缺省的隔离级别。

ARISE/IM中的『事务锁』锁定表中的某条记录，包括为该条记录建立的所有索引，以及每个索引中『索引记录左边』的开区间（称之为“data-only locking”，后续的DB2中是“type-1 indexes”）。在非唯一索引中，这个开区间左边的记录可能键值相同，但记录标志符不一样。在ARISE/IM中，如果是页级封锁，一把锁将会锁定数据页中的所有记录、它们的索引记录、索引键值对应的开区间。“修改结构的操作”需要获取一把专门针对索引树的『独占线程锁』，只读操作只在检查到页面的“结构修改位”被置位后，才会按共享模式请求这把『线程锁』。由于这两种方法非常复杂，又包含太多的细节，我们鼓励读者去看原始的论文，而不是依赖于像本文这样的二道贩子。但愿读者们读完此文后，可以更容易理解原始的ARIES论文。

微软SQL server的键值区间封锁基于Lomet的设计[85]，而Lomet的设计又基于ARIES/IM和ARIES/KVL [93, 97]。和ARIES一样，Lomet的方案也需要“瞬时锁”，即，锁持有的时间极其短；不同的则是需要一种新的锁模式：“插入锁”，仅仅应用于B-tree索引中两个键值之间的开区间。然而，在SQL Server公开的锁兼容矩阵中，“插入锁”和互斥锁太类似了，以至于不太清楚“插入锁”有什么用，为什么需要区分“插入锁”和互斥锁。最近的设计[48]即不用“瞬时锁”，也不用“插入锁”，但相比Lomet的方案，又能提供更高的并发性。

建立在层次封锁[58]基础上的键值区间封锁，能支持各种各样的锁粒度。在锁定细粒度的一个或多个资源之前，先用适当的意图锁封锁粗粒度的资源。典型的使用场景是：使用共享模式锁定整个要搜索的文件，再用互斥模式封锁要修改的少量记录。此时就需要用『IX模式的事务锁（意图获取互斥锁）』封锁整个文件，再用『互斥事务锁』封锁要修改的个别记录。这样在文件层次就能检查到冲突，具体地说，就是共享锁和IX锁之间的冲突。

> ==TODO==：Fig.4.9

图4.9显示的是层次封锁中，各类锁之间的兼容性矩阵。标记为a的字段表示这两类锁的不兼容，是由更新锁的不对称性引起的。这个兼容矩阵比传统的兼容矩阵更好，它加入了请求的锁和持有的锁组合后的模式，本文称之为聚合锁模式。比如，如果多个事务已经用IS锁将某个资源锁定（只有IS锁，没有其它类型的锁），那么聚合锁就是IS锁，基于此聚合锁，如果事务请求IX锁，就可直接授予之，而无需检查『前面事务所持有的、锁定该资源的』每一把IS锁，此外，新的聚合锁转换成IX锁。

从图4.9可以看到，两类意图锁总是兼容的，这是因为将在更细的粒度上检查真正的冲突。另外，意图锁和纯粹的读写锁之间的兼容性，和读写锁之间的兼容性完全一样；与『组合锁模式S+IX』兼容的锁，须得同时兼容共享锁和IX锁。

根据Gray和Reuter的书[59]，图4.9显示的是更新（U）锁，而非意图更新锁（IU和SIU），应该选择获取意图写锁(IX和SIX)来替代获取它们。粗粒度上的IX锁覆盖了细粒度上的更新锁。

在基于层次封锁的键值封锁中，粗粒度的封锁是锁定半开区间；细粒度的封锁要么锁定键值，要么锁定开区间。因此简单分层，就可以提供非常精确的封锁区间，以适合每个事务需要。这个设计的缺点是锁定键值（或是开区间）需要调用锁管理器两次：一次是使用意图锁封锁半开区间，一次是使用读写锁封锁键值。

由于『都是通过唯一的键值』来确定三个不同的锁（键值、开区间、以这两者组合而来的半开区间），所以在锁模式的数量和锁管理器的调用次数之间，可以做各种折衷。使用精心设计的锁模式，来描述『如何锁定半开区间、键值、开区间之间的组合』。因此，利用层次锁『封锁半开区间、键值、开区间』的系统，所需要的封锁开销，不会比那些只封锁半开区间的系统高。无需额外的运行时成本，这样的系统允许『分别锁定键值和开区间的不同事务』并发执行，也就是确保『不在开区间内插入新键值』的同时，允许事务更新与非键值无关的记录属性，这包括『指示该记录是否为幻影记录』这样的属性。因此，当另一个事务已锁定相邻的开区间时，逻辑插入或删除仍然可能。

具体地说，就是用S（共享）、X（互斥）、IS（意图共享）和IX（意图互斥）这几种模式来封锁半开区间，但不用SIX（共享+意图互斥）模式，这是因为只封锁两个资源，容易实现更为准确的锁模式；用S和X两种模式来封锁键值和开区间。两个资源（键值和开区间）上三种锁模式（S、X和N(没有锁)）的各种可能的组合，必须能被新（设计）的锁模式覆盖，意图锁（IX和IS）则以隐含的方式继续存在。比如，如果使用X模式封锁键值，则隐含表示用IX模式封锁半开区间；如果使用S模式封锁键值，X模式封锁开区间，那么半开区间上隐含的封锁模式是IX。使用两种锁模式（一个是封锁键值的锁模式，一个是封锁开区间的锁模式），来区分锁很容易。假设使用左键值封锁协议，SN锁表示『使用S模式的锁保护键值，且没有封锁键值右边的开区间』；NS锁则是没有封锁键值，但封锁了其右的开区间（S模式）。这个（新设计的）锁模式可以用于预防幻象，这正是可串行的事务隔离级别所要求的。

> ==TODO==：Fig.4.10

图4.10表示的是（这个新方案）的锁兼容矩阵，推导出这个矩阵很简单，只需分别检查第一个组件之间的兼容性和第二个组件之间的兼容性。例如，XS和NS兼容，是因为X和N兼容，而S和S也是兼容的。一个字符的锁（封锁半开区间）等价于分别是使用这个模式的锁封锁键值和开区间，但是，除非绝对必要，引入更多的锁模式没有好处。

图4.10也显示出某些情况，最终聚合后的锁模式，即不等于『先前持有的聚合锁』的模式，也不等于请求的锁模式。SN和NS组合成S模式，但更有趣的是，SN和NX不仅兼容，而且也不必定义特殊的聚合锁模式，完全是根据现有的规则推导而来。

如果二级索引中的索引记录不唯一，每个键值可能会关联多个记录，更有甚者，每个键值可能关联上千个记录,这是由于某个键值频繁使用，或是某些属性只有几个不同的键值。在非唯一索引索引中，键值封锁可能会锁定每个键值（因此也锁定了对应的整个记录集），或是锁定唯一的『键值和记录标记符』对。前者节省了每次查找所需的锁请求总量；后者则允许更新时有更高的并发。在前者的设计中，可以使用意图锁来锁定键值，从而获得高并发。根据设计细节，可能并不需要根据『二级索引中的记录标记符』单独锁定对应的记录，这是因为可能在遍历二级索引所属的表时，就已经锁定这些记录了。

除了传统的读写锁，或是共享和互斥锁外，其它的锁模式也被研究过。最有名的就是“增量”锁，增量锁使得事务可以并发增加和减少总和和总数，在明细表中几乎没有这样的并发操作，但是在汇总视图中，这类并发操作很可能成为瓶颈。在为物化视图定义的B-tree索引中，即使明细表的插入和删除影响了所有分组，并因此影响汇总记录，及汇总记录的索引，但『幻影记录、键值区间封锁和增量锁』的组合仍使得高并发成为可能。扩展键值区间封锁以支持增量锁，包括用增量锁封锁既有键值之间的区间，并不难。更多的细节可以在[51，55，79]中找到。

如果在与时间相关的属性上建立索引，那么过高的插入率将会导致B-tree的“右边缘”成为热点，使用右键值封锁，有两个解决方案。一个方案是请求“瞬时事务锁”，即，只检查能否锁定+∞（正无穷大），但并不持有，这个假设违反了『两阶段封锁协议』，但是如果『在页面上插入新键值』和『检查是否能锁定+∞』是在『同一个页面线程锁』保护下完成的话，系统仍能正常工作。另一个方案是依赖于系统事务插入幻影记录，用户事务再将这些幻影记录转换成有效的索引记录，这样用户事务之间就不会互相影响。系统事务修改数据库的逻辑内容时，不需要任何事务锁；随后的用户事务使用键值封锁，仅仅锁定要修改的B-tree索引记录。如果索引记录的插入键值是可以预测的，比如订单号，一个系统事务就可以插入多个幻影记录，因此，就可以为多个用户事务服务。

- 键区间封锁不但封锁键值，还封锁键值之间的区间。它以一种『即特殊又实用』的方式实现谓词锁，它们之间的不同体现在『设计的简单性』和『所支持的并发性』上。
- 为了满足事务的可串行性（即并发事务之间的真正隔离，等价于串行执行事务），需要封锁现有键值之间的区间，这等价于锁定不存在的新键值。

> ==TODO：==
>
> 1. 解释precision locking的含义，右键值锁定：（x,y]，左键值锁定：[x,y)
> 2. 封锁右键和封锁左键的区别

### 4.4 叶边界的键区间锁定

传统的键值区间封锁中，另一个复杂性和低效率的来源是横跨两个相邻叶节点的区间锁。例如，如果『新插入的B-tree索引记录的』键值，在其插入的叶节点中最大，那么右键值封锁需要在后一页叶节点中找到最小的键值；左键值封锁也有同样的问题，不过是在键值为插入叶节点中最小时出现。为了能快速访问下一页叶节点，许多系统都在每个节点，或者至少是在叶节点中，包含一个指向下一页的指针。避免邻接指针的替代方案是在每一个B-tree节点中引入两个“防护键”,它们定义了在这个节点中，后续键值的插入范围，其中一个表示为闭区间，另一个表示为开区间，这取决于当父节点中的分隔键完全等于搜索键时，所采取的决定。

一开始，空的B-tree只有一个节点，该节点既是根节点，又是叶节点，使用特殊的“防护键值”来表示正负无穷大，其它所有的“防护键”和分裂叶节点时所确定的分隔键完全相同：当B-tree节点（叶节点，或是分支节点）因为溢出而分裂时，父节点内的分隔键，同样在分裂后的两个子节点中保留了一份，分别作为这两页“防护键”的上界和下界。

防护键不必总是有效的B-tree记录。具体地说就是，作为闭区间的防护键有时可能是有效的数据记录；但是另一个（表示开区间的）防护键总是无效（表现为幻影记录）。如果删除了作为防护键的有效记录，那么仍然要在该页叶节点中，以幻影记录的方式保留这条记录（的键值）。实际上，使用幻影记录只是一个实现技术上的选择，但和传统的幻影记录不同，不能因为插入新记录需要空闲空间，或着因为（背台运行的）清理工具，而移除『表现为幻影记录的防护键』。不过只要新插入的键值恰好等于防护键，作为闭区间的幻影防护键可以被再次转换为有效记录。

> ==TODO==：Fig.4.11

图4.11显示的是带有防护键的叶节点和内部非叶节点（这儿是根节点）。由于防护键定义了页内可能的键值范围，因此没有必要锁定相邻节点上的键值。由于新插入的键值恰好等于防护键，而将幻影防护键转换为有效记录时，不必使用键值区间封锁，只需要锁定插入的键值即可，这些因为插入事务（实际上）并没有新建记录，而只是修改一条已存在的B-tree记录而已。

- 老旧的设计封锁两个键值之间的区间时，如果两个键值分别存储在相邻的两个叶节点上，无论其中的一个叶节点在『本次查询或更新中』是否会被用到，都需要同时访问这两个相邻叶节点。
- 防护键是『分裂叶节点时创建的』分割键的一份拷贝。在每个叶节点中，一个防护键（比如上界）总是幻影记录，而另一个防护键可以是幻影记录，也可能是有效记录。使用防护键的键值区间封锁，不必访问相邻的叶节点。

### 4.5 Key Range Locking of Separator Keys

> **In most commercial database systems, the granularities of locking are an entire index, an index leaf (page), or an individual key (with the sub-hierarchy of key value and open interval between keys, as discussed above).** Locking both physical pages and logical key ranges can be confusing, in particular when page splits, defragmentation, etc. must be considered. ==An alternative model relies on key range locking for separator keys in the B-tree level immediately above the leaves [48].== This is different from locking fence keys at the level of B-tree leaves, even if the same key values are used. The scope of each such lock is similar to a page lock, but locks on separator keys are predicate locks in the same way as key range locks in B-tree leaves. <u>==Lock management during splits of leaf pages can rely on the intermediate states of B^link^-trees or by copying the locks from one separator key to a newly posted separator.</u>==

在大多数商业数据库系统中，锁的粒度是一个完整的索引、一个索引叶（叶节点）或一个单独的键（如上所述，具有键值的**子层次结构**和键之间的**开区间**）。锁定物理页和逻辑键区间可能会令人困惑，特别是在必须考虑页分裂、碎片整理等时。另一种模型依赖<u>在叶节点直接上层的B-tree节点中的分隔键</u>的**键区间锁定**[48]。这与在B-tree叶节点锁定**fence键**不同，就算是使用相同的键值。每个此类锁的范围类似于页锁，但分隔键上的锁是**谓词锁**，其方式与B-tree叶节点中的键区间锁相同。<u>==叶节点分裂期间的锁管理可以依赖于 B^link^-tree的中间状态，或者通过将锁从一个分隔键复制到新建的分隔键</u>==。

> Very large database tables and their indexes, however, may require millions of leaf pages, forcing many transactions to acquire many thousands of locks or lock much more data than they access. Lock hierarchies with intermediate levels between an index lock and a page lock have been proposed, although not yet used in commercial systems.

但是，非常大的数据库表及其索引可能需要数百万个叶页，这迫使许多事务获取数千个锁，或锁定比它们访问的数据要多的多的数据。虽然还没有在商业系统中使用，但是已经提出了==**索引锁**==和==**页锁**==之间具有中间级别的锁层次结构。

> One such **proposal** [48] employs the B-tree structure, ==**adding key range locking on separator keys in upper B-tree levels to key range locking on leaf keys**==. In this proposal, the lock identifier includes not only a key value but also the level in the B-tree index (e.g., level 0 are leaves). This technique **promises to** adapt naturally to skewed key distributions just like the set of separator keys also adapts to the actual key distribution.

其中一个**方案**[48]利用B-tree结构，将叶节点上一层的B-tree分隔键上的**键区间锁定**添加到叶节点键上的**键区间锁定**。在该方案中，锁标识符不仅包括键值，还包括B-tree索引中的ç（例如，层0是叶节点）。这种技术可以自然地适应键的倾斜分布，就像分隔键集也可以适应键的实际分布一样。

> Another proposal [48] focuses on the B-tree keys, deriving granularities of locking from compound (i.e., multi-column) keys such as “last name, first name.” **The advantage of this method is that it promises to match predicates in queries and database applications, such that it may minimize the number of locks required.** Tandem’s “generic locking” is a rigid form of this, using a fixed number of leading bytes in the key to define ranges for key range locking.

另一个方案[48侧重于B-tree的键，从复合键（即多列键）如“姓氏，名字”中派生出锁的粒度。这种方法的优点是它承诺在查询和数据库应用程序中匹配谓词，以便它可以最小化所需锁的数量。Tandem的“通用锁定”是一种严格的形式，使用键中固定数量的前导字节来定义键区间锁定的范围。

> > Saracco and Bontempo [113] describe Tandem’s generic locking as follows: “*In addition to the ability to lock a row or a table’s partition, NonStop SQL/MP supports the notion of generic locks for key-sequenced tables. Generic locks typically affect multiple rows within a certain key range. The number of affected rows might be less than, equal to, or more than a single page. When creating a table, a database designer can specify a “lock length” parameter to be applied to the primary key. This parameter determines the table’s finest level of lock granularity. Imagine an insurance policy table with a 10-character ID column as its primary key. If a value of “3” was set for the lock length parameter, the system would lock all rows whose first three bytes of the ID column matched the user-defined search argument in the query.*” Note that Gray and Reuter [1993] explain key-range locking as locking a key prefix, not necessarily entire keys.

> Saracco和Bontempo [113]描述了Tandem的通用锁定如下：“**除了能够锁定行或表分区之外，NonStop SQL/MP还支持<u>键顺序表的通用锁</u>的概念。 通用锁通常会影响某个键范围内的多个行。受影响的行数可能小于、等于或大于单页。建表时，数据库设计者可以指定要应用于主键的“锁定长度”参数。 此参数确定表的最佳锁定粒度。假设，一个保险策略表以10个字符的ID列为主键。 如果为锁定长度参数设置为“3”，系统锁定的所有行是：其ID列的前三个字节匹配查询中用户定义的搜索参数。**”注意，Gray和Reuter[1993]将键区间锁定解释为锁定键的前缀，不一定是整个键。

> Both proposals for locks on large key ranges need many details worked out, many of which will become apparent only during a first industrial-strength implementation. A variant of this method [4] has been employed in XML storage where node identifiers follow a hierarchical scheme such that an ancestor’s identifier is always a prefix of its descendents.

**大范围锁定键值区间**的两个方案需要制定许多细节，在第一次出现工业强度的实现时，才会显现出其中许多细节。 该方法的变体[4]已被用于XML存储中，其中节点标识符遵循分层方案，使得祖先的标识符始终是其后代的前缀。

- *Large indexes require an intermediate granularity of locking between locking a key value and locking an entire index.*大型索引需要在锁定键值和锁定整个索引之间的**中等粒度锁**。
- *Traditional designs include locks on leaf pages in addition to (or instead of) locking key values. The number of pages in an index and thus the number of page locks in a query may far exceed the threshold at which the lock manager escalates to a larger granularity of locking, which is usually a few thousand locks.*除了（或代替）锁定键值之外，传统设计还包括叶节点上的锁。索引中的页节点数量，以及查询中页节点的锁定数量，可能远远超过锁管理器升级到更粗粒度锁的阈值（通常是几千）。
- *Alternatively, key range locking can be applied to separator keys in some or all branch nodes in a B-tree. This design adapts traditional hierarchical locking to B-trees and their organization in levels.*或者，**键区间锁定**可以应用于B-tree中某些或所有分支节点中的分隔键。该设计将传统的分层锁与B-tree及其层次结构相适应。

### 4.6 B^link^-trees

In the original design for B-trees, splitting an overflowing node updates at least three nodes: the overflowing node, the newly allocated node, and their parent node. In the worst case, multiple ancestors must be split. Preventing other threads or transactions from reading or even updating a data structure with incomplete updates requires latches on all affected nodes. A single thread holding latches on many B-tree nodes obviously restricts concurrency, scalability, and thus system performance. Rather than weakening the separation of threads and thus risking inconsistent B-trees, the definition of correct B-trees requires some relaxation. One such design divides a node split into two independent steps, i.e., splitting the nodes and posting a new separator key in the parent node. After the first step, the overflowing node requires a separator key and a pointer to its right neighbor, thus the name B^link^-trees [81].

Until the second step, the right neighbor is not yet referenced in the node’s parent. In other words, a single key range in the parent node and its associated child pointer really refer to two child nodes. A root-to-leaf search, upon following this pointer, must first compare the sought key with the child node’s high fence and proceed to the right neighbor if the sought key is higher. In order to ensure efficient, logarithmic search behavior, this state is only transient and ends at the first opportunity. 

The first step of splitting a node defines the separator key, creates a new right neighbor node, ensures correct fence keys in both nodes, and retains the high fence key of the new node also in the old node. The last action is not required for correct searching in the B-tree but it enables efficient consistency checks of a B-tree even with some nodes in this transient state. In this transient state, the old node could be called a “foster parent” of the new node. 

The second, independent step posts the separator key in the parent. The second step can be made a side effect of any future root-to-leaf traversal, should happen as soon as possible, yet may be delayed beyond a system reboot or even a crash and its recovery without data loss or inconsistency of the on-disk data structures (see Figure 6.11 for more details on the permissible states and invariants).

The advantage of Blink-trees is that allocation of a new node and its initial introduction into the B-tree is a local step, affecting only one preexisting node and requiring a latch only on the overflowing node. The disadvantages are that search may be a bit less efficient during the transient state, a solution is needed to prevent long lists of neighbors nodes during periods of high insertion rates, and verification of a B-tree’s structural consistency is more complex and perhaps less efficient.

> ==TODO：==Fig.4.12

Figure 4.12 illustrates a state that is not possible in a standard B-tree but is a correct intermediate state in a Blink-tree. “Correct” here means that search and update algorithms must cope with this state and that a database utility that verifies correct on-disk data structures must not report an error. In the original state, the parent node has three children. Note that these children might be leaves or branch nodes, and the parent might be the B-tree root or a branch node. The first step is to split a child, resulting in the intermediate state shown in Figure 4.12. The second step later places a fourth child pointer into the parent and abandons the neighbor pointer, unless neighbor pointers are required in a specific implementation of B-trees. Note the similarity to a ternary node in a 2-3-tree as shown in Figure 2.2.

In most cases, posting the separator key in the parent node (the second step above) can be a very fast system transaction invoked by the next root-to-leaf traversal. It is not required that this thread be part of an update transaction, because any changes in the B-tree structure will be part of the system transaction, not the user transaction. When a thread holds latches on both parent and child node, it can check for the presence of a separator key not yet posted. If so, it upgrades its latches to exclusive latches, allocates a new entry in the parent node, and moves the separator key from the child to the parent. If another thread holds a shared latch, the operation is abandoned and left to a subsequent root-to-leaf search. If the parent node cannot accommodate another separator key, a new overflow node is allocated, populated, and linked into the parent. Splitting the parent node should be a separate system transaction. If a root-to-leaf search finds that the root node has a linked overflow node, the tree should grow by another level. If any of the required latches cannot be acquired instantaneously, the system transaction may abort and leave it to a later B-tree traversal to post the separator key in the parent node.

In the unlikely event that a node must be split again before a separator key is posted in the parent node, multiple overflow nodes can form a linked list. Long linked lists due to multiple splits can be pre-vented by restricting the split operation to nodes pointed to by the appropriate parent node. These and further details of B^link^-trees have recently been described in a detailed paper [73].

The split process of B^link^-trees can be reversed in order to enable removal of B-tree nodes [88]. The first step creates a neighbor pointer and removes the child pointer from the parent node, whereupon the second step merges the removal victim with its neighbor node. The transient state of B^link^-trees might even be useful for load balancing among sibling nodes and for defragmentation of B-trees, although this idea has not been tried in research prototypes or industrial implementations.

- B^link^-trees relax the strict B-tree structure in order to enable more concurrency. Splitting a node and posting a new separator key in the parent are two separate steps.
- Each step can be a system transaction that commits to make its changes visible to other threads and other transactions. 
- In the transient state between these two steps, the old node is a “foster parent” to the new node. The transient state should be short-lived but may persist if the second step is delayed, e.g., due to concurrency conflicts. 
- B^link^-trees and their transient state may be useful for other structural changes in B-trees, e.g., removal of a node (merging the key ranges of two nodes) and load balancing among two nodes (replacing the separator key). 

### 4.7 Latches During Lock Acquisition

If locks are defined by actual key values in a B-tree, latches must be managed carefully. Specifically, while a transaction attempts to acquire a key range lock, its thread must hold a latch on the data structure in the buffer pool such that the key value cannot be removed by another thread. On the other hand, if the lock cannot be granted immediately, the thread should not hold a latch while the transaction waits. In fact, the statement could be more general: a thread must never wait while holding a latch. Otherwise, multiple threads may deadlock each other. Recall that deadlock detection and resolution is usually provided only for locks but not for latches.

There are several designs to address this potential problem. In one solution, the lock manager invocation, upon detecting a conflict, only queues a lock request and then returns. This lets the thread release appropriate latches prior to invoking the lock manager a second time and waiting for the lock to become available. It is not sufficient to merely fail the lock request in the first invocation. Until the lock request is inserted into the lock manager’s data structures, the latch on the data structure in the buffer pool is required to ensure the existence of the key value.

Another solution passes a function and appropriate function arguments to the lock manager to be called prior to waiting. This call-back function may release the latch on the data structure in the buffer pool, to be re-acquired after the wait and the lock is acquired. In either case, the sequence of actions during the lock request needs to indicate not only success versus failure but also instant versus delayed success.

While a transaction waits for a key value lock without holding the latch on the data structure in the buffer pool, other transactions might change the B-tree structure with splits, merges, load balancing, or page movements, e.g., during B-tree defragmentation or in a write-optimized B-tree on RAID or flash storage [44]. Thus, after waiting for a key value lock, a transaction must repeat its root-to-leaf search for the key. In order to minimize the cost for this repeated search, log sequence numbers of pages along the root-to-leaf path might be retained prior to the wait and verified after the wait. Alternatively, counters (of structure modifications) might be employed to decide quickly whether or not the B-tree structure might have changed [88]. These counters can be part of the system state, i.e., not part of the database state, and there is no need to recover their prior values after a system crash.

- A data page must remain latched while a key value lock is acquired in order to protect the key value from removal, but the latch on the data page must not be retained while waiting for a lock. 
- Solutions require a call-back or repeated lock manager calls, one to insert the lock into the wait queue and one to wait for lock acquisition. 

### 4.8 Latch Coupling

When a root-to-leaf traversal advances from one B-tree node to one of its children, there is a brief window of vulnerability between reading a pointer value (the page identifier of the child node) and accessing the child node. In the worst case, another thread deletes the child page from the B-tree during that time and perhaps even starts using the page in another B-tree. The probability is low if the child page is present in the buffer pool, but it cannot be ignored. If ignored or not implemented correctly, identifying this vulnerability as the cause for a corrupted database is very difficult. Additional considerations apply if the child page is not present in the buffer pool and I/O is required, which is discussed in the next sub-section.

A technique called latch coupling avoids this problem. The root-to-leaf search retains the latch on the parent page, thus protecting the page from updates, until it has acquired a latch on the child page. Once the child page is located, pinned, and latched in the buffer pool, the latch on the parent page is released. If the child page is readily available in the buffer pool, latches on parent and child pages overlap only for a very short period of time.

Latch coupling was invented fairly early in the history of B-trees [9]. For read-only queries, at most two nodes need to be locked (latched) at a time, both in shared mode. In the original design for insertions, exclusive locks are retained on all nodes along a root-to-leaf path until a node is found with sufficient free space to permit splitting a child and posting a separator key. Unfortunately, variable-size records and keys may force very conservative decisions. Instead, newer designs rely on Blink-trees (temporary neighbor pointers until a separator key can be posted) or on repeated root-to-leaf passes. The initial root-to-leaf pass employs shared latches on root and branch nodes even if the intended operation will modify the leaf node with an insertion or deletion.

Systems that rely on neighbor pointers for efficient cursors, scans, and key range locking, i.e., implementations not exploiting fence keys, employ latch coupling also among neighbor nodes. In those systems, multiple threads may attempt to latch a leaf page from different directions, which could lead to deadlocks. Recall that latches usually do not support deadlock avoidance or detection. Therefore, latch acquisition must include a fail-fast no-wait mode and the B-tree code must cope with failed latch acquisitions.

Most root-to-leaf traversals hold latches on at most two B-tree nodes at a time, i.e., a parent and a child. A split operation needs to hold three B-tree latches, including one for the newly allocated node. In addition, it needs to latch the free space information. In B^link^-trees, split operations require only two latches at a time. Even the final operation that moves separator key and pointer from the child to the parent requires only two latches; there is no need to latch the new node. On the other hand, a complete split sequence in a B^link^-trees requires two periods with exclusive latches, even if the final operation can be delayed until the appropriate latches are readily available.

- During navigation from one B-tree node to another, the pointer must remain valid. The usual implementation keeps the source latched until the destination has been latched. 
- If I/O is required, the latch ought to be released. The B-tree navigation might need to be repeated, possibly starting from the root node. 
- B^link^-trees latch at most two nodes at a time, even while splitting a node and while posting a separator key. 

### 4.9 物理逻辑日志

### 4.10 Non-logged Page Operations

Another logging optimization pertains to structural B-tree operations, i.e., splitting a node, merging neighboring nodes, and balancing the load among neighboring nodes. As for in-page compaction, detailed logging can be avoided because those operations do not change the contents of the B-tree, only its representation. Differently from in-page compaction, however, there are multiple pages involved in these operations and the contents of individual pages indeed changes.

The operations considered are actually reflected in the recovery log; in that sense, the commonly used term “non-logged” is not literally accurate. A better descriptive name might be “allocation-only logging” instead. The savings are nonetheless substantial. For example, in strict physical logging, splitting a node of 8 KB might generate 24 KB of log volume plus log record headers, a few short log records for page allocation, and transaction commit, whereas an optimized implementation might required only these few short log records.

The key insight is that the old page contents, e.g., the full page prior to the split, can be employed to ensure recoverability of both pages after the split. Thus, the old contents must be protected against over-writing until the moved contents is safely written. For example, splitting a full page proceeds in multiple steps after the page is loaded into the buffer pool and found to require a split:

1. a new page is allocated on disk and this allocation is logged, 
2. a new page frame for this new disk page is allocated in the buffer pool, 
3. half the page contents is moved to the new page within the buffer pool; this movement is logged with a short log record that does not include the contents of the moved records but probably includes the record count, 
4. the new page is written to the data store, and 
5. the old page is written to the data store with only half its original content remaining, overwriting the old page contents and thus losing the half moved to the other page. 

The careful write ordering in steps 4 and 5 is crucial. This list of actions does not include posting a new separator key in the parent node of the full node and its new sibling. Further optimizations are possible, in particular for Blink-trees [73]. The log records in the list above could be combined into a single log record in order to save space for record headers. The crucial aspect of the above list is that the last action must not be attempted until the prior one is complete. The delay between the first three actions and these last two actions can be arbitrarily long without putting reliability or recoverability in danger.

Variants of this technique also apply to other structural B-tree operations, in particular merging neighboring nodes, balancing the load among neighboring nodes, and moving entries in neighboring leaves or branch nodes in order to re-establish the desired fraction of free space for the optimal tradeoffbetween fast scans and fast future insertions. In all these cases, allocation-only logging as described above can save most of the log volume required in physical logging. More details on non-logged page operations are discussed in Section 6.6.

- “Non-logged” should be taken to mean “without logging page contents.” Another name is “allocation-only logging” or “minimal logging.” 
- When moving records from one page to another (during split, load balancing, or defragmentation), the old page can serve as backup. It must be protected until the destination page is saved on storage. 

### 4.11 Non-logged Index Creation

The term “non-logged index creation” seems to be commonly used although it is not entirely accurate. Changes in the database catalogs and in the free space management information are logged. The content of B-tree pages, however, is not logged. Thus, non-logged index creation saves perhaps 99% of the log volume compared to logged index creation.

All newly allocated B-tree pages, both leaves and branch nodes, are forced from the buffer pool to permanent storage in the database before committing the operation. Images of the B-tree nodes may, of course, remain in the buffer pool, depending on available space and the replacement policy in the buffer pool. Page allocation on disk is optimized to permit large sequential writes while writing the B-tree initially as well as large sequential reads during future index scans.

> ==TODO：==Fig.4.15

Figure 4.15 compares the log volume in logged and non-logged index creation. The voluminous operations, in particular individual record insertions or full B-tree pages, are not logged. For example, instead of millions of records, only thousands of page allocations are logged. Commit processing is slow if pages have been allowed to linger in the buffer pool during load processing. But just as table scans interact badly with LRU replacement in a buffer pool, pages filled during load processing should be ejected from the buffer pool as soon as possible.

Recovery of non-logged index creation requires precise repetition of the original index creation, in particular is space allocation operations, because subsequent user transactions and their log records may refer to specific keys in specific database pages, e.g., during row deletion. When those transactions are recovered, they must find those keys in those pages. Thus, node splits and allocation of database pages during recovery must precisely repeat the original execution

- Since indexes can be very large, logging the entire contents of a new index can exceed the available log space. Most systems have facilities for non-logged creation of secondary indexes. 
- Upon completion, the new index is forced to storage. 
- A backup of the transaction log must include the new index; otherwise, subsequent updates to the new index cannot be guaranteed even if included in the transaction log and in a log backup. 

### 4.12 Online Index Operations

The other important optimization for index creation is online index creation. Without online index creation, other transactions may be able to query the table based on pre-existing indexes; with online index creation, concurrent transactions may also update the table, including insertions and deletions, with the updates correctly applied to the index before index creation commits.

The traditional techniques described here are sufficient for small updates but it remains unadvisable to perform bulk insertions or deletion while concurrently modifying the physical database design with index creation and removal. There are two principal designs: either the concurrent updates are applied to the structure still being built or these updates are captured elsewhere and applied after the main index creation activity is complete. These designs have been called the “no side file” and “side file” [98]. The recovery log may serve as the “side file.” 

Srinivasan and Carey [119] divide online algorithms for index creation further, specifically the “side file” approach. In their comparison study, all concurrent updates are captured in a list or in an index. They do not consider capturing updates in the recovery log or in the target index (the “no side file” approach). Their various algorithms permit concurrent updates throughout the index creation or only during its scan phase. Some of their algorithms sort the list of concurrent updates or even merge it with the candidate index entries scanned and sorted by the index builder. Their overall recommendation is to use a list of concurrent updates (a side file) and to merge it with the candidate index entries of the index builder.

> ==TODO：==Fig.4.16

Figure 4.16 illustrates the data flow for online index creation with and without “side file.” The upper operation starts with creating an empty side file (unless the recovery log serves as side file). Concurrent transactions buffer their updates there, and the entire contents of the side file is propagated after the scanning, sorting, and B-tree loading are complete. The lower operation starts with creating the new index, albeit entirely empty at this time. Concurrent transactions capture both insertions and deletions into the new index, even before and during B-tree loading.

The “side file” design lets the index creation proceed without regard for concurrent updates. This index creation process ought to build the initial index as fast as an oﬄine index creation. The final “catch up” phase based on the “side file” requires either quiescent concurrent update activity or a race between capturing updates and applying them to the new index. Some systems perform a fixed number of catch-up phases, with the first catch-up phase applying updates captured during index creation, with the second catch-up phase applying updates captured during the first catch-up phase, and with the final catch-up phase applying the remaining updates and preventing new ones. 

The “no side file” design requires that the future index be created empty at the start, concurrent updates modify the future index, and the index creation process work around records in the future index inserted by concurrent update transactions. One concern is that the index creation process may not achieve write bandwidth similar to an oﬄine index creation. Another concern is that concurrent update transactions may need to delete a key in a key range not yet inserted by the index creation process. For example, the index creation may still be sorting records to be inserted into the new index.

Such deletions can be represented by a negative or “anti-matter” record. When the index creation process encounters an anti-matter record, the corresponding record is suppressed and not inserted into the new index. At that time, the anti-matter record has served its function and is removed from the B-tree. When the index creation process has inserted all its records, all anti-matter records must have been removed from the B-tree index. 

An anti-matter record is quite different from a ghost record. A ghost record represents a completed deletion, whereas an anti-matter record represents an incomplete deletion. Put in another way, an anti-matter record indicates that the index creation process must suppress a seemingly valid record. If one were to give weight to records, a valid record would have weight +1, a ghost record would have weight 0, and an anti-matter record would have weight *−*1.

It is possible that a second concurrent transaction inserts a key previously deleted by means of leaving an anti-matter record. In that case, a valid record with a suppression marker is required. The suppression marker indicates that the first transaction performed a deletion; the remainder of the valid record contains the information inserted by the second transaction into the database. A third concurrent transaction may delete this key again. Thus, the suppression marker and the ghost record are entirely orthogonal, except that a ghost record with a suppression marker must not be removed like other ghost records because that would lose the suppression information.

> ==TODO：==Fig.4.17

Figure 4.17 illustrates use of ghost bit and anti-matter bit during online index creation without side-file. Keys 47 and 11 are both updated in the index before the index creation process loads index entries with those key values. This bulk load is shown in the last two entries of Figure 4.17. The history of key value 47 starts with an insertion; thus, it never has the anti-matter bit set. The history of key value 11 starts with a deletion, which necessarily must refer to a future index entry to be loaded by the index creation process; thus, this key value retains its anti-matter bit until it is canceled against a record in the load stream. The final result for key value 11 can be an invalid (ghost) record or no record at all.

In materialized summary (“group by”) views, however, ghost marker and suppression marker can be unified into a single counter that serves a role similar to a reference count [55]. In other words, if its reference count is zero, the summary record is a ghost record; if its reference count is negative, the summary record implies suppression semantics during an online index creation. In non-unique indexes with a list of references for each unique key value, an anti-matter bit is required for individual pairs of key value and reference. The count of references for a unique key value can be used similar to a ghost bit, i.e., a key value can be removed if and only if the count is zero.

 Maintenance of indexes whose validity is in doubt applies not only to online index creation but also to index removal, i.e., dropping an index from a database, with two additional considerations. First, if index removal occurs within a larger transaction, concurrent trans-actions must continue standard index maintenance. This is required as long as the transaction including the index removal could still be aborted. Second, the actual de-allocation of database pages can be asynchronous. After the B-tee removal has been committed, updates by concurrent transaction must stop. At that time, an asynchronous utility may scan over the entire B-tree structure and inserts pages into the data structure with free space information. This process might be broken into multiple steps that may occur concurrently or with pauses in between steps.

Finally, online index creation and removal as described above can easily be perceived by database users as merely the first step. The techniques above require one or two short quiescent periods of time at beginning and end. Exclusive locks are required on the appropriate database catalogs for the table or index. Depending on the application, its transaction sizes and its response time requirements, these quiescent periods may be painfully disruptive. An implementation of “fully online” index operations probably requires multi-value concurrency control for the database catalogs and the cache of pre-compiled query execution plans. No such implementation has been described in the literature. 

- Online index operations permit updates by concurrent trans-actions while future index entries are extracted, sorted, and inserted into the new index. Most implementations lock the affected table and its schema while the new index is inserted in the database catalogs and during final transaction commit. 
- Updates by concurrent transactions may be applied to the new index immediately (“no side file”) or after initial index creation is complete (“side file”). The former requires “anti-matter” records to reflect that the history of a key value in an index started with a deletion; the latter requires “catch-up” operations based on a log of the updates. 

### 4.13 事务隔离级别

### 4.14 小结

In summary, necessity has been spawning many inventions that improve concurrency control, logging, and recovery performance for databases based on B-tree indexes. The separation of locking and latching, of database contents and in-memory data structures, is as important as key range locking aided by ghost records during deletion and possibly also insertion. Reducing log volume during large index utilities, in particular non-logged (or allocation-only logged) index creation, prevents the need for log space almost as large as the database but it introduces the need to force dirty pages from the buffer pool. Finally, weak transaction isolation levels might seem like a good idea for increased concurrency but they can introduce wrong query results and, when used in updates that compute the change from the database, wrong updates to the database.

Perhaps the most urgently needed future direction is simplification. Functionality and code for concurrency control and recovery are too complex to design, implement, test, debug, tune, explain, and maintain. Elimination of special cases without a severe drop in performance or scalability would be welcome to all database development and test teams.