# 第 7 章 Spark SQL 之 Aggregation 实现

聚合操作（ Aggregation ）指的是在原始数据的基础上按照一定的逻辑进行整合，从而得到新的数据，一般通过聚合函数（如 count、max 和 sum 等）汇总多行的信息。聚合查询一直以来都是数据分析应用中必不可少的部分，在各种 SQL 算子中占据着重要地位，本章对 Spark SQL 实现聚合查询的内部机制进行详细分析。

## 7.1 Aggregation 执行概述

一条 SQL 语句的执行会经历**逻辑计划**和**物理计划**的各个阶段，同样的，聚合查询也不例外。本节从全局视角出发，纵向考察 Aggregation 执行的整个流程（从 SQL 语句文法定义到物理执行），并在后续小节中对其中的重要技术点进行分析。

### 7.1.1　文法定义

在 Catalyst 的 SqlBase.g 4 文法文件中，聚合语句 aggregation 定义如下：在常见的聚合查询中，通常包括分组语句（group by）和聚合函数（ aggregate function ）；聚合函数出现在 Select 语句中的情形较多，定义在 `functionCall` 标签的 `primaryExpression` 表达式文法中， `qualifiedName` 对应函数名，括号内部是该函数的参数列表。

![img](https://pic2.zhimg.com/v2-1544ad107534c5b313029849b08a3ee4_r.jpg)

从上述文法定义中可以看到，完整的聚合查询的关键字包括 `group by`、`cube`、 `grouping sets` 和 `rollup` 4 种。分组语句 group by 后面可以是一个或多个分组表达式（ `groupingExpressions` ）。除简单的分组操作外，聚合查询还支持 OLAP 场景下的多维分析，包括 `rollup`、`cube` 和 `grouping sets` 3 种操作。

为了分析方便，本章以一个简单聚合查询实例作为研究对象。数据仍然使用第 3 章中创建的 student 关系表，在查询语句中加入聚合操作：按照表中的 id 列分组，并对每个分组的数据使用 count 聚合函数计算数据条数。

```SQL
select id, count(name) from student group by id
```

上述聚合查询语句生成的语法树如图 7.1 所示。相对于第 3 章中的非聚合查询，该语法树除 `id` 列外，还有对 `name` 的 `count` 操作所产生的新列，因此 `NamedExpressionSeqContext` 节点包含两个子节点。此外，代表数据表信息的 `From ClauseContext` 子树没有变化，仍然是 `QuerySpecificationContext` 节点的第 2 个子节点。

<p align="center">
 <img src="https://pic2.zhimg.com/v2-d66a1bb716202490fdd198a4a71e84ab_r.jpg" />
 图 7.1　聚合查询抽象语法树
</p>

加入聚合操作后的语法树最重要的元素是 `FunctionCallContext` 节点和 `AggregationContext` 节点。 `AggregationContext` 节点反映在语法树中即图 7.1 中 `QuerySpecificationContext` 节点下的第 3 个子节点，其子节点（从 `ExpressionContext` 一直到 ColumnReferenceContext ）对应 `group by` 语句后面的 id 列。用来表示聚合函数的 `FunctionCallContext` 节点的结构比较好理解，其子节点 `QualifiedNameContext` 代表函数名， `ExpressionContext` 表示函数的参数表达式（对应 **SQL** 语句中的 `name` 列）。

### 7.1.2　聚合语句 Unresolved LogicalPlan 生成

从上述抽象语法树生成 *Unresolved LogicalPlan* 是接下来的重要一步，该过程主要由 **AstBuilder** 完成。在分析 **AstBuilder** 具体动作之前，先简要了解一下后续聚合操作会用到的 Aggregate 逻辑算子树节点。

```scala
case class Aggregate(
    groupingExpressions: Seq[Expression],
    aggregateExpressions: Seq[NamedExpression],
    child: LogicalPlan)
  extends UnaryNode {
// ...
}
```

在 Spark SQL 中， `Aggregate` 逻辑算子树节点是 `UnaryNode` 中的一种，属于基本的逻辑算子（ ***basicLogicalOperator*** ）。如[图 7.2]() 所示，该逻辑算子树节点通过分组表达式列表（ `groupingExpressions` ）、聚合表达式列表（ `aggregateExpressions` ）和子节点（`child`）构造而成，其中分组表达式类型都是 `Expression`，而聚合表达式类型都是 `NamedExpression` ，<u>意味着聚合表达式一般都需要设置名字</u>。同时，`Aggregate` 的输出函数 `output` 对应**聚合表达式列表中**的所有**属性值**。判断一个**聚合算子**是否已经被解析过（ `resolved=true` ）需要满足 3 个条件。

- 该算子中的所有表达式都已经被解析过了。
- 其子节点已经被解析过了。
- ==该节点中不包含窗口（Window）函数表达式==。

<p align="center">
 <img src="https://pic2.zhimg.com/v2-460a93c4069487bb12446cb159d1519d_r.jpg" />
 图 7.2　逻辑算子树聚合算子
</p>

如图 7.3 所示，上述聚合查询从抽象语法树生成 Unresolved LogicalPlan 主要涉及以下 3 个函数的调用。

- 针对 QuerySpecificationContext 节点，执行 visitQuerySpecification ，会先后调用 visitFrom-Clause 和 withQuerySpecification 函数。

- 在 visitFrom Clause 函数中，针对 From ClauseContext 节点生成 UnresolvedRelation 逻辑算子节点，对应数据表。

- 在返回的 UnresolvedRelation 节点上，执行 withQuerySpecification 函数，实际上这里具体执行的是 withAggregation 函数，在 UnresolveRelation 节点上生成 Aggregate 逻辑算子树节点，返回完整的逻辑算子树。


对于上述 Unresolved LogicalPlan 的生成过程，读者有必要关注一下 visitFunctionCall 这个方法。 visitFunctionCall 方法由 visitNamedExpression 调用，属于 Select 语句中的一部分，用来生成聚合函数对应的表达式。最终得到的 UnresolvedFunction 包含了聚合函数名、参数列表和是否包含 distinct 的布尔值。

<p align="center">
 <img src="https://pic3.zhimg.com/v2-c7576d1de695b43bb23d628998baafc4_r.jpg" />
 图 7.3　逻辑算子树生成
</p>
### 7.1.3　从逻辑算子树到物理算子树
生成 Unresolved LogicalPlan 之后，就是逻辑算子树到物理算子树的各个阶段。上述聚合语句同样涉及 Analyzed LogicalPlan 、 Optimized LogicalPlan 、 SparkPlan 和 ExecutedPlan 多个步骤，如[图 7.4]() 所示。由于前面章节已经对这些步骤中的细节详细阐述过，所以本小节仅对与聚合查询相关的一些规则进行分析。

从 Unresolved LogicalPlan 到 Analyzed LogicalPlan 经过了 4 条规则的处理。对于聚合查询来说，比较重要的是其中的 ResolveFunctions 规则，用来分析聚合函数。<u>对于 UnresolvedFunction 表达式，Analyzer 会根据函数名和函数参数去 SessionCatalog 中查找，而 SessionCatalog 会根据 FunctionRegistry 中已经注册的函数信息得到对应的聚合函数（ AggregateFunction ）</u>。值得注意的是，在 FunctionRegistry 中定义了 `expression[T<:Expression](name:String)(implicit tag:ClassTag[T])`函数，与 HashMap 一样，对传入的 `Seq[Expression]` 函数参数列表进行判断，如果查找到的函数接受的是多个 Expression 参数，则参数列表会拆分为多个 Expression 参数来完成对 AggregateFunction 的构造，最后返回给 SessionCatalog 。

<p align="center">
 <img src="https://pic3.zhimg.com/v2-a927fc21ee0625741279a22505704ef3_r.jpg" />
图 7.4　从逻辑算子树到物理算子树概览
</p>

从 Analyzed LogicalPlan 到 Optimized LogicalPlan 分别经过了**别名消除**（ EliminateSubquery-Aliases ）规则与列剪裁（ ColumnPruning ）规则的处理，这里不再赘述。而从 Optimized LogcialPlan 到物理执行计划 SparkPlan 进行转换时，主要经过了 FileSourceStrategy 和 Aggregation 这两个策略（Strategy）的处理。 FileSourceStrategy 会应用到 Project 和 Relation 节点，匹配过程在第 6 章已经分析过，在此不再赘述。

![img](https://pic1.zhimg.com/v2-ad53cbdf1d0a19a81d9eb8f53c8e106e_r.jpg)
![img](https://pic1.zhimg.com/v2-40684f944b07bfc4818277f3376ddf41_r.jpg)

对于聚合查询，逻辑算子树转换为物理算子树，必不可少的是 Aggregation 转换策略。实际上， Aggregation 策略是基于 PhysicalAggregation 的。如[图 7.5]() 所示，与 `PhysicalOperation` 类似， `PhysicalAggregation` 也是一种逻辑算子树的模式，用来匹配逻辑算子树中的 Aggregate 节点并提取该节点中的相关信息。 PhysicalAggregation 在提取信息时会进行以下转换。

<p align="center">
 <img src="https://pic2.zhimg.com/v2-5381c0ce93674a15f1378845c07ce5b7_r.jpg" />
图 7.5　聚合算子匹配模式
</p>

- **去重**：对 Aggregate 逻辑算子节点中多次重复出现的聚合操作进行去重，参见 `PhysicalAggregation` 中 `aggregateExpressions` 表达式的逻辑，收集 `resultExpressions` 中的聚合函数表达式，然后执行 distinct 操作。
- **命名**：参见上述代码中 namedGroupingExpressions 的操作，对未命名的分组表达式（ Grouping expressions ）进行命名（套上一个 Alias 表达式），这样方便在后续聚合过程中进行引用。
- **分离**：对应 rewrittenResultExpressions 中的操作逻辑，从最后结果中分离出聚合计算本身的值，例如「count+1」会被拆分为 count（ AggregateExpression ）和「 count.resultAttribute +1」的最终计算。

经过上述处理， `PhysicalAggregation`  模式返回的聚合操作相关表达式如[表 7.1](https://pic2.zhimg.com/v2-baf0ae51e5a3339a8cd3b8b815816bfb_r.jpg) 所示，其中 `aggregateExpressions` 对应聚合函数，而 resultExpressions 则包含了 Select 语句中选择的所有列信息。

<p align="center">
 <img src="https://pic2.zhimg.com/v2-baf0ae51e5a3339a8cd3b8b815816bfb_r.jpg" />
表 7.1 PhysicalAggregation 匹配模式表达式生成
</p>

得到上述各种聚合信息之后， Aggregation 策略会根据这些信息生成相应的物理计划。如图 7.6 所示，不同情况下生成的物理计划也不相同。当聚合表达式中存在不支持 Partial 方式且不包含 Distinct 函数时，调用的是 `planAggregateWithoutPartial` 方法；当聚合表达式都支持 Partial 方式且不包含 Distinct 函数时，调用的是 `planAggregateWithoutDistinct` 方法；当聚合表达式都支持 `Partial` 方式且存在 `Distinct` 函数时，调用的是 `planAggregateWithOneDistinct` 方法。

<p align="center">
 <img src="https://pic1.zhimg.com/v2-5149a2d7f18099491f3bf78c5684b2c4_r.jpg" />
图 7.6　聚合算子生成策略
</p>

这里的 Partial 方式表示聚合函数的模式，能够支持预先局部聚合，这方面的内容会在下一节详细介绍。对应实例中的聚合语句，因为 count 函数支持 Partial 方式，因此调用的是 planAggregateWithoutDistinct 方法，生成了图 7.4 中的两个 HashAggregate （聚合执行方式中的一种，后续详细介绍）物理算子树节点，分别进行局部聚合与最终的聚合。最后，在生成的 SparkPlan 中添加 Exchange 节点，统一排序与分区信息，生成物理执行计划（ ExecutedPlan ）。

本章后续内容安排如下：在 7.2 节中会对聚合函数进行详细分析，包括聚合函数缓冲区和不同的聚合模式介绍，以及聚合函数的各种类型；7.3 节重点分析聚合执行的技术原理；7.4 节介绍窗口函数实现机制；7.5 节是 OLAP 场景的多维分析实现；最后在 7.6 节进行全章内容的总结。

## 7.2　聚合函数（ AggregateFunction ）

聚合函数（ `AggregateFunction` ）是聚合查询中非常重要的元素。在实现上，聚合函数是表达式中的一种，和 **Catalyst** 中定义的聚合表达式（ `AggregateExpression` ）紧密关联。无论是在逻辑算子树还是物理算子树中，聚合函数都是以聚合表达式的形式进行封装的，同时聚合函数表达式中也定义了直接生成聚合表达式的方法。

聚合表达式（ `AggregateExpression` ）的成员变量和函数如图 7.7 所示，根据命名，这些变量和函数的含义很好理解。例如， resultAttribute 表示聚合结果，获取子节点的 children 方法并返回聚合函数表达式；dataType 函数直接调用聚合函数中的 dataType 函数获取数据类型。在默认情况下，聚合表达式的 foldable 函数返回的是 false，因为聚合表达式一般无法静态得到最终的结果，需要经过进一步的计算。

<p align="center">
 <img src="https://pic4.zhimg.com/v2-28f6573ee001e02e3f8c10d563278706_r.jpg" />
图 7.7 <B>Aggregation</B> 表达式
</p>

### 7.2.1　聚合缓冲区与聚合模式（ AggregateMode ）

聚合查询在计算聚合值的过程中，通常都需要保存相关的中间计算结果，例如 `max` 函数需要保存当前最大值，`count` 函数需要保存当前的数据总数，求平均值的 `avg` 函数需要同时保存 `count` 和 `sum` 的值，更复杂的函数（如 `pencentil` 等）甚至需要临时存储全部的数据。聚合查询计算过程中产生的这些中间结果会临时保存在聚合函数缓冲区。

#### 1.聚合函数缓冲区

聚合函数缓冲区是指在同一个分组的数据聚合的过程中，用来保存聚合函数计算中间结果的内存空间。

聚合函数缓冲区的定义有一个前提条件，即聚合函数缓冲区针对的是处于同一个分组内（实例中属于同一个 id）的数据。需要注意的是，查询中可能包含多个聚合函数，因此聚合函数缓冲区是多个聚合函数所共享的。

在聚合函数的定义中，与聚合缓冲区相关的基本信息包括：聚合缓冲区的 Schema 信息（ `aggBufferSchema` ），返回为 `StructType` 类型；聚合缓冲区的数据列信息（ `aggBufferAttributes` ），返回的是 `AttributeReference` 列表（`Seq[ AttributeReference ]`），对应缓冲区数据列名。显然，聚合函数缓冲区中的值会随着数据处理而不断进行更新，因此该缓冲区是可变的（**Mutable**）。此外，当聚合函数处理新的数据行时，需要知道该数据行的列构成信息，在 `AggregateFunction` 中也定义了 `inputAggBufferAttributes` 函数来获得输入数据的组成情况。通常情况下， `inputAggBuffereAttributes` 返回的都是自动从 `aggBufferAttributes` 获得的结果。

#### 2.聚合模式

在 Spark SQL 中，聚合过程有 4 种模式，分别是 Partial 模式、 ParitialMerge 模式、Final 模式和 Complete 模式。

**Final 模式一般和 Partial 模式组合在一起使用**。Partial 模式可以看作是局部数据的聚合，在具体实现中，Partial 模式的聚合函数在执行时会根据读入的原始数据更新对应的聚合缓冲区，当处理完所有的输入数据后，返回的是聚合缓冲区中的中间数据。而 Final 模式所起到的作用是将聚合缓冲区的数据进行合并，然后返回最终的结果。如图 7.8 所示，在最终分组计算总和之前，可以先进行局部聚合处理，这样能够避免数据传输并减少计算量。因此，上述聚合过程中在 map 阶段的 sum 函数处于 Partial 模式，在 reduce 阶段的 sum 函数处于 Final 模式。

<p align="center">
 <img src="https://pic2.zhimg.com/v2-958e4a278451260870ca6b2917c088de_r.jpg" />
图 7.8　Partial 与 Final 聚合模式
</p>

**Complete 模式**和上述的 Partial/Final 组合方式不一样，不进行局部聚合计算。图 7.9 展示了同样的聚合函数采用 Complete 模式的情形。可以看到，最终阶段直接针对原始输入，中间没有局部聚合过程。一般来讲，在不支持 Partial 模式的聚合函数中应用 Complete 模式。

<p align="center">
 <img src="https://pic2.zhimg.com/v2-6bafc0018e5c5c8b392908230ca53d6c_r.jpg" />
图 7.9　Complete 聚合模式
</p>

相比 Partial、Final 和 Complete 模式， **PartialMerge 模式的聚合函数**主要是对聚合缓冲区进行合并，但此时仍然不是最终的结果。 ParitialMerge 主要应用在 distinct 语句中，如图 7.10 所示。聚合语句针对同一张表进行 sum 和 count（distinct）查询，最终的执行过程包含了 4 步聚合操作。第 1 步按照（A，C）分组，对 sum 函数进行 Partial 模式聚合计算；第 2 步是 PartialMerge 模式，对上一步计算之后的聚合缓冲区进行合并，但此时仍然不是最终的结果；第 3 步分组的列发生变化，再一次进行 Partial 模式的 count 计算；第 4 步完成 Final 模式的最终计算。

<p align="center">
 <img src="https://pic2.zhimg.com/v2-9f0e38ee9c56f6c5c4dc2fccf4ca4930_r.jpg" />
图 7.10 PartialMerge 聚合模式
</p>

### 7.2.2 DeclarativeAggregate 聚合函数

`DeclarativeAggregate` 聚合函数是一类直接由 Catalyst 中的表达式（ Expressions ）构建的聚合函数，主要逻辑通过调用 4 个表达式完成，分别是 initialValues （聚合缓冲区初始化表达式）、 updateExpressions （聚合缓冲区更新表达式）、 mergeExpressions （聚合缓冲区合并表达式）和 evaluateExpression （最终结果生成表达式）。下面以 Count 函数为例对这种类型的聚合函数的实现进行说明。

![img](https://pic3.zhimg.com/v2-ca01f31bbc990af85703b29729c357eb_r.jpg)

通常来讲，实现一个基于表达式的 DeclarativeAggregate 函数包含以下几个重要的组成部分。

- 定义一个或多个聚合缓冲区的聚合属性（ bufferAttribute ），例如 count 函数中只需要 count，这些属性会在 updateExpressions 等各种表达式中用到。
- 设定 DeclarativeAggregate 函数的初始值，count 函数的初始值为 0。
- 实现数据处理逻辑表达式 updateExpressions ，在 count 函数中，当处理新的数据时，上述定义的 count 属性转换为 Add 表达式，即 count+1L，注意其中对 Null 的处理逻辑。
- 实现 merge 处理逻辑的表达式，函数中直接把 count 相加，对应上述代码中的「 count.left + count.right 」，由 DeclarativeAggregate 中定义的 RichAttribute 隐式类完成。
- 实现结果输出的表达式 evaluateExpression ，返回 count 值。

### 7.2.3 ImperativeAggregate 聚合函数

不同于基于 Catalyst 表达式实现 `DeclarativeAggregate` 聚合函数方式， `ImperativeAggregate` 聚合函数需要显式地实现 `initialize` 、`update` 和 `merge` 方法来操作聚合缓冲区中的数据。一个比较显著的不同是， `ImperativeAggregate` 聚合函数所处理的聚合缓冲区本质上是基于行（ `InternalRow` 类型）。

聚合缓冲区是共享的，可能对应多个聚合函数，因此特定的 `ImperativeAggregate` 聚合函数会通过偏移量进行定位。例如，数据表有 3 列，分别是 key、x、y，查询语句中有两个求平均值的函数 avg（x）和 avg（y）（注：假设这里用 `ImperativeAggregate` 的方式来实现平均值函数）。这两个函数共享聚合缓冲区`[sum1，count1，sum2，count2]`，如图 7.11 所示，那么第一个 avg 函数的缓冲区偏移量为 0，第二个 avg 函数的缓冲区偏移量为 2，可以通过 `mutableAggBufferOffset + fieldNumber` 方式来访问具体的中间变量。

<p align="center">
 <img src="https://pic2.zhimg.com/v2-a092fe1d1181c743956b57bb5ca7959e_r.jpg" />
图 7.11 ImperativeAggregate 聚合函数
</p>

在 `ImperativeAggregate` 聚合函数中，还有输入聚合缓冲区（ `InputAggBuffer` ）的概念。 `InputAggBuffer` 是不可变的，在聚合处理过程中将两个聚合缓冲区进行合并的时候，实现方式就是将该缓冲区的值更新到可变的聚合缓冲区中。除不可变外， InputAggBuffer 中相对聚合缓冲区还可能包含额外的属性，例如 group by 语句中的列，对应的缓冲区即[key，sum1，count1，sum2，count2]。因此，在 ImperativeAggregate 聚合函数中还有 inputAggBuferOff set 的概念，用来访问 InputAggBuffer 中对应的中间值。

### 7.2.4 TypedImperativeAggregate 聚合函数

`TypedImperativeAggregate[T]` 聚合函数允许使用用户自定义的 Java 对象 T 作为内部的聚合缓冲区，因此这种类型的聚合函数是最灵活的。不过需要注意的是， Typed ImperativeAggregate 聚合缓冲区容易带来与内存相关的问题。通常来讲， Typed ImperativeAggregate 聚合函数的执行逻辑分为以下几步。

1. **初始化聚合缓冲区对象**。聚合算子执行框架会调用 `initialize` 函数创建一个空的聚合缓冲区。 `initialize` 函数会执行 `createAggregationBuffer` 函数来获得初始化的缓冲区对象，并将其设置**在全局的缓冲区中**。
2. **处理输入数据**
   1. 如果当前聚合函数的聚合模式是 **Partial 或 Complete** ，则执行框架会调用 `update` 函数来处理输入的数据行。在 `update` 函数的实现逻辑中，会从**全局缓冲区**获得缓冲对象，然后对其数据进行更新。
   2. 如果当前聚合函数的聚合模式是 **PartialMerge 或 Final**，则执行框架会调用 `merge` 函数来处理数据，所处理的数据来自于其他节点序列化后的缓冲区对象。在 `merge` 函数的实现逻辑中，会将二进制数据反序列化成对应的 Java 对象，然后进行合并操作
3. **输出结果**
   1. 如果当前聚合函数的聚合模式是 **Partial 或 PartialMerge** ，则执行框架会调用 `serializeAggregateBufferInPlace` 函数将全局聚合缓冲区中的 Java 对象替换为序列化后的二进制数据，并将它们 Shuffle 到其他的节点。
   2. 如果当前聚合函数的聚合模式是 **Final 或 Complete**，则执行框架会调用 eval 函数来计算最终的结果并将结果返回。

*Note* ： Typed ImperativeAggregate 聚合函数采用二进制数据类型 BinaryType 作为聚合缓冲区的存储方式，而 BinaryType 不支持基于 hash 的聚合方式，因此当聚合算子中包含 Typed Imperative-Aggregate 聚合函数时，一般实现为 **sortaggregation** 方式。

## 7.3　聚合执行

聚合执行本质上是将 RDD 的每个 Partition 中的数据进行处理。如图 7.12 所示，对于每个 Partition 中的输入数据即 Input（通过 `InputIterator` 进行读取），经过聚合执行计算之后，得到相应的结果数据即 Result（通过 Aggregation Iterator 来访问）。

<p align="center">
 <img src="https://pic1.zhimg.com/v2-c683a410549736693bcdcce57059ce62_r.jpg" />
图 7.12　聚合执行
</p>

在 Spark 2.1 版本中，聚合查询的最终执行有两种方式：基于排序的聚合执行方式（ `SortAggregateExec` ）与基于 Hash 的聚合执行方式（ `HashAggregateExec` ）。在后续版本中，又加入了 `ObjectHashAggregateExec` 的执行方式（SPARK-17949），读者可自行研究其实现方式。常见的聚合查询语句通常采用 HashAggregate 方式，当存在以下几种情况时，会用 SortAggregate 方式来执行。

- 查询中存在**不支持 Partial 方式的聚合函数**：此时会调用 AggUtils 中的 `planAggregateWithoutPartial` 方法，直接生成 SortAggregateExec 聚合算子节点。
- 聚合函数结果不支持 Buffer 方式：如果结果类型不属于[NullType， BooleanType ，Byte-Type，ShortType， IntegerType ，LongType，FloatType， DoubleType ，DateType， TimestampType ， DecimalType ]集合中的任意一种，则需要执行 SortAggregateExec 方式，例如 collect_set 和 collect_list 函数。
- 内存不足：如果在 HashAggregate 执行过程中，内存空间已满，那么聚合执行会切换到 SortAggregateExec 方式，这种情况将在 7.3.3 小节进行分析。

聚合查询的执行过程有一个通用的框架，主要接口定义在图 7.12 中的 Aggregation Iterator 中。本节后续内容首先介绍 Aggregation Iterator 的实现，然后分别分析 SortAggregate 与 HashAggregate 的技术实现原理。

### 7.3.1　执行框架 Aggregation Iterator

聚合执行框架指的是聚合过程中抽象出来的通用功能，包括聚合函数的初始化、聚合缓冲区更新合并函数和聚合结果生成函数等。这些功能都在聚合迭代器（ Aggregation Iterator ）中得到了实现。

如[图 7.13]() 所示，聚合迭代器定义了 3 个重要的功能，分别是

1. **聚合函数初始化**： `initializeAggregateFunctions`
2. **生成数据处理函数**：`generateProcessRow`
3. **生成聚合结果输出函数**：`generateResultProjection`

 `SortBasedAggregationIterator` 和 `TungstenAggregationIterator` 继承自 `AggregationIterator` ，实现具体的操作，分别对应 `SortAggregateExec` 和 `HashAggregateExec` 两种执行方式。在 `SortBasedAggregationIterator` 和 `TungstenAggregationIterator` 中分别通过 processCurrentSortedGroup 与 processInputs 方法得到最终的聚合结果，而这两个方法均依赖上述 Aggregation Iterator 功能。

**<u>第一个功能聚合函数初始化可以细分为两个阶段</u>**，分别得到 `funcWithBoundReference` 和 `funcWithUpdatedAggBufferOffset` 表达式。**第一阶段**， `funcWithBoundReference` 执行 `bindReference` **或**设置聚合缓冲区偏移量。针对 **Partial 和 Complete** 模式的 `ImperativeAggregate` 聚合函数， `AttributeReference` 表达式会转换为 `BoundReference` 表达式。例如，假设 `Count(A)` 处理的输入数据行为整型的（A，B，C），经过转换后，得到的是 `Count(BoundReference [0，Int，false])`，提取出的是属性下标等信息；而对于 **PartialMerge 和 Final** 模式的 `ImperativeAggregate` 聚合函数，会设置输入缓冲区的偏移量 `withNewInputAggBufferOffset` 。**第二阶段**， `funcWithUpdatedAggBufferOffset` 设置 `ImperativeAggregate` 函数聚合缓冲区的偏移量 `withNewMutableAggBufferOffset` 。

<p align="center">
 <img src="https://pic4.zhimg.com/v2-341ccbb46c62fb36399bfc2152893916_r.jpg" />
图 7.13　执行框架 Aggregation Iterator
</p>

**第二个功能生成数据处理函数，得到数据处理函数** `processRow` ，其参数类型是（ InternalRow ， InternalRow ），分别代表当前的聚合缓冲区 **currentBufferRow** 和**输入数据行 row**，输出是 Unit 类型。数据处理函数 processRow 的核心操作是获取各 Aggregation 中的 update 函数或 merge 函数。

1. 对于 **Partial** 和 **Complete** 模式，处理的是原始输入数据，因此采用的是 update 函数
2. 对于 **Final** 和 **PartialMerge** 模式，处理的是聚合缓冲区，因此采用的是 merge 函数。

```scala
val updateFunctions = functions.zipWithIndex.collect {
  case (ae: ImperativeAggregate, i) =>
  expressions(i).mode match {
    case Partial | Complete =>
    (buffer: InternalRow, row: InternalRow) => ae.update(buffer, row)
    case PartialMerge | Final =>
    (buffer: InternalRow, row: InternalRow) => ae.merge(buffer, row)
  }
}.toArray
```

**第三个功能生成聚合结果输出函数，计算最终的聚合结果**，输入类型是（UnsafeRow， InternalRow ），输出的是 `UnsafeRow` 最终的数据类型。

1. 对于 **Partial** 或 **PartialMerge** 模式的聚合函数，因为只是中间结果，所以需要保存 grouping 语句与 buffer 中所有的属性
2. 对于 **Final** 和 **Complete** 聚合模式，直接对应 `resultExpressions` 表达式。特别注意，如果不包含任何聚合函数且只有分组操作，则直接创建 projection 。

### 7.3.2　基于排序的聚合算子 SortAggregateExec

基于排序的聚合算子 SortAggregateExec 的实现比较简单。顾名思义，这是一种基于排序的聚合实现，在进行聚合之前，会根据 grouping key 进行分区并在分区内排序，将具有相同 groupingkey 的记录分布在同一个 partition 内且前后相邻。如图 7.14 所示，聚合时只需要顺序遍历整个分区内的数据，即可得到聚合结果。

通过查看 SortAggregateExec 实现可知， requiredChildOrdering 中对输入数据的有序性做了约束，分组表达式列表（ groupingExpressions ）中的每个表达式 e 都必须满足升序排列，即 SortOrder （e，Ascending），因此在 SortAggregateExec 节点之前通常都会添加一个 SortExec 节点。 SortBasedAggregation Iterator 是 SortAggregateExec 实现的关键，由于数据已经预先排好序，因此实现相对简单，按照分组进行聚合即可。在其具体实现中， currentGroupingKey 和 nextGroupingKey 分别表示当前的分组表达式和下一个分组表达式， sortBasedAggregationBuffer 为其聚合缓冲区。比较重要的方法是 initialize 和 processCurrentSortedGroup ，用来初始化基本信息和当前分组数据的处理。

<p align="center">
 <img src="https://pic1.zhimg.com/v2-bcfce22fc2eb18aca81600da79445f71_r.jpg" />
图 7.14　基于排序的聚合算子 SortAggregateExec 的执行过程
</p>

分组数据处理算法逻辑如 Algorithm 1 所示，由算法可知，在 while 循环过程中，不断获取输入数据并将其赋值给 currentRow ，执行 groupingProjection 得到 groupingKey 分组表达式。如果当前的分组表达式 currentGroupingKey 和 groupingKey 相同，那么意味着当前输入数据仍然属于同一个分组内部，因此调用 Aggregation Iterator 中得到的 processRow 函数来处理当前数据。注意，数据处理之前首先通过 safeProj 将 currentRow 从 Unsafe 类型转换为 Safe 类型。

### 7.3.3　基于 Hash 的聚合算子 HashAggregateExec
`HashAggregateExec` 从逻辑上很好实现，只要构建一个 Map 类型的数据结构，以分组的属性作为 key，将数据保存到该 Map 中并进行聚合计算即可。然而，在实际系统中，无法确定性地申请到足够的空间来容纳所有数据，底层还涉及复杂的内存管理，因此相对 SortAggregateExec 的实现方式反而更加复杂。

![img](https://pic1.zhimg.com/v2-4b793dd46ddc3c414a519bcd568f5239_r.jpg)

观察 HashAggregateExec 的实现， requiredChildDistribution 对输入数据的分布做了约束，如果存在分区表达式，那么数据分布必须是 ClusteredDistribution 类型。类似 SortAggregateExec 中的情形， HashAggregateExec 的实现关键在于 TungstenAggregation Iterator 类，如图 7.15 所示。整体实现机制很容易理解，核心之处在于 UnsafeFixedW idthAggregationMap 这种特殊的 Map 数据结构。

<p align="center">
 <img src="https://pic4.zhimg.com/v2-472e5c6e499d0b9e7fcba8acc256fcfe_r.jpg" />
图 7.15　基于 Hash 的聚合执行过程 HashAggregateExec
</p>

实际上， HashAggregateExec 可能因为内存不足的原因退化为 SortAggregateExec 执行方式，因此 TungstenAggregation Iterator 需要处理各种场景，相比 SortBasedAggregation Iterator 的实现要复杂得多。从整体上来看， TungstenAggregation Iterator 代码包含 8 部分内容。

- 聚合函数初始化相关的操作。
- 设置聚合缓冲区、数据处理和聚合结果生成的各种方法。
- 执行基于 Hash 聚合方式的各种方法。
- 切换到基于排序聚合的各种方法。
- 执行基于排序聚合方式的各种方法。
- 加载输入数据并处理输入数据的各种方法。
- 提供给外部调用的 Iterator 公共方法。
- 在不包含分组表达式或没有输入的情况下生成结果的辅助方法。

TungstenAggregationIterator 通过执行 processInputs 方法触发聚合操作，其主要逻辑如 Algorithm 2 所示。当分组表达式 groupingExpressions 为空时，过程比较简单，只要在获取输入数据的迭代过程中不断调用 processRow 函数来处理数据即可，因此算法省略了此部分内容。

这里以 processInputs 方法中的逻辑为主线梳理一下聚合操作的过程：在 while 循环中不断获取输入数据 new Input ，然后得到分组表达式 groupingKey ，并以此为 key 到 hashMap（注： UnsafeFixedW idthAggregationMap 类型，内部存储 groupingKey 与 UnsafeRow 的映射关系）中获取对应的聚合操作缓冲区（buffer），如果 buffer 不为空，则直接调用 processRow 处理。如果获取不到对应的 buffer，则意味着 hashMap 内存空间已满，这种情况下调用 destructAndCreate-ExternalSorter 方法将内存数据 spill 到磁盘以释放内存空间。注意，spill 到磁盘的数据会通过 UnsafeKVExternalSorter 数据结构访问，多次 spill 的外部数据还会进行合并操作。然后，再次从 hashMap 获取聚合缓冲区，此时如果无法获取，则会抛出 OOM 错误。

最后，检查全局 externalSorter 对象，如果不为空，则意味着聚合操作过程因为内存不足没能执行成功，部分数据存储在磁盘上。此时，将 hashMap 中最后的数据 spill 到磁盘并与 externalSorter 中的数据合并，然后调用 free 方法释放 hashMap，切换到基于排序的聚合执行方式。具体实现参考 `switchToSortBasedAggregation` 方法，其逻辑与 SortAggregateExec 方式的逻辑类似，这里不再展开讲解。

总体来看， HashAggregateExec 的执行过程还是比较清晰的，内存不足时借助磁盘空间实现聚合。中间涉及的一些特殊的数据结构（包括 UnsafeKVExternalSorter 和 UnsafeFixedW idthAggregationMap 等），其内部实现细节读者可以先暂时跳过，待熟悉了 Tungsten 相关的内容之后再自行研究。

![img](https://pic3.zhimg.com/v2-816a73f5923a3e0b9f4527427cda181c_r.jpg)

## 7.4　窗口（Window）函数

窗口函数（ Window Function ）首先在 SQL-2003 标准中被提出，在 SQL-2008 标准中被细化，并在后续标准更新中多次扩展，最新的版本是 SQL-2011 标准。从功能本质来看，窗口函数和普通聚合函数（ Aggregate Function ）类似，都是对多行数据信息进行整合。不同之处在于，窗口函数中多了一个灵活的「窗口」，支持用户指定更加复杂的聚合行为（如数据划分和范围设置等），因此在某些特殊分析场景下扮演着重要的角色。

### 7.4.1　窗口函数定义与简介

通常情况下，聚合操作会按照 **Group By** 子句对数据进行分组，然后在每个分组内执行聚合函数，得到一条结果。然而，这种常规的方式在面对一些复杂的分析需求时会显得捉襟见肘。例如，需要统计每个班前 5 名学生的成绩，或者需要计算每个学生的成绩与班级最高分的差距等。针对这类特殊的场景，窗口函数就有了用武之地。

窗口函数是 SQL 中的一类特别的函数，针对的是一个由 `OVER` 子句定义的「窗口」，即**窗口函数的作用域**。窗口是标准的 SQL 术语，形象地刻画了函数在计算过程中不断变化的数据范围。窗口函数可以通过计算每行周围窗口上的集合值来分析数据，例如，计算一定记录范围内、一定值域内或一段时间内的累计和移动平均值等。

具体来说，窗口和窗口函数在 Spark SQL 中的文法定义如下。窗口函数相比普通函数只不过多了 OVER 子句，其中的窗口信息（ windowSpec ）可以事先定义并在 SQL 中引用（ windowRef 标签），也可以直接指定（ windowDef 标签）。在 windowDef 标签的文法中，包括两个分支，分别对应 CLUSTERBY 和 PARTITION / DISTRIBUTEBY 开头的关键字。根据在实际系统中的观察， PARTITION BY 配合 ORDERBY 关键字的使用频率最高，因此这里以此作为分析的对象，其他情况读者可自行分析。

![img](https://pic2.zhimg.com/v2-9b7b6fe96f3aa347541e9cf2c074929c_r.jpg)

进一步来看，窗口函数涉及了 3 个核心元素，分别是分区（ (PARTITION | DISTRIBUTE)BY ）信息、排序（(ORDER|SORT)BY）信息和窗框定义（ windowFrame ）。

- **分区信息**：分区元素由 PARTITION BY 子句定义，并被所有的窗口函数支持。类似 SparkPlan 中的 Partitioning ，数据基于分区表达式执行 Hash 类型的 Shuffl e 操作。在极端情况下，如果没有设定分区表达式，则所有数据都会集中到一个节点上。从另一个角度来看，分区可以算是对窗口的初步限制，只有值相同的数据才能进入同一个窗口。例如，窗口函数中使用 PARTITION BY ID ，当前数据行的 ID 为 1，那么当前行所在的窗口中必然只能包括 ID 值为 1 的数据。
- **排序信息**：排序元素定义分区内数据的顺序，在标准 SQL 中，所有函数都支持排序元素。排序子句所起的作用比较好理解，例如，对于排名函数（Rank），当使用降序排序时，排名函数返回对应分区内大于当前值的记录的个数加 1；当使用升序排序时，排名函数返回小于当前值的记录的个数加 1。实际上，某些窗口函数已经隐含地对数据有序性进行了要求，即使 SQL 语句中没有显示地指定， Spark SQL 后续解析时也会相应地添加。
- **窗框定义**：本质上，窗框是一个在分区内对行进行进一步限制的筛选器，适用于聚合窗口函数，也适用于 3 个偏移函数，即 FIRST_VALUE、LAST_VALUE 和 NTH_VALUE。可以把这个窗口元素想象成基于特定的顺序、在当前行所在分区中定义的两个「点」，两点范围内的数据行才会参与计算。在标准的窗框描述中，可以用 ROWS 或 RANGE 关键字来定义如何选取开始行和结束行。ROWS 允许用相对于当前行的偏移行数来指定窗框的起点和终点；RANGE 则更具灵活性，可以以窗框起点和终点的值与当前行的值的差异来定义偏移行数。因此，ROWS 在物理级别（数据读取的数目）定义了窗口里有多少行，RANGE 则限定了排序之后的值在窗口里有多少行。此外，文法中的 PRECEDING 关键字可以定义窗口的上限，窗口从当前行向前若干行处开始， UNBOUNDED PRECEDING 表示没有上限（从第一行数据开始）。 FOLLOW ING 关键字定义窗口的下限，窗口从当前行向后若干行处结束， UNBOUNDED FOLLOW ING 代表窗口没有下限（一直到最后一行数据）。表 7.2 列举了 3 个窗框的使用案例。

<p align="center">
 <img src="https://pic2.zhimg.com/v2-24e73976c47bc21c141e7684b35d1b98_r.jpg" />
表 7.2 Window Frame 案例
</p>
下面以 `row_number()` 函数为例讲解窗口函数的使用。假设有关系表 exam（gradeID，classID，studentID，score)，这 4 列分别代表年级 ID、班级 ID、学生 ID 和学生成绩，需要对每个年级每个班的学生按成绩排序并得到其排序号。那么，使用窗口函数 row_number()的 SQL 语句及其执行过程如图 7.16 所示。

```sql
select 
  studentID, 
  row_number() over (partition by gradeID, classID order by score desc) as ranking
from exam
```
<p align="center">
 <img src="https://pic4.zhimg.com/v2-1c6f94da0ef67659270dc9853ce4f940_r.jpg" />
图 7.16　窗口函数实例
</p>

总体来看，窗口函数除输入、输出行相等外，还包括如下特性和优势：类似 Group By 的聚合，支持非顺序的数据访问；可以对窗口函数使用分析函数、聚合函数和排名函数；简化了 SQL 代码（消除 Join）并可以避免中间表。

### 7.4.2　窗口函数相关表达式
在分析窗口函数的执行过程之前，有必要对 Catalyst 中窗口函数的关键元素进行介绍，这部分内容定义在窗口函数相关的表达式中。

在 Catalyst 中，窗口表达式（ WindowExpression ）包含了窗口函数（ WindowFunction ）和窗框的定义。如图 7.17 所示， WindowExpression 的窗口函数（ WindowFunction ）是 Expression 类型，即上述案例中的 `row_number()` 函数；窗口定义是 WindowSpecDefinition 类型，代表 SQL 语句中 over 关键字之后括号里边的内容。

<p align="center">
 <img src="https://pic4.zhimg.com/v2-37964d421faf7894e1a169deb4db05ac_r.jpg" />
图 7.17　窗口表达式
</p>

在 WindowSpecDefinition 中包含了 7.4.1 小节中提到的窗口函数的 3 个核心元素：分区信息、排序信息和窗框定义。分区信息类型为 Seq[ Expression ]，在上述案例中表示按照 gradeID 和 classID 这两列进行分区；排序信息类型为 Seq[SortOrder]，对应 score 列的降序；窗框（ WindowFrame ）定义比较重要，有 UnspecifiedFrame 和 SpecifiedWindowFrame 两个子类，7.4.1 小节中的案例未指定这方面的信息，对应的是 UnspecifiedFrame 子类。

SpecifiedWindowFrame 表示一个完整的窗框定义，包含 frameType 、 frameStart 、frameEnd 3 个元素，分别代表窗框类型（FrameType）、起始的窗口边界（ FrameBoundary ）和终止的窗口边界（ FrameBoundary ）。如图 7.18 所示， FrameBoundary 包含 UnboundedPreceding 、 ValuePreceding （value:Int）、 CurrentRow 、 ValueFollow ing （value:Int）和 UnboundedFollowing 5 种。

![img](https://pic2.zhimg.com/v2-a1ab976ea2930fe2f3bf3b76ba8e2fe4_r.jpg)

图 7.18　窗框（ WindowFrame ）的相关概念

FrameType 有 RowFrame 和 RangeFrame 两种。RowFrame 针对窗口分区中的所有数据行，当 ValuePreceding 和 ValueFollowing 作为窗口边界时，其中的 value 值代表物理偏移量，例如「 ROW BETWEEN 1 PRECEDING AND 2 FOLLOW ING 」表示 4 行数据构成的窗框，即从当前行的前一行到后两行。 RangeFrame 针对的是用于排序的列，当 ValuePreceding 和 ValueFollowing 作为窗口边界时，其中的 value 值代表逻辑偏移量，例如假设当前行的 score 值为 87，而窗框的边界定义为「 RANGEBETWEEN 1 PRECEDINGAND 1 FOLLOW ING 」，那么所对应的数据窗口为 score 在[86，88]范围内的数据行。

窗口函数是 Expression 的子类，需要定义 W indowFrame 来设定该函数执行的默认窗口范围。目前在 Spark SQL 中，内置实现的窗口函数共有 8 个，如表 7.3 所示。

![img](https://pic3.zhimg.com/v2-71d17cdffaa2036622a5e51fc596da4d_r.jpg)

表 7.3 WindowFunction 种类

在窗口函数中，lead 和 lag 都属于 OffsetWindowFunction 的子类，用于计算与偏移量相关的数据；其他都属于 AggregateWindowFunction 类型，在窗框内执行聚合计算。需要注意的是，在窗口查询中，除上述窗口函数外，也支持常见的函数。

### 7.4.3　窗口函数的逻辑计划阶段与物理计划阶段

在最终执行之前，有必要对包含窗口函数的查询中间过程进行分析。本节仍以 7.4.1 小节的案例作为分析对象，详细阐述其逻辑计划和物理计划间的转换。

上述 SQL 语句生成的完整的抽象语法树（AST）和之前的简单查询语法树大同小异，这里不再全部罗列，仅重点展示窗口函数的部分。如图 7.19 所示，W indow 函数的主要特点是 FunctionCallContext 节点下面多了一些与窗口相关的节点信息。图 7.19 中的 WindowDefContext 节点代表窗口的定义，对于上述案例来讲，其中包含了一个 ExpressionContext 节点的列表（partition）来对应分区表达式，以及一个返回 SortItem Context 节点列表的函数（sortItem）来对应排序表达式。

![img](https://pic4.zhimg.com/v2-6be3b99ec0cb05300167c78bf2465743_r.jpg)

图 7.19　窗口函数 AST

图 7.19 中同时标出了函数名 row_number 和各个列所对应的子树。值得一提的是，如果 SQL 查询中涉及了窗框的相关信息，则 W indowDefContext 节点下面还会包含对应的 W indowFrame-Context 节点。

当 `FunctionCallContext` 节点下面出现 `WindowDefContext` 节点时， ASTbuilder 会将该函数对应的表达式封装成 `WindowExpression` 表达式。如图 7.20 所示为该查询生成的 **Unresolved** `LogicalPlan` 结构，有 `UnresolvedRelation` 和 `Project` 两个节点。

<p align="center">
 <img src="https://pic1.zhimg.com/v2-248c43779690632a745829ab1a7788e0_r.jpg" />
图 7.20　逻辑算子树（ ResolveWindow Frame ）
</p>

生成的 Unresolved LogicalPlan 经过 ResolveRelations 、 ResolveReference 和 ResolveFunctions 3 个解析规则的转换，得到的逻辑算子树如图 7.20 左下方所示。接下来，该逻辑算子树匹配到 ResolveWindowFrame 规则，用来处理窗口函数中的窗框（ WindowFrame ）信息。该规则的具体逻辑可以参见下面的代码。

```scala
  object ResolveWindowFrame extends Rule[LogicalPlan] {
    def apply(plan: LogicalPlan): LogicalPlan = plan resolveExpressions {
      case WindowExpression(wf: WindowFunction, 
                            WindowSpecDefinition(_, _, f: SpecifiedWindowFrame))
          if wf.frame != UnspecifiedFrame && wf.frame != f =>
        failAnalysis(s"Window Frame $f must match the required frame ${wf.frame}")
      case WindowExpression(wf: WindowFunction, 
                            s @ WindowSpecDefinition(_, _, UnspecifiedFrame))
          if wf.frame != UnspecifiedFrame =>
        WindowExpression(wf, s.copy(frameSpecification = wf.frame))
      case we @ WindowExpression(e, s @ WindowSpecDefinition(_, o, UnspecifiedFrame))
          if e.resolved =>
        val frame = if (o.nonEmpty) {
          SpecifiedWindowFrame(RangeFrame, UnboundedPreceding, CurrentRow)
        } else {
          SpecifiedWindowFrame(RowFrame, UnboundedPreceding, UnboundedFollowing)
        }
        we.copy(windowSpec = s.copy(frameSpecification = frame))
    }
  }
```

`ResolveWindowFrame` 规则在解析并处理 `WindowFrame` 信息时有 3 种情况。

- 如果 `WindowSpecDefinition` 中指定了 `WindowFrame` （对应包含 SpecifiedWindowFrame 表达式），而窗口函数中也设置了 WindowFrame 且与该 Window Frame 不相同，则 SQL 语句抛出分析异常。

- 如果查询中未指定 WindowFrame ，则将 WindowExpression 中的 Window Frame 设置为窗口函数中的 WindowFrame 表达式。

- 查询中未指定 WindowFrame ，而且函数不是 Window Function 类型，因此不包含 WindowFrame 信息，此时会将 WindowExpression 设置为默认的 WindowFrame 表达式。


查询实例中的情况对应第二种模式，WindowSpecDefinition 表达式中未定义 Window Frame 信息（ UnspecifiedFrame ），因此直接使用窗口函数 row_number 中的 W indowFrame 设置（处理数据的范围是从开头到当前行）。生成的逻辑算子树如[图 7.20]() 右下方所示，可以看到其中 WindowExpression 表达式发生了变化。

从前面的分析可知，在最初生成的逻辑算子树中，与窗口相关的内容都是以表达式来表示的，而不是单独的窗口节点。例如，在案例中窗口表达式依赖 Project 节点。而要执行窗口函数相关的查询，生成窗口机制相关的逻辑算子节点 Window 乃至后续的物理算子节点 WindowExec 的步骤是必不可少的。

实际上，这一步复杂的转换过程由逻辑计划阶段的 `ExtractWindowExpressions` 规则完成。顾名思义， `ExtractWindowExpressions` 规则用来从 `WindowExpression` 表达式中提取相关信息进行整合并生成单独的 Window 逻辑算子树节点。具体来讲，该规则处理以下 3 种情况。

- Project 节点的 projectList 表达式列表中包含的 WindowExpression 表达式。

-  Aggregate 节点的 aggregateExpressions 表达式列表中包含的 WindowExpression 表达式。

- [Filter->Aggregate]逻辑算子树结构模式。

针对每种情形， ExtractW indowExpressions 规则涉及以下两个重要的步骤。

1. 表达式列表拆分：对于一个 `NamedExpression` 表达式列表（ `projectList` 或 `aggregateExpressions` ），将其分为两部分，其中一个是常规的表达式列表，另一个是所有的 `WindowExpression` 表达式列表。例如，假设 select 语句中的表达式如下：
   ```sql
      col1, sum(col2 + col3) OVER (PARTITION BY col4 ORDER BY col5)
   ```
   该规则会提取「col1」「col2+col3」「col4」「col5」，并根据列信息进行替换，那么该表达式列表会拆分为 [col1，col2+col3 as_w0，col4 as_w1，col5 as_w2]和 [sum（_w0）OVER（ PARTITION BY _w1ORDERBY_w2）]两个表达式列表。

2. Window 逻辑算子树节点创建：首先，对于上一步提取出来的所有 W indowExpression 表达式，根据其不同的窗口定义（ W indowSpecDefinition ）进行分组（注：相同的 WindowSpecDefinition 对应的窗口函数可以放在一起进行处理）；接着，针对每个不同的窗口定义（ W indowSpec-Definition ），创建 W indow 逻辑算子树节点并将其插入到逻辑算子树中，需要注意的是每个 W indow 逻辑算子节点相应地处理一个 W indowSpecDefinition 的窗口函数。


经过 `ExtractWindowExpressions` 规则的处理，得到的逻辑算子树如[图 7.21]() 所示。可以看到， `WindowExpression` 已经被提取出来生成了 `Window` 逻辑算子节点，同时围绕该节点添加了 3 个 Project 节点。

<p align="center">
 <img src="https://pic4.zhimg.com/v2-7f1f1384ede65b8d49950ad04feb10fa_r.jpg" />
图 7.21　逻辑算子树（ ExtractWindowExpressions ）
</p>

实际上，在 `Analyzer` 中，对于与 Window 相关的逻辑算子树，除上述两条规则外，还有 `ResolveWindowOrder` 规则，主要用来对排序功能进行验证，确保窗口语句中包含排序语句或 Rank 之类的窗口函数。因为本案例中已经包含了排序语句，所以上述过程没有体现出来。

在图 7.21 所示的 Analyzed LogicalPlan 的基础上， `Optimizer` 会进行进一步的优化。经过别名消除和 Project 节点整合之后，得到的逻辑算子树如图 7.22 所示，有 Relation、Project、Window 和 Project 等 4 个逻辑算子节点。

从逻辑算子树生成物理算子树的过程比较简单，分别应用 BasicOperatiors 中的映射策略和 FileSourceStrategy 策略，生成对应的 SparkPlan ，如图 7.23 所示。在物理算子树中， WindowExec 节点对应 Window 逻辑算子节点，很显然，该节点在 Partitioning 和 Ordering 方面都有正确性的需求，因此在最后阶段的 EnsureRequirements 规则中，会添加 Exchange 节点进行 Shuffle 操作，以及添加 SortExec 节点进行排序操作，得到包含 6 个节点的物理执行计划。

<p align="center">
 <img src="https://pic1.zhimg.com/v2-f7d764984dd04a17db5fa01eea9330a5_r.jpg" />
图 7.23　物理执行计划
</p>

### 7.4.4　窗口函数的执行

生成物理执行计划之后，就提交到 Spark 集群进行计算了。窗口函数的执行逻辑在 WindowExec 中实现，本小节将对其进行详细分析。

窗口聚合的执行过程如图 7.24 所示， `WindowExec` 类的 `requiredChildDistribution` 和 `requiredChildOrdering` 方法分别规定了输入数据分布和有序性要求，因此在执行 `WindowExec` 之前，`Exchange` 和 `Sort` 完成重分区及分区内数据的排序。对应于本节的查询实例，Exchange 会按照`（gradeID，classID）`进行分区，并按照 *score* 进行排序。

在 WindowExec 执行中，有两个比较重要的概念：一个是 FramedFunctions ，记录不同的窗口表达式（ WindowExpression ）间的映射关系，同一个窗口可能包含多个窗口表达式，也就是多个窗口函数；另一个是 AggregateProcessor ，类似聚合语句中的 AggregateIterator ，通过执行具体的窗口函数进行实际的计算。

物理执行计划 WindowExec 用于在单个有序的数据分区中计算并输出窗口聚合结果，与普通聚合过程不同的是，窗口聚合会根据窗口函数的窗框等设置对每一行数据进行计算。根据前面的分析，共有 5 种窗框类型。

<p align="center">
 <img src="https://pic4.zhimg.com/v2-87e896ab05ba2c9e38d0fada7b311076_r.jpg" />
图 7.24　窗口聚合的执行过程概览
</p>

- 全部数据分区（ Entire Partition ）：即 UNBOUNDED PRECEDING AND UNBOUNDED FOLLOW ING 。在这种情况下，对于每条数据，需要处理该数据所在数据分区中的所有数据，对应实现为 `UnboundedWindowFunctionFrame` 类。

- 扩张框（ Growing Frame ）：即 UNBOUNDED PRECEDING AND...... 在这种情况下，每次都会移动到新的数据行进行处理，并添加一些数据行扩展该框架，在这种类型的窗框中数据不会被移除，只会不停地加入新数据，窗口范围不停地「扩张」，对应的实现为 UnboundedPrecedingW indow FunctionFrame 类，案例中的 row_number()函数就属于这种类型。

- 收缩框（ Shrinking Frame ）：该框架只会移除数据，即 ......ANDUNBOUNDED FOLLOW ING 。在这种情况下，每次都会移动到新的数据行进行处理，并从窗框中移除一些数据行，这种类型的窗框中不会添加数据，窗口不停地「收缩」，对应的实现为 UnboundedFollowing-W indowFunctionFrame 类。

- 移动框（ Moving Frame ）：每次处理到新的数据行，都会添加一些数据，同时也会删除一些数据，例如（1 PRECEDING AND CURRENT ROW ）或（1 FOLLOW ING AND 2 FOLLOW ING ），对应的实现为 SlidingW indow FunctionFrame 类。

- 偏移框（ Offset Frame ）：该窗框仅包含一行数据，即距离当前数据行特定偏移量的数据。这里需要注意的是，偏移框仅适用于 Off setW indowFunction 类型的窗口函数。


这 5 种类型的窗框可以统一抽象为窗口函数执行框架，如图 7.25 所示。从抽象层面来看，窗口函数执行阶段只处理两件事情，即准备数据行缓冲区（RowBuffer）和写入结果，对应的实现为 prepare 和 write 函数。

<p align="center">
 <img src="https://pic4.zhimg.com/v2-dc2c1aa942fedddf893080855493fd01_r.jpg" />
图 7.25　窗口函数执行框架
</p>

行缓冲区（RowBuffer）服务于单个窗口数据分区，实例中每个（gradeID，classID）分区的数据对应一个 RowBuffer 。考虑到窗口函数处理过程中需要反复扫描数据行，因此 RowBuffer 在本质上起到物化（ Materialize ）分区数据行的作用。 RowBuffer 定义为抽象类，支持 size、next、skip 和 copy 4 种操作，具体实现包括 ArrayRowBuffer 和 ExternalRowBuffer 两种，分别对应内存和外存的情况。

根据 WindowExec 类 doExecute 方法可知，整体的执行逻辑分为两步。

（1）按分区读取数据（ fetchNextPartition ），并将其保存在 ArrayBuffer 中，在此过程中先构造一个 ArrayRowBuffer 存储数据，如果超过阈值（默认为 4096），则切换为基于磁盘的 UnsafeExternalSorter 数据结构；当前分区数据读取结束后，根据 UnsafeExternalSorter 是否为空，判断分区数据保存在 ArrayRowBuffer 还是 ExternalRowBuffer 数据结构中。

（2）遍历 RowBuffer 中的数据，逐条执行 w rite 操作，最终调用 AggregateProcessor （注：封装 row_number()函数）中的 update 等方法完成计算。

`AggregateProcessor` 中的实现方式和普通的聚合操作的实现方式类似，这里不再展开讲解。值得一提的是， RowBuffer 在后续版本中替换成了更加安全、高效的实现（ `ExternalAppendOnlyUnsafeRowArray` ），读者可进一步自行研究。