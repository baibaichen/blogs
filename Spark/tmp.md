# ExchangeCoordinator

A coordinator used to determines how we shuffle data between stages generated by Spark SQL. Right now, the work of this coordinator is to determine the number of post-shuffle partitions for a stage that needs to fetch shuffle data from one or multiple stages. A coordinator is constructed with three parameters, `numExchanges`,`targetPostShuffleInputSize`, and `minNumPostShufflePartitions`.

 - `numExchanges` is used to indicated that how many [[ShuffleExchange]]s that will be registered to this coordinator. So, when we start to do any actual work, we have a way to make sure that we have got expected number of [[ShuffleExchange]]s.
 - `targetPostShuffleInputSize` is the targeted size of a post-shuffle partition's input data size. With this parameter, we can estimate the number of post-shuffle partitions. This parameter is configured through    `spark.sql.adaptive.shuffle.targetPostShuffleInputSize`.
 - `minNumPostShufflePartitions` is an optional parameter. If it is defined, this coordinator will try to make sure that there are at least `minNumPostShufflePartitions` post-shuffle partitions.

The workflow of this coordinator is described as follows:
 - Before the execution of a [[SparkPlan]], for a [[ShuffleExchange]] operator, if an [[ExchangeCoordinator]] is assigned to it, it registers itself to this coordinator. This happens in the `doPrepare` method.
 - Once we start to execute a physical plan, a [[ShuffleExchange]] registered to this coordinator will call `postShuffleRDD` to get its corresponding post-shuffle [[ShuffledRowRDD]]. If this coordinator has made the decision on how to shuffle data, this [[ShuffleExchange]] will immediately get its corresponding post-shuffle [[ShuffledRowRDD]].
 - If this coordinator has not made the decision on how to shuffle data, it will ask those registered [[ShuffleExchange]]s to submit their pre-shuffle stages. Then, based on the size statistics of pre-shuffle partitions, this coordinator will determine the number of post-shuffle partitions and pack multiple pre-shuffle partitions with continuous indices to a single post-shuffle partition whenever necessary.
 - Finally, this coordinator will create post-shuffle [[ShuffledRowRDD]]s for all registered [[ShuffleExchange]]s. So, when a [[ShuffleExchange]] calls `postShuffleRDD`, this coordinator can lookup the corresponding [[RDD]].

The strategy used to determine the number of post-shuffle partitions is described as follows. To determine the number of post-shuffle partitions, we have a target input size for a post-shuffle partition. Once we have size statistics of pre-shuffle partitions from stages corresponding to the registered [[ShuffleExchange]]s, we will do a pass of those statistics and pack pre-shuffle partitions with continuous indices to a single post-shuffle partition until adding another pre-shuffle partition would cause the size of a post-shuffle partition to be greater than the target size.

For example, we have two stages with the following pre-shuffle partition size statistics:
stage 1: [100 MB, 20 MB, 100 MB, 10MB, 30 MB]
stage 2: [10 MB,  10 MB, 70 MB,  5 MB, 5 MB]
assuming the target input size is 128 MB, we will have three post-shuffle partitions, which are:
 - post-shuffle partition 0: pre-shuffle partition 0 (size 110 MB)
 - post-shuffle partition 1: pre-shuffle partition 1 (size 30 MB)
 - post-shuffle partition 2: pre-shuffle partition 2 (size 170 MB)
 - post-shuffle partition 3: pre-shuffle partition 3 and 4 (size 50 MB)

# ShuffledRowRDD

This is a specialized version of [[org.apache.spark.rdd.ShuffledRDD]] that is optimized for shuffling rows instead of Java key-value pairs. Note that something like this should eventually be implemented in Spark core, but that is blocked by some more general refactorings to  shuffle interfaces / internals.

This RDD takes a [[ShuffleDependency]] (`dependency`), and an optional array of partition start indices as input arguments (`specifiedPartitionStartIndices`).

The `dependency` has the parent RDD of this RDD, which represents the dataset before shuffle (i.e. map output). Elements of this RDD are (partitionId, Row) pairs. Partition ids should be in the range [0, numPartitions - 1]. `dependency.partitioner` is the original partitioner used to partition map output, and `dependency.partitioner.numPartitions` is the number of pre-shuffle partitions (i.e. the number of partitions of the map output).

When `specifiedPartitionStartIndices` is defined, `specifiedPartitionStartIndices.length` will be the number of post-shuffle partitions. For this case, the `i`th post-shuffle partition includes `specifiedPartitionStartIndices[i]` to `specifiedPartitionStartIndices[i+1] - 1` (inclusive).

When `specifiedPartitionStartIndices` is not defined, there will be `dependency.partitioner.numPartitions` post-shuffle partitions. For this case, a post-shuffle partition is created for every pre-shuffle partition.

# SortShuffleManager


In sort-based shuffle, incoming records are sorted according to their target partition ids, then written to a single map output file. Reducers fetch contiguous regions of this file in order to read their portion of the map output. In cases where the map output data is too large to fit in memory, sorted subsets of the output can are spilled to disk and those on-disk files are merged to produce the final output file.

Sort-based shuffle has two different write paths for producing its map output files:

 - Serialized sorting: used when all three of the following conditions hold:
   1. The shuffle dependency specifies no aggregation or output ordering.
   2. The shuffle serializer supports relocation of serialized values (this is currently
      supported by KryoSerializer and Spark SQL's custom serializers).
   3. The shuffle produces fewer than 16777216 output partitions.
 - Deserialized sorting: used to handle all other cases.

-----------------------
Serialized sorting mode
-----------------------

In the serialized sorting mode, incoming records are serialized as soon as they are passed to the shuffle writer and are buffered in a serialized form during sorting. This write path implements several optimizations:

 - Its sort operates on serialized binary data rather than Java objects, which reduces memory
   consumption and GC overheads. This optimization requires the record serializer to have certain
   properties to allow serialized records to be re-ordered without requiring deserialization.
   See SPARK-4550, where this optimization was first proposed and implemented, for more details.

 - It uses a specialized cache-efficient sorter ([[ShuffleExternalSorter]]) that sorts
   arrays of compressed record pointers and partition ids. By using only 8 bytes of space per
   record in the sorting array, this fits more of the array into cache.

 - The spill merging procedure operates on blocks of serialized records that belong to the same
   partition and does not need to deserialize records during the merge.

 - When the spill compression codec supports concatenation of compressed data, the spill merge
   simply concatenates the serialized and compressed spill partitions to produce the final output
   partition.  This allows efficient data copying methods, like NIO's `transferTo`, to be used
   and avoids the need to allocate decompression or copying buffers during the merge.

For more details on these optimizations, see SPARK-7081.
