#ExchangeCoordinator

A coordinator used to determines how we shuffle data between stages generated by Spark SQL. Right now, the work of this coordinator is to determine the number of post-shuffle partitions for a stage that needs to fetch shuffle data from one or multiple stages. A coordinator is constructed with three parameters, `numExchanges`,`targetPostShuffleInputSize`, and `minNumPostShufflePartitions`.

 - `numExchanges` is used to indicated that how many [[ShuffleExchange]]s that will be registered to this coordinator. So, when we start to do any actual work, we have a way to make sure that we have got expected number of [[ShuffleExchange]]s.
 - `targetPostShuffleInputSize` is the targeted size of a post-shuffle partition's input data size. With this parameter, we can estimate the number of post-shuffle partitions. This parameter is configured through    `spark.sql.adaptive.shuffle.targetPostShuffleInputSize`.
 - `minNumPostShufflePartitions` is an optional parameter. If it is defined, this coordinator will try to make sure that there are at least `minNumPostShufflePartitions` post-shuffle partitions.

The workflow of this coordinator is described as follows:
 - Before the execution of a [[SparkPlan]], for a [[ShuffleExchange]] operator, if an [[ExchangeCoordinator]] is assigned to it, it registers itself to this coordinator. This happens in the `doPrepare` method.
 - Once we start to execute a physical plan, a [[ShuffleExchange]] registered to this coordinator will call `postShuffleRDD` to get its corresponding post-shuffle [[ShuffledRowRDD]]. If this coordinator has made the decision on how to shuffle data, this [[ShuffleExchange]] will immediately get its corresponding post-shuffle [[ShuffledRowRDD]].
 - If this coordinator has not made the decision on how to shuffle data, it will ask those registered [[ShuffleExchange]]s to submit their pre-shuffle stages. Then, based on the size statistics of pre-shuffle partitions, this coordinator will determine the number of post-shuffle partitions and pack multiple pre-shuffle partitions with continuous indices to a single post-shuffle partition whenever necessary.
 - Finally, this coordinator will create post-shuffle [[ShuffledRowRDD]]s for all registered [[ShuffleExchange]]s. So, when a [[ShuffleExchange]] calls `postShuffleRDD`, this coordinator can lookup the corresponding [[RDD]].

The strategy used to determine the number of post-shuffle partitions is described as follows. To determine the number of post-shuffle partitions, we have a target input size for a post-shuffle partition. Once we have size statistics of pre-shuffle partitions from stages corresponding to the registered [[ShuffleExchange]]s, we will do a pass of those statistics and pack pre-shuffle partitions with continuous indices to a single post-shuffle partition until adding another pre-shuffle partition would cause the size of a post-shuffle partition to be greater than the target size.

For example, we have two stages with the following pre-shuffle partition size statistics:
stage 1: [100 MB, 20 MB, 100 MB, 10MB, 30 MB]
stage 2: [10 MB,  10 MB, 70 MB,  5 MB, 5 MB]
assuming the target input size is 128 MB, we will have three post-shuffle partitions, which are:
 - post-shuffle partition 0: pre-shuffle partition 0 (size 110 MB)
 - post-shuffle partition 1: pre-shuffle partition 1 (size 30 MB)
 - post-shuffle partition 2: pre-shuffle partition 2 (size 170 MB)
 - post-shuffle partition 3: pre-shuffle partition 3 and 4 (size 50 MB)

#ShuffledRowRDD

This is a specialized version of [[org.apache.spark.rdd.ShuffledRDD]] that is optimized for shuffling rows instead of Java key-value pairs. Note that something like this should eventually be implemented in Spark core, but that is blocked by some more general refactorings to  shuffle interfaces / internals.

This RDD takes a [[ShuffleDependency]] (`dependency`), and an optional array of partition start indices as input arguments (`specifiedPartitionStartIndices`).

The `dependency` has the parent RDD of this RDD, which represents the dataset before shuffle (i.e. map output). Elements of this RDD are (partitionId, Row) pairs. Partition ids should be in the range [0, numPartitions - 1]. `dependency.partitioner` is the original partitioner used to partition map output, and `dependency.partitioner.numPartitions` is the number of pre-shuffle partitions (i.e. the number of partitions of the map output).

When `specifiedPartitionStartIndices` is defined, `specifiedPartitionStartIndices.length` will be the number of post-shuffle partitions. For this case, the `i`th post-shuffle partition includes `specifiedPartitionStartIndices[i]` to `specifiedPartitionStartIndices[i+1] - 1` (inclusive).

When `specifiedPartitionStartIndices` is not defined, there will be `dependency.partitioner.numPartitions` post-shuffle partitions. For this case, a post-shuffle partition is created for every pre-shuffle partition.