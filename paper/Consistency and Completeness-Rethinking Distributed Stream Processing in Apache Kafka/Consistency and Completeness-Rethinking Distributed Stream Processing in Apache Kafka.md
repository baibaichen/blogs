To summarize, in this paper we present that by tackling the consistency and completeness properties separately with a small tribute to cost, Kafka Streams can help stream processing developers to achieve both correctness and performance in their applications. Its design is ==underpinned== by the following technical contributions:

1. We discuss the two correctness challenges in stream processing, namely consistency in the face of failures and completeness with out-of-order data. We survey state of the art solutions for these challenges and their implicit latency trade-offs.
2. We introduce the Kafka Streams client library of Apache Kafka, which leverages Kafka’s core architecture as a persistent, immutable commit log to support stream processing capabilities. It tackles streaming consistency by transforming all computational results as log appends with idempotent and transactional protocols, and handles out-of-order data with a fine-grained speculative approach.
3. We demonstrate how Kafka Streams has been used in large-scale streaming application deployments in production environments that rely heavily on the correctness guarantees, and we describe their performance metrics and insights.

The remainder of this paper is organized as follows: Section [2](#page3) identifies two key challenges to support distributed stream processing correctness guarantees. Section [3 ](#page4)gives an overview of the core design principles of Apache Kafka and its streaming library, Kafka Streams. Section [4 ](#page5)describes how Kafka Streams implements its exactly-once semantics via idempotent and transactional log write protocols to support consistency with failure recovery. Section [5](#page8) illustrates the revision-based speculative processing mechanism employed by Kafka Streams to handle out-of-order data and reason about streaming completeness. Finally, Section [6 ](#page10)presents exist-ing large-scale deployments of Kafka Streams and discusses their performance insights related to its consistency configurations. Section [7 ](#page11)summarizes related work, followed by our conclusions and future work in Section 8.

> ==总之==，我们在本文中提出，通过分别处理一致性和完整性，在增加一点点成本的情况下，Kafka Stream 可帮助流式应用开发人员实现正确性和性能。主要有以下技术贡献：
>
> 1. 我们讨论了流处理中的两个正确性挑战，即**失败时的一致性**和**无序数据的完整性**。我们调查了这些挑战的最新解决方案及其隐含的延迟权衡。
> 2. 我们介绍了 Apache Kafka 中的 Kafka Streams 客户端库，利用 Kafka 的核心架构（持久的、不可变的提交日志）来支持流处理能力。将所有计算结果转换为日志追加，因此能使用**幂等**和**事务协议**，来解决流一致性问题；并使用细粒度的推测方法**处理乱序数据**。
> 3. 我们展示了在严重依赖正确性保证的生产环境中，大规模流应用程序是如何使用 Kafka Streams 的，并描述了其性能指标和**==表现==**。
>
> 本文的余下部分组织如下：第 [2](#page3) 节<u>确定了支持分布式流处理正确性保证的两个关键挑战</u>。第 [3 ](#page4) 节概述了 Apache Kafka 及 Kafka Streams 的核心设计原则。第 [4 ](#page5) 节描述了 Kafka Streams 如何通过幂等和事务日志的写入协议实现**exactly-once** 语义，以支持故障恢复的一致性。第 [5](#page8) 节说明了 Kafka Streams 使用的基于版本的推测处理机制来解决乱序数据，和以及如何判断 Stream 是否完整。最后，第 [6 ](#page10) 节介绍了 Kafka Streams 现有的大规模部署，并讨论了与其一致性配置相关的性能表现。第 [7 ](#page11) 节总结了相关工作，第 8 节总结了结论和未来的工作。



# 2   STREAMING CORRECTNESS CHALLENGES 

## 2.1 Fault Tolerance and Consistency

Consistency has for long been an open research issue in stream processing partially due to the lack of a formal specification of the problem itself. In this paper, we primarily focus on consistency guarantees in the face of a failure. Most streaming systems should be able to produce correct results during failure-free executions, but completely masking a failure is quite hard. In distributed systems, fault tolerance is the capability to continue system operations in spite of failures. Additionally, the delivered service should be as if no failures ever happened. When it comes to stream processing, people usually refer to this guarantee as exactly-once semantics. In recent years, this term has lent itself to several different interpretations and for clarity of presentation we define exactly-once as the following: for each record from the input data stream, its processing results will be reflected exactly once even under failures. Results are reflected in two ways: result records in the output data streams, but also internal state updates in stateful stream processing operators.

A streaming system must be tolerant to a number of failure scenarios which may even occur at the same time in practice:

​	**The storage engine can fail**: If a stream processing system does not guarantee that all of its running state is persisted—e. g. if it stores all state in main memory or if it asynchronously flushes the state to persistent storage engines—then upon failures some or all of its state may be lost. In this case, the stream processing system would have to redo computations from the beginning, hoping the processed input data streams have not vanished yet. During the recomputation period, new streaming data would not be processed, reducing the system’s availability. What is worse, if the recomputation cannot generate exactly the same state as before the failure due to reasons such as an approximate restart point or nondeterministic logic, then queries before and after the failure may return inconsistent results.

​	**The stream processor can fail**: When a stream processor fails over to a new host, it must be able to recover exactly the same state and resume processing at the exact point where it left off. However, the processor may crash after successfully processing a record and persisting a state update but before acknowledging the reception of that record. Hence, when it resumes from a failure event, the same record could be received and processed again, causing not only duplicated outgoing records but also incorrect state with double updates. Even worse, when a processor temporarily loses connectivity to others in a distributed environment, it may be deemed as failed permanently and a new instance may be started up to replace it while the disconnected processor continues to work on its own. In this case, we can have duplicated processors fetching and processing the same data streams, producing duplicate outputs. We call this the problem of zombie instances.

​	**The inter-processor RPC can fail**: In a distributed environment, stream processors communicate with each other and propagate processing results through message passing. The durability of those sent messages usually depends on the sender receiving an acknowledgement (ack) from the receiver. The failure to receive that ack does not necessarily mean that the message did not reach the other side: for example, it is possible that the receiving processor success-fully gets the message, processes its associated records and updates its states, but fails to send an ack back to the sender. It is also possible that the receiving processor does not fail at all but a jitter in the network delays the ack back to the sender, exceeding the sender’s ack timeout. Since the sender cannot know the exact reason for the lack of acknowledgements, in practice it is forced to assume the message was not received and processed successfully and hence needs to retry. In this case, the same record may be sent multiple times between processors, causing the record to be processed more than once in the streaming system.

Today, most distributed streaming systems rely on checkpoint-ing mechanisms to tolerate failures: during normal execution, the system periodically checkpoints its processing state, which can then act as snapshots to which the system falls back in case of a failure. To align multiple state checkpoints throughout the **processing pipeline**, these systems usually inject **<u>==punctuations==</u>** into data streams as synchronization barriers [[31\]. ](#page12)As a result, the completion of the checkpoints is not only determined by the amount of data to checkpoint, but also on the speed at which punctuations flow through the application. If there is backpressure in data processing, checkpoint efficiency would also be impacted. In addition, the checkpointing mechanism alone is not sufficient to accomplish the desired semantics: since processing results may have already been emitted before a failure, after state is rolled back to a prior checkpoint, re-processing the input records may cause duplicated results in the output streams. In the literature, this is usually termed as the *output commit problem* [[34\]. ](#page12)There are several known solutions to resolve this problem, such as assigning unique ids that contain lineage information of streaming records, or keeping track of the received records’ unique logical timestamps for deduplication [[12, 27, 32\].](#page12) However, because the inter-operator communication channels are usually memoryless, such de-duplication approaches have to depend on the downstream operators themselves to detect and discard duplicated inputs based on locally maintained lineage information.

> 长期以来，**一致性一直是流处理中一个开放的研究问题**，部分原因是缺乏<u>==对问题本身的正式规范==</u>。本文主要关注面对失败时的一致性保证。**==大多数流式应用能够在无故障执行期间产生正确的结果==**，但完全掩盖故障相当困难。<u>分布式系统中的容错，是指系统即使出现故障也能继续运行的能力</u>。此外，（交付的）服务应该就像从未发生过故障一样。当谈到流处理时，人们通常将这种保证称为 **exactly-once 语义**。近年来，这个术语有几种不同的解释，为了表述清晰，本文将精确 **exactly-once 语义**如下：对于来自输入数据流的每条记录，即使在失败的情况下，其处理结果只会出现一次。以两种方式反映：1）输出数据流中的结果记录只出现一次，以及 2）在**有状态的流处理运算符**中，只更新其内部状态一次。
>
> Stream 系统必须能够容忍各种故障场景，这些场景在实践中甚至可能会同时发生：
>
> ​	**存储引擎可能会失败**：如果流处理系统不能保证持久化其所有运行状态 —— 例如：如果它将所有状态存储在主内存中，或者如果它异步地将状态刷新到持久存储引擎中，那么在发生故障时，可能会丢失其部分或全部状态。这种情况，流处理系统必须从头开始重新计算，希望前面处理过的输入数据流尚未消失。重新计算时不会处理新的流数据，因此降低了系统的可用性。更糟糕的是，由于某些原因，重新计算无法生成<u>**和失败之前完全一致**</u>的状态，比如重启点只是近似（而不是 exactly 一致），或者存在非确定性的逻辑（比如有随机数），那么失败前后的查询可能会返回不一致的结果。
>
> ​	**流处理器可能发生故障**：由于故障，流处理器转移到新主机时，它必须能够恢复完全相同的状态，并在其中断的位置恢复处理。但是，流处理器可能崩溃<u>在成功处理记录并持久化状态更新之后</u>，但<u>在确认接收到该记录之前</u>。因此，当它从失败事件中恢复时，可能会再次接收和处理相同的记录，这不仅会传出重复的记录，还会重复更新从而导致错误的状态。更糟糕的是，当一个处理器在分布式环境中暂时失去与其他处理器的连接时，它可能被视为永久故障，并且可能会启动一个新实例来替换它，而断开连接的处理器仍在继续自己的工作。此时，系统中有两个流处理器**同时**获取和处理相同的数据流，产生重复的输出。我们称之为僵尸实例问题。
>
> ​	**处理器间的 RPC 可能失败**：==在分布式环境中，流处理器相互通信并通过消息传递传播处理结果==。这些已发送消息的持久性通常取决于发送方是否收到来自接收方的确认 (ack)。未能接收到 ack 并不一定意味着消息没有到达另一端：例如，接收处理器可能成功地获取了消息、处理其相关记录并更新其状态，但未能将 ack 发送回发送方。也有可能接收处理器根本没有失败，但网络中的抖动将 ack 延迟回发送方，既产生了超时。由于发送方无法知道缺少 ack 的确切原因，因此假设接收方没有成功接收和处理消息，所以需要重试。在这种情况下，可能会在处理器之间多次发送同一记录，从而导致该记录在流系统中被处理不止一次。
>
> 今天，大多数分布式流式系统依靠**检查点**机制来容忍故障：正常执行期间，系统会定期检查其处理状态，然后可以作为系统在发生故障时回退到的快照。为了在整个**处理管道**中对齐多个状态检查点，这些系统通常将<u>**标点符号**</u>注入数据流中作为同步屏障 [[31]](#page12)。因此，检查点的完成不仅取决于检查点的数据量，还取决于<u>**标点符号**</u>流经应用程序的速度。如果数据处理存在背压，检查点效率也会受到影响。此外，单靠检查点机制并不足以完成所需的语义：由于处理结果可能在失败之前已经发出，在状态回滚到先前的检查点后，重新处理输入记录可能会导致重复结果在输出流中。在文献中，这通常被称为**<u>输出提交问题</u>** [[34]](#page12)。解决这个问题有几种已知的解决方案，例如：分配包含流记录血缘关系的唯一ID，或者<u>跟踪</u>接收到记录的**唯一逻辑时间戳**以删除重复数据 [[12, 27 , 32]](#page12)。然而，由于**<u>流运算符之间</u>**的通信信道通常无状态，这种方法必须依赖<u>下游运算符基于本地维护的血缘关系</u>来检测和丢弃重复记录。

## 2.2  Out-of-order Handling and Completeness

Streaming systems receive and process input data streams continuously in a certain order to provide semantically correct results. The order of the input records typically represents the logical precedence when the input stream records are generated, as indicated by their timestamps. However, in practice, the order of data stream records may be disturbed, where some records may appear in a data stream with smaller timestamps after other records with larger timestamps. The most common external factor that results in *out-of-order* data streams are clock skewness and network delays. Moreover, stream processors that read multiple input streams and merge them into a single output stream might also shuffle the order in which records are transmitted and introduce artificial disorder. 

If no out-of-order data exists in the input data streams, guaranteeing complete final results is straightforward: whenever an input record with timestamp t + 1 is received, we can infer that all the events up to timestamp t have been completed and their results reflected in the emitted outputs. However, if out-of-order data does exist, a streaming processor may emit partial results from incom-plete input streams. Reasoning about completeness and managing disorder are fundamental considerations for streaming systems. Checkpoint-based approaches tend to block emitting results until the system is confident that all events up to a certain point in time are complete. To make this determination, they either rely on external signals or internal indicators injected into the input streams. Similarly, micro-batching techniques [[36, 40]](#page12) break down unbounded streams into batches of bounded data in which each batch represents a window of the unbounded stream. When the batch of input records are considered as complete, its processing will then be triggered synchronously and the updated states be committed to external storage.

> 流式系统以一定的顺序连续地接收和处理输入数据流，以提供语义正确的结果。输入记录的顺序通常表示为**<u>输入流记录</u>**生成时的先后顺序，例如，由它们（生成时）的时间戳表示。但在实践中，数据流记录的顺序可能会被打乱，出现时间戳较大的数据流记录之后，可能会出现时间戳较小的数据流记录。导致无序数据流的最常见外部因素是时钟偏差和网络延迟。此外，读取**<u>==多个输入流并将其合并为单个输出流==</u>**的流处理器也可能改变记录传输的顺序，人为引入无序。
>
> 如果输入数据流中不存在乱序数据，保证最终结果是完整的则很简单：每当接收到时间戳为 t + 1 的输入记录时，我们可以推断时间戳为 t 的所有事件都已完成，并且它们的结果反映在发出的输出中。但是，如果的确存在乱序数据，流处理器可能会从不完整的输入流中发出部分结果。管理无序和推测完整性是流式系统的基本功能。基于检查点的方法往往会暂停发送结果，直到系统确信某个时间点之前的所有事件都已完成处理。为了做出这个决定，要么依赖于外部信号，要么依赖于注入到输入流中的内部指标。类似地，微批处理技术 [[36, 40]](#page12) 将无界流分解为有界数据的批次，其中每个批次代表无界流的一个窗口。当一批输入记录被认为是完整的时，将触发数据的同步处理，并将更新的状态将提交到外部存储中。

# 3  STREAM PROCESSING IN APACHE KAFKA

Apache Kafka has been used in many of the largest companies across industries as the backbone for data pipelines, streaming analytics, data integration, and mission-critical applications. Data streams stored in Kafka are organized in topics, and each topic can be divided into one or more partitions. Each partition is maintained as an immutable sequence of records, i. e. a log. Partitions can be continuously appended by producer clients and continuously read by consumer clients [[25]](#page12).

> Apache Kafka 已被许多跨行业的大型公司用作数据管道、流式分析、数据集成和关键任务应用程序的支柱。 存储在 Kafka 中的数据流按<u>主题</u>进行组织，每个<u>主题</u>可以划分为一个或多个分区。每个分区按一个不可变的记录序列进行维护，即，日志。分区由生产者客户端连续追加，消费者客户端可以持续读取 [[25]](#page12)。



## 3.1  Partitions and Timestamped Records

**The partitioning mechanism allows for the horizontal scalability of Kafka where log partitions are hosted on different Kafka brokers**. The actual processing of records is done by the consumer clients. Consumer clients can subscribe to one or more topic partitions and read the records from those partition logs in append order. In addition, multiple consumer clients with a common subscription can form a consumer group. Kafka assigns the subscribed topic partitions to the members of the consumer group such that each topic partition is consumed by a single member at any time. Kafka consumer groups handle task assignment, rebalancing due to membership changes, and durable progress tracking, providing a solid distributed systems foundation upon which stream processing applications may be built. 

Records stored in the topic partitions are key-value pairs, and each record has an assigned incremental offset when appended to the partition log to uniquely identify the record’s position in the log. Kafka records within a partition log are fetched in the same order they were appended, called the offset order. However, as mentioned in Section [2.2, ](#page3)in practice this offset order does not necessarily reflect the logical order of the streaming events those records represent. To tackle this issue, each record also has an embedded timestamp field. Producers can set the record timestamp to represent event time. Processing systems may use the timestamp field to properly handle out-of-order data, in which records that are appended later actually have smaller timestamps than some others that are appended earlier in the log [[33]](#page12).

> 日志分区托管在不同的 Kafka **brokers** 上，这种分区机制使得 Kafka 可以水平可伸缩。**<u>消费者客户端</u>**实际处理记录。消费者客户端可以订阅一个或多个主题分区，并按追加顺序从这些分区日志中读取记录。此外，订阅共同主题的多个消费者客户端可以组成一个消费者组。Kafka 将订阅的主题分区分配给消费者组的成员，使得每个主题分区在任何时候都被单个成员消费。 Kafka 消费者组处理<u>任务分配</u>、由成员更改而引起的重平衡以及持久化进度的跟踪，为构建流式应用提供了坚实的分布式系统基础。
>
> 存储在主题分区中的记录是键值对，每条记录在追加到分区日志时都有一个分配的增量偏移量，以唯一标识该记录在日志中的位置。分区日志中的 Kafka 记录按它们追加的相同顺序（称为偏移顺序）获取。然而，正如 [2.2](#page3) 节提到在实践中，这个偏移顺序并不一定反映这些记录所代表的事件流的逻辑顺序。为了解决这个问题，每条记录还有一个内嵌的**时间戳字段**。生产者可以设置记录时间戳来表示事件时间。处理系统可能会使用时间戳字段来正确处理**乱序数据**，既后续追加的记录实际上比日志中更早追加的某些记录有更小的时间戳 [[33]](#page12)。

## 3.2  Streams DSL and Operator Topology

Kafka Streams is a Java library included in Apache Kafka that allows users to build real-time stateful stream processing applications. The library contains a high-level DSL for users to specify their streaming logic that continuously reads data streams from source Kafka topics with consumer clients, transforms the input streams into new streams and evolving tables, and finally pipes the result streams back to sink Kafka topics. It is also used as the underlying parallel runtime of ksqlDB [[24]](#page12), an event streaming database built to work with streaming data in Apache Kafka. ksqlDB takes input data streams stored in Kafka and applies continuous queries that derive new data streams or materialized views such as continuous aggregates over windows. Those continuous queries submitted to ksqlDB are compiled and executed as Kafka Streams applications that run indefinitely until terminated. 

Figure [2 ](#page4)illustrates an example application written in the Kafka Streams DSL. The application first reads in an event stream from Kafka topic pageview-events, filters out those pageview events whose period is less than 30 seconds, and then creates a windowed count of the number of pageviews per category every 5 seconds. The windowed count is considered an evolving table whose aggre-gate results are continuously updated and new update records are appended as a changelog stream. The changelog stream is written back to another Kafka topic pageview-windowed-counts. 

Kafka Streams translates the application logic in Figure [2 ](#page4)to a topology composed of connected data transformation operators. A topology is sub-divided into sub-topologies where each sub-topology consists of consecutive operators between which no data shuffling through network is required. For example, the filter and the map operator in the example application belong to the same sub-topology. No data shuffling is required between them since the filter only removes records from the data stream and does not change the partitioning key of the records. In contrast, the map and the count aggregation operator do not belong to the same sub-topology. This is because map may change the partitioning key of the records and the key-based count requires all records with the same key to be contained in one partition, therefore data shuffling based on the new key is required between the operators. Operators within a sub-topology are effectively fused together as upstream operators can directly pass data to down stream operators within the sub-topology without incurring any network overhead [[20]](#page12).

When the processing logic requires reshuffling input streams for data localities, such as the key-based count operator in our example, Kafka Streams routes the data through a repartition topic. Upstream sub-topologies produce to the repartition topic as a sink and determine which partition to send to based on the record key. Downstream sub-topologies consume from the repartition topic as a source. Once downstream sub-topologies have processed some records in offset order, they can request Kafka to delete these records from the repartition topics. These repartition topics serve as linearized, durable, fault-tolerant channels of communication between sub-topologies. The infinite capacity of repartition topics relieves Kafka Streams of the need to handle streaming backpres-sures. When upstream sub-topologies temporarily process faster than downstream sub-topologies, the excess data is simply buffered persistently in the repartition topic.

Stateful operators within a sub-topology are associated with state stores. Users can either configure to use the built-in persistent or in-memory state stores from Kafka Streams, or they can provide custom implementations from the supplied state store APIs. By default, writes to the states stores are also replicated to Kafka as changelog topics. Kafka brokers hosting these changelog topics will remove records for which another record was appended with the same key but a higher offset, effectively compacting the log. Both the repartition and changelog Kafka topics are considered internal data streams and abstracted away from the users.

Kafka Streams 是 Apache Kafka 中包含的 Java 库，允许用户构建有状态的实时流处理应用程序。

该库包含一个高级 DSL，供用户指定他们的流逻辑，该逻辑与消费者客户端从源 Kafka 主题连续读取数据流，将输入流转换为新的流和演化表，最后将结果流通过管道返回到接收 Kafka 主题.它还用作 ksqlDB [[24]](#page12) 的底层并行运行时，这是一个事件流数据库，用于处理 Apache Kafka 中的流数据。 ksqlDB 获取存储在 Kafka 中的输入数据流，并应用连续查询来派生新数据流或物化视图，例如窗口上的连续聚合。那些提交给 ksqlDB 的连续查询作为 Kafka Streams 应用程序被编译和执行，这些应用程序无限期地运行直到终止。

该库包含一个高级DSL，供用户指定其流逻辑，该流逻辑通过用户客户端连续读取来自源Kafka主题的数据流，将输入流转换为新流和不断变化的表，最后将结果流传回sink Kafka主题。它还用作ksqlDB[[24]]（#page12）的底层并行运行时，ksqlDB是一个事件流数据库，用于处理Apache Kafka中的流数据。ksqlDB获取存储在Kafka中的输入数据流，并应用连续查询来派生新的数据流或物化视图，例如windows上的连续聚合。提交给ksqlDB的那些连续查询被编译并作为Kafka流应用程序执行，这些应用程序将无限期地运行直到终止。

## 3.3  Data-Parallelism and Tasks

A sub-topology is executed as one or more tasks; one task per source topic partition. A task represents the smallest parallel unit of work in Kafka Streams. Tasks execute independently from each other irrespective of whether they execute the same sub-topology for another partition or a different sub-topology. Within a task, input records from the source topic partition are processed in-order by traversing the sub-topology, updating the state stores and generating output records to the changelog and sink topic partitions. Repartition topics may also be configured with different numbers of partitions than the external input topics so that different sub-topologies may have a different number of tasks. 

A Kafka Streams application can be deployed on multiple com-puting nodes as instances of the same application. When new in-stances of the application are launched or existing ones shutdown or crash, tasks will be re-distributed across instances automati-cally to balance the workload. Since both the input and output records of a task are persisted in Kafka logs, tasks can be inde-pendently paused, resumed, and migrated between instances. If a task with stateful operators needs to migrate to a new instance, an exact copy of the state is restored by replaying the corresponding changelog topics. Note that a single instance can potentially host and execute multiple tasks, and Kafka Streams tries to achieve both workload balance among instances and task stickiness to minimize the amount of state migration required when deciding how tasks should be re-distributed among instances. 

A sketch topology generated from the above Streams DSL example is shown in Figure [3 ](#page5)(with some trivial operators omitted). It is composed of two sub-topologies connected by a repartition topic. The repartition topic is needed because the map operator changes the key of the records and the key-based count requires all records with the same key in the same partition to achieve data locality. The first sub-topology contains the filter, map, and groupByKey that represents the repartitioning operation. The second sub-topology contains the windowBy and aggregation operators where the for-mer assigns each input record read from the repartition topic to its associated window(s) and the latter aggregates the input record values into the count for each window. Assuming the source topic pageview-events has two partitions, while the internal repartition topic and the sink topic pageview-windowed-counts have three partitions each, these two sub-topologies will have two and three tasks respectively. The changelog topic of the aggregation state store would also have three partitions, as determined by the number of tasks of the second sub-topology. Tasks may be executed inde-pendently and in parallel on different instances. Optimizations are also applied in Kafka Streams when generating the topology. In our example, the changelog topic of the state stores associated with the aggregation operator can potentially piggy-back on the sink topic since there is no further transformations between the aggregation operator and the sink operator sending the outgoing changelog data stream. For the sake of our running example demonstration in this section though, we skip such optimization and still display two separate Kafka topics. 

Kafka Streams uses embedded producer clients to send output records to Kafka brokers. If a produce request gets timed out, the producer would retry assuming those records were not received by the brokers. However as mentioned in Section [2.1 ](#page3)it is possible that the broker did receive the records and append to the logs, but the acknowledgements get delayed over the wire. In this case retries would result in duplicated appends. In addition, during normal processing Kafka Streams would periodically commit its current processing position on the source topics after it has flushed the outgoing records to sink topics as well as the updates to the state stores. But if a failure happens in between these operations, then upon failover the same source topic records could be refetched after the processing state are restored from the changelogs. In that case, we will reprocess these records while the processing results have already been persisted to the sink topics and state stores, causing at-least-once stream processing scenarios. In the next section, we will explain how Kafka solves the challenge to recover from failures and provide consistency guarantees.

# 4   EXACTLY-ONCE IN KAFKA STREAMS 

A key design principle behind Kafka’s architecture is to persist streaming records on the brokers, making them durable and im-mutable. This design is based on the assumption that persistence is, and will continue to be, not expensive. Past years have shown that this trend holds even as storage devices move from hard disk drives to solid state drives. Durability in Kafka is achieved via replication, such that every record written to a topic partition is persisted and replicated on n different broker machines (n is configurable). For each partition, one broker is elected to host the leader replica, which takes writes from the producer clients. Records appended to the leader replica’s local log are then fetched and replicated to the logs of the other n − 1 brokers that host follower replicas. Each Kafka broker may host multiple leader replicas of different topic partitions. When a broker fails, each leader replica it hosted will have a new leader elected from among that partition’s follower replicas. As a result, Kafka can tolerate n − 1 broker failures, meaning that a partition is available as long as at least one broker replica is avail-able [[38\]. ](#page12)The data replication protocol of Kafka guarantees that once a record has been appended successfully to the leader replica, it will be replicated to all available replicas. 

As a persistent storage layer, Kafka eliminates communication dependencies between streaming tasks: since records sent to Kafka are durably buffered, upstream tasks do not have any backpressure issues when downstream tasks are temporarily slow at processing. Persistent buffering reduces the complexity of recovering a failed stream processing instance and makes task failover and resump-tion relatively lightweight. In addition, since all records are stored durably, changelog topics inside Kafka Streams can be considered as the source-of-truth for state management. State stores, on the other hand, become disposable materialized views of the changelogs since they can always be restored by replaying the corresponding changelog streams. As a result, we can reduce the fault tolerant state management and output commit problems to a contract re-garding which input records are deemed as processed and which output records are visible under the various failure scenarios. In Kafka this contract is described as idempotent and transactional record appends.

## 4.1  Idempotent Writes Per Partition

An idempotent operation can be performed many times without causing a different effect than only being performed once. When it comes to stream processing in Kafka, an idempotent producer client can send the same record multiple times to a partition log and only one append would happen on that log. For that single partition, idempotent producer send operations can remove the possibility of duplicated records due to various failure scenarios such as ack time-outs due to network jitters. 

Idempotent producers work in a way similar to TCP: each record sent by the producer will be associated with a monotonically in-creasing sequence number. This sequence number combined with the producer client’s unique identifier can be used by the brokers to de-duplicate record appends within that producer’s lifetime. Unlike TCP, which provides guarantees only within a transient in-memory connection, sequence numbers are persisted along with the records to the replicated log. When multiple records are being appended to the log as a batch, we only need to encode one sequence number as the one for the first record as other sequence numbers can be inferred monotonically. On the broker side, latest sequence num-bers per-producer are cached and persisted when the brokers are shutting down. Thus, if a Kafka broker fails, any other broker that takes over its hosted topic partitions as the new leader replica will be able to re-populate its sequence number cache by looking at the local logs, and then be able to determine whether a producer send operation is a duplicate.

 

## 4.2  Transactional Writes Across Partitions

As mentioned in Section [3, ](#page4)stream processing in Kafka follows a read-process-write cycle: an embedded consumer reads records from the source topics, processing logic defined as a Kafka Streams sub-topology transforms those records and modifies the states, and an embedded producer writes the output streaming records to the sink and changelog topics. Finally, record offsets on the source topics can be committed, indicating the completion of processing. 

Exactly-once stream processing in this case requires the ability to execute a read-process-write operation exactly one time even under failures. In other words, all actions within this operation should be done atomically: 1) output records should be appended to the sink topics and became readable for downstream consumers, 2) processing state stores should be updated, 3) input record offsets on the source topics should be committed. Among those actions, state updates are replicated with appends to the underlying changelog topics which, upon failure, can be replayed to roll back the local state. In addition, offset commits in Kafka are translated internally as appends to an internal Kafka topic as well. Therefore, all these actions can be translated as record appends to some Kafka logs. Atomicity can be achieved by making these appends to different Kafka logs in a transactional manner: either all appends succeed and become visible to consumers or none do. 

Comparing with traditional two-phase commit protocols that require the data to be written twice (once for the log and another for the data), Kafka’s transactional protocol only requires writing data once in the log, and leverages on the append offset ordering to avoid exposing aborted data to the clients. In addition, Kafka avoids single points of failure by making the transaction coordi-nator highly-available with replicated transactional logs. Details about how transactional writes work in Kafka are described in the subsequent sections. 

### 4.2.1 Transaction Coordinator and Transaction Log. 

When transactional writes are enabled on a Kafka producer, that producer is assigned a unique identifier called the transactional id. This id is used to identify the same transactional producer instance across restarts. An incremental epoch is associated with the transactional id to distinguish between multiple lifetimes of the producer when it restarts due to failover or client migrations. 

As shown in Figure [4, ](#page7)within each Kafka broker there is a module called the transaction coordinator that manages metadata of all the transactional producers assigned to it. There is also a transaction log maintained as another internal Kafka topic, and each coordina-tor owns some subset of the partitions in the transaction log. The transaction coordinator keeps the metadata of each transaction it owns in memory, and also writes any updates of the metadata to the transaction log (Figure 4.a). It is worth noting that the transaction log only stores the latest metadata of a transaction and not the actual records sent within the transaction. The transaction’s meta-data include its current state, like Ongoing, PrepareCommit, and Completed, as well as the topic partitions that are registered with this transaction. The transaction coordinator is the only component to read and write the transaction log. 

As described at the beginning of Section [4, ](#page5)when a broker host-ing one or more transaction log partitions fails, new leader replicas for those transaction log partitions will be elected. Those replicas act as the new transaction coordinators owning those partitions, and rebuild an in-memory collection of the current transactions by replaying the metadata update records from the transaction logs. In this way, we leverage Kafka’s own replication protocol to ensure that the transaction coordinators are highly available and that all transaction state is updated durably. Every producer’s trans-actional id is mapped to a specific partition of the transaction log through a hashing function. This means any transactional producer with a given transactional id would only talk to one transaction coordinator across its lifetimes. 

After startup, the first operation of a transactional producer is to explicitly register its transactional id with the transaction coordi-nator (Figure 4.b). When it does so, the coordinator checks for any open transactions with the given transactional id and completes them based on their current states: only if the state is already in PrepareCommit it would roll forward the transaction, otherwise it would abort the transaction. It also increments the epoch associ-ated with the transactional id before acknowledging the producer’s registration. Once the epoch is bumped and persisted on the trans-action log, any producers with the same transactional id and an older epoch are considered zombies and hence fenced off, i. e. fu-ture transactional writes from those producers would be rejected. After getting the registration acknowlegement from the transaction coordinator, the producer client completes the initialization process and is ready to start sending records in transactions. 

### 4.2.2 Two-Phase Transactional Commit. 

A transactional producer can have at most one ongoing transaction at a given time. Each time the producer is about to send records to a new Kafka topic partition within a transaction, it first registers the partition with the transaction coordinator (Figure 4.c). In Kafka Streams, these partitions can belong to a sink topic, a changelog topic representing a state store, or the internal Kafka offset topic for offset committing. After registering new partitions in a transaction with the coordina-tor, the producer sends data to the actual data partitions as normal (Figure 4.d). 

After one or more records are sent within a transaction, the pro-ducer can try to commit the transaction: First, the producer flushes all its writes, awaiting acks for those writes from the Kafka brokers. Then it sends another request to the transaction coordinator to initiate the commit process through a two phase commit protocol (Figure 4.e). In the first phase, the coordinator updates its state to PrepareCommit and records its state change in the transaction log. This update is considered the synchronization barrier of the transac-tion: once the state update is replicated in the transaction log, there is no turning back. The transaction is guaranteed to be commit-ted even if the transaction coordinator crashes immediately after. The coordinator can then begin the second phase asynchronously, where it writes a transaction commit marker to each of the transac-tion’s registered partitions. Transaction markers are special records in the Kafka logs indicating all records appended from the specified transactional id before the marker are now committed (Figure 4.f). Just like data records, transaction markers written to Kafka logs are also replicated across brokers. After all the transaction markers have been acked by the partition leaders, the transaction coordina-tor updates its transaction state to CompleteCommit, which allows the producer client to start a new transaction. 

On the other hand, if an error happens during processing the producer can also abort the current transaction by sending an abort request to the coordinator. The transaction coordinator itself could also abort an ongoing transaction when the transaction times out. Abortion is handled in a similar way to commit: first the state of the transaction transits to PrepareAbort, then transaction abort markers are written to that transaction’s registered partitions in-dicating all records appended by the transactional id before this marker should be aborted and may not be returned to consumers. After all markers are acked the transaction state would transit to CompleteAbort. 

### 4.2.3 Reading Transactional Records. 

In Kafka Streams, upstream tasks execute atomic read-process-write cycles and append results to intermediate repartition topics. Downstream tasks then consume from these intermediate topics to continue further processing. The consumer clients of downstream tasks are configured to read only committed data, in which case records appended in a transaction are only delivered to the downstream tasks when their transactions are committed and commit markers are received. If the transaction is aborted with abort markers, its records would not be returned from the consumer clients. 

In addition, a task’s committed offsets on the source topics are also only reflected when the ongoing transaction is committed. If the current transaction is aborted, the committed offsets would be dropped and the committed offset would effectively roll back to the last committed transaction. As a result, when a Streams task migrates to a new instance, restores its state from the changelog topics and reset its positions on the source topics, the starting positions on the source topics would align with its restored state as well as the output records appended successfully to the sink topics. This guarantees that no source records get dropped or double processed even upon failures (Figure 4.g). 

## 4.3  Performance Implications

In Kafka Streams, users can switch from at-least-once semantics to exactly-once semantics with a single configuration. The exactly-once configuration enables both idempotence and transactional features. When multiple tasks are assigned to the same Streams instance, their processing can be grouped as a single on-going transaction by the instance’s embedded producer, and the producer needs to talk with its transaction coordinator in order to fence zom-bie writers and manage on-going transactions. With at-least-once configuration, producers within Kafka Streams will not conduct this additional coordination when writing data to Kafka topics. 

Figure 5.a illustrates the overhead of exactly-once with different number of transactional partitions. The evaluation was done on a three-node Kafka cluster that hosts two topics: an input topic written by a streaming data generator, and an output topic with varying number of partitions from 1 to 1000. A single-node Kafka Streams application is deployed that reads from the input topic, does a stateful reduce operation that reads from and writes to its local state store, and finally emits results to the output topic. Application’s commit interval is set to 100 milliseconds. The output topic is fetched by a consumer configured to read committed data only. The end-to-end latency is calculated per-record based on the record creation time when produced to the input topic, and the consumer reception time for that record’s result. All instances were deployed on dedicated i3.large AWS EC2 nodes. 

As observed in the figure, the throughput degradation of exactly-once semantics (EOS) is relatively small, ranging from about 10 to 20 percent compared with at-least-once semantics (ALOS). The first reason is that idempotence in Kafka producers only requires a few extra numeric fields with each batch of records to be persisted on the log. With a reasonable batch size in practice, these fields add negligible overhead. In addition, the write amplification cost of a transaction—additional RPCs between clients and the transaction coordinator and between the transaction coordinator and other Kafka brokers hosting the partitions involved in the transaction—is constant and independent of the number of records written within the transaction. Although this transaction cost is indeed dependent on the number of output partitions participated in a transaction, the impact is not massive since producers can batch multiple writing partitions in a single registration request. On the other hand, end-to-end latency is heavily dependent on the number of partitions since the transaction markers written per transaction increases linearly with the number of partitions, and the consumer can only fetch the committed records after the marker is received. 

The major factor impacting transactional commit throughput and latency is the commit interval: a longer commit interval will re-sult in larger transactions in terms of number of records and hence smaller amortized cost. However, longer commit intervals would also increase end-to-end processing latency because the consumer can only read records from the output topic written by the applica-tion’s transaction when that transaction has been committed. We illustrate its effect in Figure 5.b, which has the same experimental setup and application logic but fixes the number of partitions to 10 and varies on commit interval from 10 milliseconds to 10 seconds instead. To compare with checkpoing-based approaches, we also evaluate Apache Flink [[31\] ](#page12)(version 1.12.1) which is a well-known framework that leverages stream barriers to checkpoint state snap-shots. Flink’s checkpointing mechanism requires a persistent data source that can replay records in case of failures to achieve con-sistency, and when integrated with Kafka as its data source, its exactly-once implementation also relies on Kafka’s transactional protocols. In our experiment, we configure Flink to incrementally checkpoint its local state to an S3 bucket, and vary the checkpoint interval correspondingly as we vary Kafka Stream’s commit inter-val. As shown in the figure, both Kafka Streams and Flink have higher throughput at the cost of longer latency when commit/ checkpoint interval increases. Compared with Kafka Streams’ finer-grained changelogs though, Flink’s checkpointing is per-file based and hence would take longer time when only a small number of keys are updated within the interval. Since the transaction can only be committed after the checkpoint is completed, it would result in longer latency. As we increase the commit/checkpoint interval, more keys would be updated per commit and hence the latency gap becomes smaller.

